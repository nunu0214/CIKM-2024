{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "yXwXG1r0yP__",
        "-Vj68rbBEKES",
        "VV4alT_R22se",
        "UQxAwmUYnWkp",
        "SjjX6nDgnKwI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djpBLPTRl2fW",
        "outputId": "d4943811-b149-4d29-b140-39eab3a8f7bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python -V\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 导入所需要环境"
      ],
      "metadata": {
        "id": "yXwXG1r0yP__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install einops matplotlib matplotlib scikit_learn scipy scipy open3d meshio pandas vtk==9.2.6"
      ],
      "metadata": {
        "id": "76-vyRjomPMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09c1e063-ad70-4b30-c405-d208f3d5859c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: scikit_learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Collecting open3d\n",
            "  Downloading open3d-0.18.0-cp310-cp310-manylinux_2_27_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting meshio\n",
            "  Downloading meshio-5.3.5-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Collecting vtk==9.2.6\n",
            "  Downloading vtk-9.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn) (3.5.0)\n",
            "Collecting dash>=2.6.0 (from open3d)\n",
            "  Downloading dash-2.18.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: werkzeug>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.0.4)\n",
            "Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (5.10.4)\n",
            "Collecting configargparse (from open3d)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting ipywidgets>=8.0.4 (from open3d)\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting addict (from open3d)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from open3d) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open3d) (4.66.5)\n",
            "Collecting pyquaternion (from open3d)\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from meshio) (13.8.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.2.5)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.15.0)\n",
            "Collecting dash-html-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dash-table==5.0.0 (from dash>=2.6.0->open3d)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\n",
            "Collecting retrying (from dash>=2.6.0->open3d)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (71.0.4)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets>=8.0.4->open3d)\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.13)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=2.2.3->open3d) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->meshio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->meshio) (2.16.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (8.1.7)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.47)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.20.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->meshio) (0.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.0.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2024.8.30)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n",
            "Downloading vtk-9.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (79.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading open3d-0.18.0-cp310-cp310-manylinux_2_27_x86_64.whl (399.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading meshio-5.3.5-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.2/166.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash-2.18.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, addict, widgetsnbextension, retrying, pyquaternion, jedi, configargparse, comm, vtk, meshio, ipywidgets, dash, open3d\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.9\n",
            "    Uninstalling widgetsnbextension-3.6.9:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.9\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed addict-2.4.0 comm-0.2.2 configargparse-1.7 dash-2.18.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 ipywidgets-8.1.5 jedi-0.19.1 meshio-5.3.5 open3d-0.18.0 pyquaternion-0.9.9 retrying-1.3.4 vtk-9.2.6 widgetsnbextension-4.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 下载官方数据"
      ],
      "metadata": {
        "id": "v8Fimlw_yZj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IJCAI2024官方比赛数据,press数据与cikm一致,所以press使用IJCAI数据"
      ],
      "metadata": {
        "id": "OKb3Ot3uKqP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget --header=\"Host: ai-studio-online.bj.bcebos.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Referer: https://aistudio.baidu.com/\" \"https://ai-studio-online.bj.bcebos.com/v1/1638f9c292b9437bb46885186407a63e584856c91f9f4c18908b87abd46471e0?responseContentDisposition=attachment%3B%20filename%3Dtrack_B.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-05-05T03%3A02%3A25Z%2F-1%2F%2Fcfdfd6b6a9e096c761ee8e7d863d586741c69a9e6de89f9c3696706d35f8b265\" -c -O 'track_B.zip'\n",
        "!wget --header=\"Host: ai-studio-online.bj.bcebos.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Referer: https://aistudio.baidu.com/\" \"https://ai-studio-online.bj.bcebos.com/v1/2dddd05e577849ad95e1fe1133d3af29d13085ac0cfd499c853ff5d9df2ac07f?responseContentDisposition=attachment%3B%20filename%3Dtrain_data.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-05-05T03%3A23%3A33Z%2F-1%2F%2F8540633c7e39fddf8471d6d8206c3b761748c58c06005acb218593a8df19d7f1\" -c -O 'train_data.zip'\n",
        "!wget --header=\"Host: ai-studio-online.bj.bcebos.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Referer: https://aistudio.baidu.com/\" \"https://ai-studio-online.bj.bcebos.com/v1/a96dc8ba8201445b966980a0a48f52705338a48e29e64c53bddb7ef8861c5123?responseContentDisposition=attachment%3B%20filename%3Dtrack_A.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-05-06T07%3A54%3A51Z%2F-1%2F%2F17b5155bd16a8af1e4af971498082687656af7fcecfc5a8e57591b85053210ec\" -c -O 'track_A.zip'\n",
        "# !wget --header=\"Host: ai-studio-online.bj.bcebos.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Referer: https://aistudio.baidu.com/\" \"https://ai-studio-online.bj.bcebos.com/v1/dcaba9f0e87549e395e1682a4a0a43c547a896034e3f4417a4d10ba85a949944?responseContentDisposition=attachment%3B%20filename%3DPaddleScience.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-05-11T16%3A01%3A57Z%2F-1%2F%2Fb2149791689d3b19c02a86782229901cb75743d07c7163dfa185fea286cd1f02\" -c -O 'PaddleScience.zip'\n",
        "# !wget --header=\"Host: ai-studio-online.bj.bcebos.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Referer: https://aistudio.baidu.com/\" \"https://ai-studio-online.bj.bcebos.com/v1/38e9adf0fce84527aad3558cc3e82d0e9a251aac4c934297afae9b74d9b3d1e9?responseContentDisposition=attachment%3B%20filename%3Dtrain_track_B.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-06-04T03%3A21%3A02Z%2F-1%2F%2Facd359add161bace603a52c7a268467406cb3c1889a7114bbb687de8002b55f6\" -c -O 'train_track_B.zip'\n",
        "# !wget --header=\"Host: ai-studio-online.bj.bcebos.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Referer: https://aistudio.baidu.com/\" \"https://ai-studio-online.bj.bcebos.com/v1/7877c2fd300345599ed3365feda50425f4857caa71bf4af9bf047fb08e35aa97?responseContentDisposition=attachment%3B%20filename%3Dmesh_B_0603.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-06-04T11%3A48%3A26Z%2F-1%2F%2Fec6cce492ba6afff841ef197860065742a7ba8220def02c1cbc311333aa993b2\" -c -O 'mesh_B_0603.zip'\n",
        "# !wget --header=\"Host: ai-studio-online.bj.bcebos.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Referer: https://aistudio.baidu.com/\" \"https://ai-studio-online.bj.bcebos.com/v1/a02dba5700974c6a811f579fff216ccf9a4129b849994dcca99390b222c28572?responseContentDisposition=attachment%3B%20filename%3D3rd_lib.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-06-08T05%3A29%3A20Z%2F-1%2F%2Fef71819149664a3c8438d3dfd02544e77cdb4fda679efbdd402b4e2c77a2c06d\" -c -O '3rd_lib.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdw5Dap9Kokm",
        "outputId": "70e26953-ff1f-4692-e158-55913b2ca934"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-15 15:11:43--  https://ai-studio-online.bj.bcebos.com/v1/1638f9c292b9437bb46885186407a63e584856c91f9f4c18908b87abd46471e0?responseContentDisposition=attachment%3B%20filename%3Dtrack_B.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-05-05T03%3A02%3A25Z%2F-1%2F%2Fcfdfd6b6a9e096c761ee8e7d863d586741c69a9e6de89f9c3696706d35f8b265\n",
            "Resolving ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)... 103.235.47.176, 2409:8c04:1001:1203:0:ff:b0bb:4f27\n",
            "Connecting to ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)|103.235.47.176|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1012191818 (965M) [application/octet-stream]\n",
            "Saving to: ‘track_B.zip’\n",
            "\n",
            "track_B.zip         100%[===================>] 965.30M  15.9MB/s    in 80s     \n",
            "\n",
            "2024-09-15 15:13:05 (12.0 MB/s) - ‘track_B.zip’ saved [1012191818/1012191818]\n",
            "\n",
            "--2024-09-15 15:13:05--  https://ai-studio-online.bj.bcebos.com/v1/2dddd05e577849ad95e1fe1133d3af29d13085ac0cfd499c853ff5d9df2ac07f?responseContentDisposition=attachment%3B%20filename%3Dtrain_data.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-05-05T03%3A23%3A33Z%2F-1%2F%2F8540633c7e39fddf8471d6d8206c3b761748c58c06005acb218593a8df19d7f1\n",
            "Resolving ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)... 36.110.192.178, 2409:8c04:1001:1203:0:ff:b0bb:4f27\n",
            "Connecting to ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)|36.110.192.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46031310 (44M) [application/octet-stream]\n",
            "Saving to: ‘train_data.zip’\n",
            "\n",
            "train_data.zip      100%[===================>]  43.90M  10.5MB/s    in 5.2s    \n",
            "\n",
            "2024-09-15 15:13:12 (8.45 MB/s) - ‘train_data.zip’ saved [46031310/46031310]\n",
            "\n",
            "--2024-09-15 15:13:12--  https://ai-studio-online.bj.bcebos.com/v1/a96dc8ba8201445b966980a0a48f52705338a48e29e64c53bddb7ef8861c5123?responseContentDisposition=attachment%3B%20filename%3Dtrack_A.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-05-06T07%3A54%3A51Z%2F-1%2F%2F17b5155bd16a8af1e4af971498082687656af7fcecfc5a8e57591b85053210ec\n",
            "Resolving ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)... 36.110.192.178, 2409:8c04:1001:1203:0:ff:b0bb:4f27\n",
            "Connecting to ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)|36.110.192.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4688102 (4.5M) [application/octet-stream]\n",
            "Saving to: ‘track_A.zip’\n",
            "\n",
            "track_A.zip         100%[===================>]   4.47M  3.13MB/s    in 1.4s    \n",
            "\n",
            "2024-09-15 15:13:15 (3.13 MB/s) - ‘track_A.zip’ saved [4688102/4688102]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CIKM2024官方数据"
      ],
      "metadata": {
        "id": "QjzltE6OKxjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --header=\"Host: drive.usercontent.google.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Cookie: __Secure-ENID=12.SE=Yd0Bj-CLJ14fnd4qzdJHmwUs4B5zz46UaPC1cPJigNqqFV9PtM2CYyBpSbCkOyzUwzlEdZ1nZFf-igtGi7wSdJ_gqQSfQfh84r9egqFQAy9-GKayCRbdQKdera-2mkpuIT-c64CyR9vfNojM3hxZ9Dej-dGvtxlGjal9ttEHybw; __gsas=ID=ae0421b9a34b478c:T=1710758437:RT=1710758437:S=ALNI_MZP13R9ZOHbCzC0rgHSMrGXj6GCsg; SEARCH_SAMESITE=CgQI35oB; SID=g.a000lgiBaVVDJxM_nyJHv8SGKdW8l-Ea7eoYBmsE1puBGqeYmuoUD3CsQuje5kFlDuyAjS6tbwACgYKAewSARQSFQHGX2MiQwM3sBqt9QySR0B5yipflRoVAUF8yKo6yV91BccjS1Z6o32Ux6ew0076; __Secure-1PSID=g.a000lgiBaVVDJxM_nyJHv8SGKdW8l-Ea7eoYBmsE1puBGqeYmuoUdkz-2WaNotDz3XhWHdZaewACgYKAdMSARQSFQHGX2Mi2DyEcyhQnLt_3xtpRFnETBoVAUF8yKoObySYGWwftPorBYI_hdIh0076; __Secure-3PSID=g.a000lgiBaVVDJxM_nyJHv8SGKdW8l-Ea7eoYBmsE1puBGqeYmuoUMoY9B1xAja7hD33yfC_l8gACgYKAXESARQSFQHGX2Miy7Eh3rWIzqh1GS_d-315mhoVAUF8yKpBtafXePsFsQwbeu_1xhEZ0076; HSID=AqK8DRIVZrNtHDQEM; SSID=AF8GbXclnoQKbndEK; APISID=eaYPuocswpo7W9-H/AOCjbMPSZJ096e5h4; SAPISID=xTIl5TE60VtPYf3j/A-54gZvfpsOMhgwjL; __Secure-1PAPISID=xTIl5TE60VtPYf3j/A-54gZvfpsOMhgwjL; __Secure-3PAPISID=xTIl5TE60VtPYf3j/A-54gZvfpsOMhgwjL; NID=515=plru-gK0AjVf1Sy8SC5LvpAocQ8efRsdx0CZ1kdvR10iJMaeT-fsU6T5AyKgdFpWE-kImnEI5utU-487cxMnVkrz0_-aB-3blUbOI1wu31UI-2dAjxd2LWGEgubs2zJi8cjE8hfCzP-TgVqkwzSNkE6riAOmKvZvkdSWWgnwFiQLxr3X7et1TNvcPZvpEHzG8iqoylaY55lZje6L4CmiT0b8N2PzagLdpOZdUGkWyRJDlTsAeSnlvMM5M8ZTWjZm7UvVNqMRBh-CA4KJ-BkCavo-Pt0F4-3-qSceoZ3nNz6AUfFk_2J9dWLmijh23NbSZsQP4z2u7O2WDMwTmntI1Qc5QiIuXjxGDLDz6GQPSEMIyWZCRTsNxZtmNlKQROEsPwKKMmmylxBjRXLFtSKyLitb6oDPhkp8YbUh984lDVAuZkAOcjAuCgYFdAEmNyGGrWWFPmdaUAUZ-uxIL8lClSnbNx0zECu88QhO-QtallWvKzDYr3ZK7czaeJi2nQX53eUWfJM0eyOeeric1Ltf1wA1_w0zXPaBF5rPBONe7Rzm5phafNrSAl9XpofCiBT_B4mmb81hfFlGrBI3_4Rtj7UohFGwMaKcrIyPLBok4nQogBBPAzhPsnNFLJgBOUqQLAMDIoup982yf7v8ISeApMkZdgQ; __Secure-1PSIDTS=sidts-CjEB4E2dkZiFPZveD2L67T0j-mwN6ErulV1FpkkkhAlfHtcpBwyaLEFAl9KPDrhmId99EAA; __Secure-3PSIDTS=sidts-CjEB4E2dkZiFPZveD2L67T0j-mwN6ErulV1FpkkkhAlfHtcpBwyaLEFAl9KPDrhmId99EAA; SIDCC=AKEyXzV9L2DyLa7cOj7QEH5jq731oO3bjUfxB9xunhlp0aLuj_Yrks1puUhcLsZ8rELAfcHJjDmdqg; __Secure-1PSIDCC=AKEyXzUqTJxy1COupBctW9fNufvYQ4ZxCVzD4btsqLhg-i8UKiKC1jI3mI_5nliA_a0z-FM7PD0I; __Secure-3PSIDCC=AKEyXzU45-0-sVl11D0Mg7O6UieJpansLMEGJeXrmWGqtoNEtTeyHkgewKyRGMZVmJHm49Bzj1IM\" --header=\"Connection: keep-alive\" \"https://drive.usercontent.google.com/download?id=17M0nVsBdeiZxMnFVhKMuq89APc9pbA91&export=download&authuser=0&confirm=t&uuid=de518810-0b86-44da-9924-b3044aedae65&at=APZUnTWx_R5StqP3iGT6U7FBhIm-:1720979898499\" -c -O 'CIKM_Test.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqpqp4gvs3ov",
        "outputId": "eb298903-ac04-4246-f69a-1d1e4b2d4fe9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-15 15:13:15--  https://drive.usercontent.google.com/download?id=17M0nVsBdeiZxMnFVhKMuq89APc9pbA91&export=download&authuser=0&confirm=t&uuid=de518810-0b86-44da-9924-b3044aedae65&at=APZUnTWx_R5StqP3iGT6U7FBhIm-:1720979898499\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.197.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94200676 (90M) [application/octet-stream]\n",
            "Saving to: ‘CIKM_Test.zip’\n",
            "\n",
            "CIKM_Test.zip       100%[===================>]  89.84M   158MB/s    in 0.6s    \n",
            "\n",
            "2024-09-15 15:13:17 (158 MB/s) - ‘CIKM_Test.zip’ saved [94200676/94200676]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5N6wkdR75nJ",
        "outputId": "05ddac5e-9ada-46d6-f1ae-61aa300258d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-15 15:13:18--  https://drive.usercontent.google.com/download?id=1Y0ejShdcx6YiaE3jJ0yxcOTKxirR-MAh&export=download&authuser=0&confirm=t&uuid=90e67168-c423-4396-a7ac-4cfcb1f2cc01&at=APZUnTXTP3YsMMcYVNsvI5-B_UtN:1720979862404\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.197.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.197.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1089154958 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘CIKM_Training.zip’\n",
            "\n",
            "CIKM_Training.zip   100%[===================>]   1.01G  22.3MB/s    in 15s     \n",
            "\n",
            "2024-09-15 15:13:35 (68.0 MB/s) - ‘CIKM_Training.zip’ saved [1089154958/1089154958]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --header=\"Host: drive.usercontent.google.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Cookie: __Secure-ENID=12.SE=Yd0Bj-CLJ14fnd4qzdJHmwUs4B5zz46UaPC1cPJigNqqFV9PtM2CYyBpSbCkOyzUwzlEdZ1nZFf-igtGi7wSdJ_gqQSfQfh84r9egqFQAy9-GKayCRbdQKdera-2mkpuIT-c64CyR9vfNojM3hxZ9Dej-dGvtxlGjal9ttEHybw; __gsas=ID=ae0421b9a34b478c:T=1710758437:RT=1710758437:S=ALNI_MZP13R9ZOHbCzC0rgHSMrGXj6GCsg; SEARCH_SAMESITE=CgQI35oB; SID=g.a000lgiBaVVDJxM_nyJHv8SGKdW8l-Ea7eoYBmsE1puBGqeYmuoUD3CsQuje5kFlDuyAjS6tbwACgYKAewSARQSFQHGX2MiQwM3sBqt9QySR0B5yipflRoVAUF8yKo6yV91BccjS1Z6o32Ux6ew0076; __Secure-1PSID=g.a000lgiBaVVDJxM_nyJHv8SGKdW8l-Ea7eoYBmsE1puBGqeYmuoUdkz-2WaNotDz3XhWHdZaewACgYKAdMSARQSFQHGX2Mi2DyEcyhQnLt_3xtpRFnETBoVAUF8yKoObySYGWwftPorBYI_hdIh0076; __Secure-3PSID=g.a000lgiBaVVDJxM_nyJHv8SGKdW8l-Ea7eoYBmsE1puBGqeYmuoUMoY9B1xAja7hD33yfC_l8gACgYKAXESARQSFQHGX2Miy7Eh3rWIzqh1GS_d-315mhoVAUF8yKpBtafXePsFsQwbeu_1xhEZ0076; HSID=AqK8DRIVZrNtHDQEM; SSID=AF8GbXclnoQKbndEK; APISID=eaYPuocswpo7W9-H/AOCjbMPSZJ096e5h4; SAPISID=xTIl5TE60VtPYf3j/A-54gZvfpsOMhgwjL; __Secure-1PAPISID=xTIl5TE60VtPYf3j/A-54gZvfpsOMhgwjL; __Secure-3PAPISID=xTIl5TE60VtPYf3j/A-54gZvfpsOMhgwjL; NID=515=plru-gK0AjVf1Sy8SC5LvpAocQ8efRsdx0CZ1kdvR10iJMaeT-fsU6T5AyKgdFpWE-kImnEI5utU-487cxMnVkrz0_-aB-3blUbOI1wu31UI-2dAjxd2LWGEgubs2zJi8cjE8hfCzP-TgVqkwzSNkE6riAOmKvZvkdSWWgnwFiQLxr3X7et1TNvcPZvpEHzG8iqoylaY55lZje6L4CmiT0b8N2PzagLdpOZdUGkWyRJDlTsAeSnlvMM5M8ZTWjZm7UvVNqMRBh-CA4KJ-BkCavo-Pt0F4-3-qSceoZ3nNz6AUfFk_2J9dWLmijh23NbSZsQP4z2u7O2WDMwTmntI1Qc5QiIuXjxGDLDz6GQPSEMIyWZCRTsNxZtmNlKQROEsPwKKMmmylxBjRXLFtSKyLitb6oDPhkp8YbUh984lDVAuZkAOcjAuCgYFdAEmNyGGrWWFPmdaUAUZ-uxIL8lClSnbNx0zECu88QhO-QtallWvKzDYr3ZK7czaeJi2nQX53eUWfJM0eyOeeric1Ltf1wA1_w0zXPaBF5rPBONe7Rzm5phafNrSAl9XpofCiBT_B4mmb81hfFlGrBI3_4Rtj7UohFGwMaKcrIyPLBok4nQogBBPAzhPsnNFLJgBOUqQLAMDIoup982yf7v8ISeApMkZdgQ; __Secure-1PSIDTS=sidts-CjEB4E2dkZiFPZveD2L67T0j-mwN6ErulV1FpkkkhAlfHtcpBwyaLEFAl9KPDrhmId99EAA; __Secure-3PSIDTS=sidts-CjEB4E2dkZiFPZveD2L67T0j-mwN6ErulV1FpkkkhAlfHtcpBwyaLEFAl9KPDrhmId99EAA; SIDCC=AKEyXzURbTd6qCpA02QoGXCORelVic8I05vY0Fudj1oUukq6szKRAuO4mcverQ_TFNKg0pPsF-tiZg; __Secure-1PSIDCC=AKEyXzVPd347dt-7_3JSjQc3gmzPqwRtmRgaQpuVqN0qtN34VORpmjxOBVKFZsXxU0m0JibWfONm; __Secure-3PSIDCC=AKEyXzWa_JoS5Duud9ECYjQhLb8ENX9_ML6X5YK4El1eSaET9Cq21vXwSW53TCFtQ0e-KwYt3BEK\" --header=\"Connection: keep-alive\" \"https://drive.usercontent.google.com/download?id=1Y0ejShdcx6YiaE3jJ0yxcOTKxirR-MAh&export=download&authuser=0&confirm=t&uuid=90e67168-c423-4396-a7ac-4cfcb1f2cc01&at=APZUnTXTP3YsMMcYVNsvI5-B_UtN:1720979862404\" -c -O 'CIKM_Training.zip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir Dataset\n",
        "# !unzip -o track_B.zip -d Dataset/\n",
        "!unzip -o train_data.zip -d Dataset/\n",
        "!unzip -o track_A.zip -d Dataset/\n",
        "# !mkdir Dataset/train_track_B && unzip -o train_track_B.zip -d Dataset/train_track_B/\n",
        "# !unzip -o mesh_B_0603.zip -d Dataset/\n",
        "!unzip -q CIKM_Training.zip\n",
        "!unzip -q CIKM_Test.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WVPfk_AtC6I",
        "outputId": "f8a7beab-2321-450b-8505-a94b3aa45f13"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  train_data.zip\n",
            "  inflating: Dataset/data/mesh_001.ply  \n",
            "  inflating: Dataset/data/mesh_002.ply  \n",
            "  inflating: Dataset/data/mesh_004.ply  \n",
            "  inflating: Dataset/data/mesh_005.ply  \n",
            "  inflating: Dataset/data/mesh_006.ply  \n",
            "  inflating: Dataset/data/mesh_007.ply  \n",
            "  inflating: Dataset/data/mesh_008.ply  \n",
            "  inflating: Dataset/data/mesh_010.ply  \n",
            "  inflating: Dataset/data/mesh_012.ply  \n",
            "  inflating: Dataset/data/mesh_013.ply  \n",
            "  inflating: Dataset/data/mesh_017.ply  \n",
            "  inflating: Dataset/data/mesh_018.ply  \n",
            "  inflating: Dataset/data/mesh_021.ply  \n",
            "  inflating: Dataset/data/mesh_022.ply  \n",
            "  inflating: Dataset/data/mesh_023.ply  \n",
            "  inflating: Dataset/data/mesh_025.ply  \n",
            "  inflating: Dataset/data/mesh_026.ply  \n",
            "  inflating: Dataset/data/mesh_027.ply  \n",
            "  inflating: Dataset/data/mesh_028.ply  \n",
            "  inflating: Dataset/data/mesh_029.ply  \n",
            "  inflating: Dataset/data/mesh_030.ply  \n",
            "  inflating: Dataset/data/mesh_031.ply  \n",
            "  inflating: Dataset/data/mesh_032.ply  \n",
            "  inflating: Dataset/data/mesh_034.ply  \n",
            "  inflating: Dataset/data/mesh_035.ply  \n",
            "  inflating: Dataset/data/mesh_039.ply  \n",
            "  inflating: Dataset/data/mesh_040.ply  \n",
            "  inflating: Dataset/data/mesh_043.ply  \n",
            "  inflating: Dataset/data/mesh_044.ply  \n",
            "  inflating: Dataset/data/mesh_045.ply  \n",
            "  inflating: Dataset/data/mesh_046.ply  \n",
            "  inflating: Dataset/data/mesh_047.ply  \n",
            "  inflating: Dataset/data/mesh_048.ply  \n",
            "  inflating: Dataset/data/mesh_049.ply  \n",
            "  inflating: Dataset/data/mesh_050.ply  \n",
            "  inflating: Dataset/data/mesh_051.ply  \n",
            "  inflating: Dataset/data/mesh_052.ply  \n",
            "  inflating: Dataset/data/mesh_054.ply  \n",
            "  inflating: Dataset/data/mesh_055.ply  \n",
            "  inflating: Dataset/data/mesh_056.ply  \n",
            "  inflating: Dataset/data/mesh_058.ply  \n",
            "  inflating: Dataset/data/mesh_059.ply  \n",
            "  inflating: Dataset/data/mesh_060.ply  \n",
            "  inflating: Dataset/data/mesh_061.ply  \n",
            "  inflating: Dataset/data/mesh_062.ply  \n",
            "  inflating: Dataset/data/mesh_063.ply  \n",
            "  inflating: Dataset/data/mesh_064.ply  \n",
            "  inflating: Dataset/data/mesh_065.ply  \n",
            "  inflating: Dataset/data/mesh_067.ply  \n",
            "  inflating: Dataset/data/mesh_069.ply  \n",
            "  inflating: Dataset/data/mesh_070.ply  \n",
            "  inflating: Dataset/data/mesh_071.ply  \n",
            "  inflating: Dataset/data/mesh_072.ply  \n",
            "  inflating: Dataset/data/mesh_073.ply  \n",
            "  inflating: Dataset/data/mesh_074.ply  \n",
            "  inflating: Dataset/data/mesh_075.ply  \n",
            "  inflating: Dataset/data/mesh_076.ply  \n",
            "  inflating: Dataset/data/mesh_077.ply  \n",
            "  inflating: Dataset/data/mesh_078.ply  \n",
            "  inflating: Dataset/data/mesh_079.ply  \n",
            "  inflating: Dataset/data/mesh_080.ply  \n",
            "  inflating: Dataset/data/mesh_081.ply  \n",
            "  inflating: Dataset/data/mesh_083.ply  \n",
            "  inflating: Dataset/data/mesh_084.ply  \n",
            "  inflating: Dataset/data/mesh_085.ply  \n",
            "  inflating: Dataset/data/mesh_086.ply  \n",
            "  inflating: Dataset/data/mesh_087.ply  \n",
            "  inflating: Dataset/data/mesh_088.ply  \n",
            "  inflating: Dataset/data/mesh_090.ply  \n",
            "  inflating: Dataset/data/mesh_091.ply  \n",
            "  inflating: Dataset/data/mesh_092.ply  \n",
            "  inflating: Dataset/data/mesh_094.ply  \n",
            "  inflating: Dataset/data/mesh_095.ply  \n",
            "  inflating: Dataset/data/mesh_096.ply  \n",
            "  inflating: Dataset/data/mesh_097.ply  \n",
            "  inflating: Dataset/data/mesh_100.ply  \n",
            "  inflating: Dataset/data/mesh_101.ply  \n",
            "  inflating: Dataset/data/mesh_102.ply  \n",
            "  inflating: Dataset/data/mesh_105.ply  \n",
            "  inflating: Dataset/data/mesh_106.ply  \n",
            "  inflating: Dataset/data/mesh_107.ply  \n",
            "  inflating: Dataset/data/mesh_109.ply  \n",
            "  inflating: Dataset/data/mesh_110.ply  \n",
            "  inflating: Dataset/data/mesh_111.ply  \n",
            "  inflating: Dataset/data/mesh_112.ply  \n",
            "  inflating: Dataset/data/mesh_113.ply  \n",
            "  inflating: Dataset/data/mesh_114.ply  \n",
            "  inflating: Dataset/data/mesh_115.ply  \n",
            "  inflating: Dataset/data/mesh_116.ply  \n",
            "  inflating: Dataset/data/mesh_117.ply  \n",
            "  inflating: Dataset/data/mesh_118.ply  \n",
            "  inflating: Dataset/data/mesh_119.ply  \n",
            "  inflating: Dataset/data/mesh_120.ply  \n",
            "  inflating: Dataset/data/mesh_121.ply  \n",
            "  inflating: Dataset/data/mesh_123.ply  \n",
            "  inflating: Dataset/data/mesh_124.ply  \n",
            "  inflating: Dataset/data/mesh_125.ply  \n",
            "  inflating: Dataset/data/mesh_126.ply  \n",
            "  inflating: Dataset/data/mesh_127.ply  \n",
            "  inflating: Dataset/data/mesh_128.ply  \n",
            "  inflating: Dataset/data/mesh_129.ply  \n",
            "  inflating: Dataset/data/mesh_130.ply  \n",
            "  inflating: Dataset/data/mesh_131.ply  \n",
            "  inflating: Dataset/data/mesh_133.ply  \n",
            "  inflating: Dataset/data/mesh_134.ply  \n",
            "  inflating: Dataset/data/mesh_136.ply  \n",
            "  inflating: Dataset/data/mesh_137.ply  \n",
            "  inflating: Dataset/data/mesh_138.ply  \n",
            "  inflating: Dataset/data/mesh_139.ply  \n",
            "  inflating: Dataset/data/mesh_140.ply  \n",
            "  inflating: Dataset/data/mesh_141.ply  \n",
            "  inflating: Dataset/data/mesh_142.ply  \n",
            "  inflating: Dataset/data/mesh_143.ply  \n",
            "  inflating: Dataset/data/mesh_144.ply  \n",
            "  inflating: Dataset/data/mesh_145.ply  \n",
            "  inflating: Dataset/data/mesh_146.ply  \n",
            "  inflating: Dataset/data/mesh_147.ply  \n",
            "  inflating: Dataset/data/mesh_148.ply  \n",
            "  inflating: Dataset/data/mesh_149.ply  \n",
            "  inflating: Dataset/data/mesh_150.ply  \n",
            "  inflating: Dataset/data/mesh_151.ply  \n",
            "  inflating: Dataset/data/mesh_152.ply  \n",
            "  inflating: Dataset/data/mesh_153.ply  \n",
            "  inflating: Dataset/data/mesh_155.ply  \n",
            "  inflating: Dataset/data/mesh_156.ply  \n",
            "  inflating: Dataset/data/mesh_157.ply  \n",
            "  inflating: Dataset/data/mesh_158.ply  \n",
            "  inflating: Dataset/data/mesh_159.ply  \n",
            "  inflating: Dataset/data/mesh_160.ply  \n",
            "  inflating: Dataset/data/mesh_161.ply  \n",
            "  inflating: Dataset/data/mesh_162.ply  \n",
            "  inflating: Dataset/data/mesh_163.ply  \n",
            "  inflating: Dataset/data/mesh_165.ply  \n",
            "  inflating: Dataset/data/mesh_166.ply  \n",
            "  inflating: Dataset/data/mesh_170.ply  \n",
            "  inflating: Dataset/data/mesh_172.ply  \n",
            "  inflating: Dataset/data/mesh_173.ply  \n",
            "  inflating: Dataset/data/mesh_175.ply  \n",
            "  inflating: Dataset/data/mesh_176.ply  \n",
            "  inflating: Dataset/data/mesh_177.ply  \n",
            "  inflating: Dataset/data/mesh_178.ply  \n",
            "  inflating: Dataset/data/mesh_179.ply  \n",
            "  inflating: Dataset/data/mesh_180.ply  \n",
            "  inflating: Dataset/data/mesh_181.ply  \n",
            "  inflating: Dataset/data/mesh_182.ply  \n",
            "  inflating: Dataset/data/mesh_183.ply  \n",
            "  inflating: Dataset/data/mesh_184.ply  \n",
            "  inflating: Dataset/data/mesh_186.ply  \n",
            "  inflating: Dataset/data/mesh_190.ply  \n",
            "  inflating: Dataset/data/mesh_191.ply  \n",
            "  inflating: Dataset/data/mesh_192.ply  \n",
            "  inflating: Dataset/data/mesh_193.ply  \n",
            "  inflating: Dataset/data/mesh_195.ply  \n",
            "  inflating: Dataset/data/mesh_196.ply  \n",
            "  inflating: Dataset/data/mesh_198.ply  \n",
            "  inflating: Dataset/data/mesh_199.ply  \n",
            "  inflating: Dataset/data/mesh_200.ply  \n",
            "  inflating: Dataset/data/mesh_201.ply  \n",
            "  inflating: Dataset/data/mesh_202.ply  \n",
            "  inflating: Dataset/data/mesh_203.ply  \n",
            "  inflating: Dataset/data/mesh_205.ply  \n",
            "  inflating: Dataset/data/mesh_207.ply  \n",
            "  inflating: Dataset/data/mesh_210.ply  \n",
            "  inflating: Dataset/data/mesh_211.ply  \n",
            "  inflating: Dataset/data/mesh_212.ply  \n",
            "  inflating: Dataset/data/mesh_213.ply  \n",
            "  inflating: Dataset/data/mesh_214.ply  \n",
            "  inflating: Dataset/data/mesh_215.ply  \n",
            "  inflating: Dataset/data/mesh_217.ply  \n",
            "  inflating: Dataset/data/mesh_219.ply  \n",
            "  inflating: Dataset/data/mesh_220.ply  \n",
            "  inflating: Dataset/data/mesh_221.ply  \n",
            "  inflating: Dataset/data/mesh_222.ply  \n",
            "  inflating: Dataset/data/mesh_223.ply  \n",
            "  inflating: Dataset/data/mesh_224.ply  \n",
            "  inflating: Dataset/data/mesh_225.ply  \n",
            "  inflating: Dataset/data/mesh_227.ply  \n",
            "  inflating: Dataset/data/mesh_228.ply  \n",
            "  inflating: Dataset/data/mesh_229.ply  \n",
            "  inflating: Dataset/data/mesh_230.ply  \n",
            "  inflating: Dataset/data/mesh_231.ply  \n",
            "  inflating: Dataset/data/mesh_232.ply  \n",
            "  inflating: Dataset/data/mesh_233.ply  \n",
            "  inflating: Dataset/data/mesh_234.ply  \n",
            "  inflating: Dataset/data/mesh_235.ply  \n",
            "  inflating: Dataset/data/mesh_236.ply  \n",
            "  inflating: Dataset/data/mesh_237.ply  \n",
            "  inflating: Dataset/data/mesh_241.ply  \n",
            "  inflating: Dataset/data/mesh_243.ply  \n",
            "  inflating: Dataset/data/mesh_244.ply  \n",
            "  inflating: Dataset/data/mesh_245.ply  \n",
            "  inflating: Dataset/data/mesh_246.ply  \n",
            "  inflating: Dataset/data/mesh_247.ply  \n",
            "  inflating: Dataset/data/mesh_248.ply  \n",
            "  inflating: Dataset/data/mesh_249.ply  \n",
            "  inflating: Dataset/data/mesh_251.ply  \n",
            "  inflating: Dataset/data/mesh_252.ply  \n",
            "  inflating: Dataset/data/mesh_253.ply  \n",
            "  inflating: Dataset/data/mesh_255.ply  \n",
            "  inflating: Dataset/data/mesh_257.ply  \n",
            "  inflating: Dataset/data/mesh_258.ply  \n",
            "  inflating: Dataset/data/mesh_259.ply  \n",
            "  inflating: Dataset/data/mesh_260.ply  \n",
            "  inflating: Dataset/data/mesh_261.ply  \n",
            "  inflating: Dataset/data/mesh_262.ply  \n",
            "  inflating: Dataset/data/mesh_263.ply  \n",
            "  inflating: Dataset/data/mesh_264.ply  \n",
            "  inflating: Dataset/data/mesh_266.ply  \n",
            "  inflating: Dataset/data/mesh_267.ply  \n",
            "  inflating: Dataset/data/mesh_268.ply  \n",
            "  inflating: Dataset/data/mesh_269.ply  \n",
            "  inflating: Dataset/data/mesh_271.ply  \n",
            "  inflating: Dataset/data/mesh_272.ply  \n",
            "  inflating: Dataset/data/mesh_273.ply  \n",
            "  inflating: Dataset/data/mesh_274.ply  \n",
            "  inflating: Dataset/data/mesh_275.ply  \n",
            "  inflating: Dataset/data/mesh_276.ply  \n",
            "  inflating: Dataset/data/mesh_277.ply  \n",
            "  inflating: Dataset/data/mesh_278.ply  \n",
            "  inflating: Dataset/data/mesh_279.ply  \n",
            "  inflating: Dataset/data/mesh_280.ply  \n",
            "  inflating: Dataset/data/mesh_281.ply  \n",
            "  inflating: Dataset/data/mesh_282.ply  \n",
            "  inflating: Dataset/data/mesh_283.ply  \n",
            "  inflating: Dataset/data/mesh_285.ply  \n",
            "  inflating: Dataset/data/mesh_286.ply  \n",
            "  inflating: Dataset/data/mesh_289.ply  \n",
            "  inflating: Dataset/data/mesh_290.ply  \n",
            "  inflating: Dataset/data/mesh_291.ply  \n",
            "  inflating: Dataset/data/mesh_292.ply  \n",
            "  inflating: Dataset/data/mesh_293.ply  \n",
            "  inflating: Dataset/data/mesh_294.ply  \n",
            "  inflating: Dataset/data/mesh_295.ply  \n",
            "  inflating: Dataset/data/mesh_296.ply  \n",
            "  inflating: Dataset/data/mesh_297.ply  \n",
            "  inflating: Dataset/data/mesh_298.ply  \n",
            "  inflating: Dataset/data/mesh_299.ply  \n",
            "  inflating: Dataset/data/mesh_300.ply  \n",
            "  inflating: Dataset/data/mesh_301.ply  \n",
            "  inflating: Dataset/data/mesh_302.ply  \n",
            "  inflating: Dataset/data/mesh_304.ply  \n",
            "  inflating: Dataset/data/mesh_305.ply  \n",
            "  inflating: Dataset/data/mesh_306.ply  \n",
            "  inflating: Dataset/data/mesh_308.ply  \n",
            "  inflating: Dataset/data/mesh_309.ply  \n",
            "  inflating: Dataset/data/mesh_310.ply  \n",
            "  inflating: Dataset/data/mesh_311.ply  \n",
            "  inflating: Dataset/data/mesh_312.ply  \n",
            "  inflating: Dataset/data/mesh_313.ply  \n",
            "  inflating: Dataset/data/mesh_314.ply  \n",
            "  inflating: Dataset/data/mesh_315.ply  \n",
            "  inflating: Dataset/data/mesh_319.ply  \n",
            "  inflating: Dataset/data/mesh_320.ply  \n",
            "  inflating: Dataset/data/mesh_321.ply  \n",
            "  inflating: Dataset/data/mesh_322.ply  \n",
            "  inflating: Dataset/data/mesh_323.ply  \n",
            "  inflating: Dataset/data/mesh_324.ply  \n",
            "  inflating: Dataset/data/mesh_325.ply  \n",
            "  inflating: Dataset/data/mesh_327.ply  \n",
            "  inflating: Dataset/data/mesh_328.ply  \n",
            "  inflating: Dataset/data/mesh_329.ply  \n",
            "  inflating: Dataset/data/mesh_331.ply  \n",
            "  inflating: Dataset/data/mesh_332.ply  \n",
            "  inflating: Dataset/data/mesh_333.ply  \n",
            "  inflating: Dataset/data/mesh_334.ply  \n",
            "  inflating: Dataset/data/mesh_335.ply  \n",
            "  inflating: Dataset/data/mesh_337.ply  \n",
            "  inflating: Dataset/data/mesh_338.ply  \n",
            "  inflating: Dataset/data/mesh_339.ply  \n",
            "  inflating: Dataset/data/mesh_340.ply  \n",
            "  inflating: Dataset/data/mesh_341.ply  \n",
            "  inflating: Dataset/data/mesh_344.ply  \n",
            "  inflating: Dataset/data/mesh_345.ply  \n",
            "  inflating: Dataset/data/mesh_347.ply  \n",
            "  inflating: Dataset/data/mesh_348.ply  \n",
            "  inflating: Dataset/data/mesh_349.ply  \n",
            "  inflating: Dataset/data/mesh_350.ply  \n",
            "  inflating: Dataset/data/mesh_352.ply  \n",
            "  inflating: Dataset/data/mesh_353.ply  \n",
            "  inflating: Dataset/data/mesh_354.ply  \n",
            "  inflating: Dataset/data/mesh_355.ply  \n",
            "  inflating: Dataset/data/mesh_356.ply  \n",
            "  inflating: Dataset/data/mesh_357.ply  \n",
            "  inflating: Dataset/data/mesh_358.ply  \n",
            "  inflating: Dataset/data/mesh_360.ply  \n",
            "  inflating: Dataset/data/mesh_362.ply  \n",
            "  inflating: Dataset/data/mesh_364.ply  \n",
            "  inflating: Dataset/data/mesh_365.ply  \n",
            "  inflating: Dataset/data/mesh_366.ply  \n",
            "  inflating: Dataset/data/mesh_367.ply  \n",
            "  inflating: Dataset/data/mesh_369.ply  \n",
            "  inflating: Dataset/data/mesh_371.ply  \n",
            "  inflating: Dataset/data/mesh_372.ply  \n",
            "  inflating: Dataset/data/mesh_373.ply  \n",
            "  inflating: Dataset/data/mesh_374.ply  \n",
            "  inflating: Dataset/data/mesh_375.ply  \n",
            "  inflating: Dataset/data/mesh_376.ply  \n",
            "  inflating: Dataset/data/mesh_378.ply  \n",
            "  inflating: Dataset/data/mesh_379.ply  \n",
            "  inflating: Dataset/data/mesh_380.ply  \n",
            "  inflating: Dataset/data/mesh_381.ply  \n",
            "  inflating: Dataset/data/mesh_384.ply  \n",
            "  inflating: Dataset/data/mesh_385.ply  \n",
            "  inflating: Dataset/data/mesh_389.ply  \n",
            "  inflating: Dataset/data/mesh_392.ply  \n",
            "  inflating: Dataset/data/mesh_393.ply  \n",
            "  inflating: Dataset/data/mesh_397.ply  \n",
            "  inflating: Dataset/data/mesh_398.ply  \n",
            "  inflating: Dataset/data/mesh_399.ply  \n",
            "  inflating: Dataset/data/mesh_401.ply  \n",
            "  inflating: Dataset/data/mesh_402.ply  \n",
            "  inflating: Dataset/data/mesh_403.ply  \n",
            "  inflating: Dataset/data/mesh_404.ply  \n",
            "  inflating: Dataset/data/mesh_405.ply  \n",
            "  inflating: Dataset/data/mesh_407.ply  \n",
            "  inflating: Dataset/data/mesh_408.ply  \n",
            "  inflating: Dataset/data/mesh_410.ply  \n",
            "  inflating: Dataset/data/mesh_412.ply  \n",
            "  inflating: Dataset/data/mesh_413.ply  \n",
            "  inflating: Dataset/data/mesh_414.ply  \n",
            "  inflating: Dataset/data/mesh_415.ply  \n",
            "  inflating: Dataset/data/mesh_417.ply  \n",
            "  inflating: Dataset/data/mesh_418.ply  \n",
            "  inflating: Dataset/data/mesh_419.ply  \n",
            "  inflating: Dataset/data/mesh_420.ply  \n",
            "  inflating: Dataset/data/mesh_422.ply  \n",
            "  inflating: Dataset/data/mesh_424.ply  \n",
            "  inflating: Dataset/data/mesh_425.ply  \n",
            "  inflating: Dataset/data/mesh_427.ply  \n",
            "  inflating: Dataset/data/mesh_430.ply  \n",
            "  inflating: Dataset/data/mesh_431.ply  \n",
            "  inflating: Dataset/data/mesh_433.ply  \n",
            "  inflating: Dataset/data/mesh_435.ply  \n",
            "  inflating: Dataset/data/mesh_436.ply  \n",
            "  inflating: Dataset/data/mesh_437.ply  \n",
            "  inflating: Dataset/data/mesh_439.ply  \n",
            "  inflating: Dataset/data/mesh_440.ply  \n",
            "  inflating: Dataset/data/mesh_443.ply  \n",
            "  inflating: Dataset/data/mesh_444.ply  \n",
            "  inflating: Dataset/data/mesh_446.ply  \n",
            "  inflating: Dataset/data/mesh_447.ply  \n",
            "  inflating: Dataset/data/mesh_448.ply  \n",
            "  inflating: Dataset/data/mesh_449.ply  \n",
            "  inflating: Dataset/data/mesh_450.ply  \n",
            "  inflating: Dataset/data/mesh_451.ply  \n",
            "  inflating: Dataset/data/mesh_452.ply  \n",
            "  inflating: Dataset/data/mesh_453.ply  \n",
            "  inflating: Dataset/data/mesh_454.ply  \n",
            "  inflating: Dataset/data/mesh_455.ply  \n",
            "  inflating: Dataset/data/mesh_456.ply  \n",
            "  inflating: Dataset/data/mesh_457.ply  \n",
            "  inflating: Dataset/data/mesh_459.ply  \n",
            "  inflating: Dataset/data/mesh_460.ply  \n",
            "  inflating: Dataset/data/mesh_462.ply  \n",
            "  inflating: Dataset/data/mesh_463.ply  \n",
            "  inflating: Dataset/data/mesh_464.ply  \n",
            "  inflating: Dataset/data/mesh_465.ply  \n",
            "  inflating: Dataset/data/mesh_466.ply  \n",
            "  inflating: Dataset/data/mesh_467.ply  \n",
            "  inflating: Dataset/data/mesh_468.ply  \n",
            "  inflating: Dataset/data/mesh_469.ply  \n",
            "  inflating: Dataset/data/mesh_470.ply  \n",
            "  inflating: Dataset/data/mesh_472.ply  \n",
            "  inflating: Dataset/data/mesh_473.ply  \n",
            "  inflating: Dataset/data/mesh_474.ply  \n",
            "  inflating: Dataset/data/mesh_475.ply  \n",
            "  inflating: Dataset/data/mesh_476.ply  \n",
            "  inflating: Dataset/data/mesh_478.ply  \n",
            "  inflating: Dataset/data/mesh_479.ply  \n",
            "  inflating: Dataset/data/mesh_480.ply  \n",
            "  inflating: Dataset/data/mesh_482.ply  \n",
            "  inflating: Dataset/data/mesh_483.ply  \n",
            "  inflating: Dataset/data/mesh_486.ply  \n",
            "  inflating: Dataset/data/mesh_487.ply  \n",
            "  inflating: Dataset/data/mesh_488.ply  \n",
            "  inflating: Dataset/data/mesh_490.ply  \n",
            "  inflating: Dataset/data/mesh_493.ply  \n",
            "  inflating: Dataset/data/mesh_494.ply  \n",
            "  inflating: Dataset/data/mesh_495.ply  \n",
            "  inflating: Dataset/data/mesh_496.ply  \n",
            "  inflating: Dataset/data/mesh_497.ply  \n",
            "  inflating: Dataset/data/mesh_498.ply  \n",
            "  inflating: Dataset/data/mesh_499.ply  \n",
            "  inflating: Dataset/data/mesh_501.ply  \n",
            "  inflating: Dataset/data/mesh_502.ply  \n",
            "  inflating: Dataset/data/mesh_503.ply  \n",
            "  inflating: Dataset/data/mesh_504.ply  \n",
            "  inflating: Dataset/data/mesh_505.ply  \n",
            "  inflating: Dataset/data/mesh_507.ply  \n",
            "  inflating: Dataset/data/mesh_508.ply  \n",
            "  inflating: Dataset/data/mesh_509.ply  \n",
            "  inflating: Dataset/data/mesh_511.ply  \n",
            "  inflating: Dataset/data/mesh_512.ply  \n",
            "  inflating: Dataset/data/mesh_513.ply  \n",
            "  inflating: Dataset/data/mesh_514.ply  \n",
            "  inflating: Dataset/data/mesh_515.ply  \n",
            "  inflating: Dataset/data/mesh_516.ply  \n",
            "  inflating: Dataset/data/mesh_518.ply  \n",
            "  inflating: Dataset/data/mesh_519.ply  \n",
            "  inflating: Dataset/data/mesh_521.ply  \n",
            "  inflating: Dataset/data/mesh_522.ply  \n",
            "  inflating: Dataset/data/mesh_523.ply  \n",
            "  inflating: Dataset/data/mesh_524.ply  \n",
            "  inflating: Dataset/data/mesh_525.ply  \n",
            "  inflating: Dataset/data/mesh_527.ply  \n",
            "  inflating: Dataset/data/mesh_529.ply  \n",
            "  inflating: Dataset/data/mesh_530.ply  \n",
            "  inflating: Dataset/data/mesh_532.ply  \n",
            "  inflating: Dataset/data/mesh_533.ply  \n",
            "  inflating: Dataset/data/mesh_536.ply  \n",
            "  inflating: Dataset/data/mesh_538.ply  \n",
            "  inflating: Dataset/data/mesh_539.ply  \n",
            "  inflating: Dataset/data/mesh_540.ply  \n",
            "  inflating: Dataset/data/mesh_542.ply  \n",
            "  inflating: Dataset/data/mesh_543.ply  \n",
            "  inflating: Dataset/data/mesh_545.ply  \n",
            "  inflating: Dataset/data/mesh_547.ply  \n",
            "  inflating: Dataset/data/mesh_548.ply  \n",
            "  inflating: Dataset/data/mesh_549.ply  \n",
            "  inflating: Dataset/data/mesh_550.ply  \n",
            "  inflating: Dataset/data/mesh_551.ply  \n",
            "  inflating: Dataset/data/mesh_552.ply  \n",
            "  inflating: Dataset/data/mesh_553.ply  \n",
            "  inflating: Dataset/data/mesh_554.ply  \n",
            "  inflating: Dataset/data/mesh_555.ply  \n",
            "  inflating: Dataset/data/mesh_560.ply  \n",
            "  inflating: Dataset/data/mesh_561.ply  \n",
            "  inflating: Dataset/data/mesh_562.ply  \n",
            "  inflating: Dataset/data/mesh_564.ply  \n",
            "  inflating: Dataset/data/mesh_565.ply  \n",
            "  inflating: Dataset/data/mesh_566.ply  \n",
            "  inflating: Dataset/data/mesh_567.ply  \n",
            "  inflating: Dataset/data/mesh_568.ply  \n",
            "  inflating: Dataset/data/mesh_569.ply  \n",
            "  inflating: Dataset/data/mesh_572.ply  \n",
            "  inflating: Dataset/data/mesh_573.ply  \n",
            "  inflating: Dataset/data/mesh_574.ply  \n",
            "  inflating: Dataset/data/mesh_576.ply  \n",
            "  inflating: Dataset/data/mesh_577.ply  \n",
            "  inflating: Dataset/data/mesh_579.ply  \n",
            "  inflating: Dataset/data/mesh_581.ply  \n",
            "  inflating: Dataset/data/mesh_582.ply  \n",
            "  inflating: Dataset/data/mesh_583.ply  \n",
            "  inflating: Dataset/data/mesh_584.ply  \n",
            "  inflating: Dataset/data/mesh_587.ply  \n",
            "  inflating: Dataset/data/mesh_588.ply  \n",
            "  inflating: Dataset/data/mesh_589.ply  \n",
            "  inflating: Dataset/data/mesh_591.ply  \n",
            "  inflating: Dataset/data/mesh_593.ply  \n",
            "  inflating: Dataset/data/mesh_594.ply  \n",
            "  inflating: Dataset/data/mesh_595.ply  \n",
            "  inflating: Dataset/data/mesh_596.ply  \n",
            "  inflating: Dataset/data/mesh_597.ply  \n",
            "  inflating: Dataset/data/mesh_598.ply  \n",
            "  inflating: Dataset/data/mesh_600.ply  \n",
            "  inflating: Dataset/data/mesh_602.ply  \n",
            "  inflating: Dataset/data/mesh_604.ply  \n",
            "  inflating: Dataset/data/mesh_608.ply  \n",
            "  inflating: Dataset/data/mesh_610.ply  \n",
            "  inflating: Dataset/data/mesh_611.ply  \n",
            "  inflating: Dataset/data/mesh_612.ply  \n",
            "  inflating: Dataset/data/mesh_613.ply  \n",
            "  inflating: Dataset/data/mesh_615.ply  \n",
            "  inflating: Dataset/data/mesh_616.ply  \n",
            "  inflating: Dataset/data/mesh_617.ply  \n",
            "  inflating: Dataset/data/mesh_618.ply  \n",
            "  inflating: Dataset/data/mesh_620.ply  \n",
            "  inflating: Dataset/data/mesh_621.ply  \n",
            "  inflating: Dataset/data/mesh_622.ply  \n",
            "  inflating: Dataset/data/mesh_623.ply  \n",
            "  inflating: Dataset/data/mesh_625.ply  \n",
            "  inflating: Dataset/data/mesh_626.ply  \n",
            "  inflating: Dataset/data/mesh_627.ply  \n",
            "  inflating: Dataset/data/mesh_628.ply  \n",
            "  inflating: Dataset/data/mesh_629.ply  \n",
            "  inflating: Dataset/data/mesh_630.ply  \n",
            "  inflating: Dataset/data/mesh_631.ply  \n",
            "  inflating: Dataset/data/mesh_632.ply  \n",
            "  inflating: Dataset/data/mesh_633.ply  \n",
            "  inflating: Dataset/data/mesh_634.ply  \n",
            "  inflating: Dataset/data/mesh_635.ply  \n",
            "  inflating: Dataset/data/mesh_636.ply  \n",
            "  inflating: Dataset/data/mesh_638.ply  \n",
            "  inflating: Dataset/data/mesh_639.ply  \n",
            "  inflating: Dataset/data/mesh_640.ply  \n",
            "  inflating: Dataset/data/mesh_641.ply  \n",
            "  inflating: Dataset/data/mesh_642.ply  \n",
            "  inflating: Dataset/data/mesh_643.ply  \n",
            "  inflating: Dataset/data/mesh_644.ply  \n",
            "  inflating: Dataset/data/mesh_645.ply  \n",
            "  inflating: Dataset/data/mesh_646.ply  \n",
            "  inflating: Dataset/data/mesh_647.ply  \n",
            "  inflating: Dataset/data/mesh_648.ply  \n",
            "  inflating: Dataset/data/mesh_649.ply  \n",
            "  inflating: Dataset/data/mesh_651.ply  \n",
            "  inflating: Dataset/data/mesh_652.ply  \n",
            "  inflating: Dataset/data/mesh_654.ply  \n",
            "  inflating: Dataset/data/mesh_655.ply  \n",
            "  inflating: Dataset/data/mesh_656.ply  \n",
            "  inflating: Dataset/data/mesh_657.ply  \n",
            "  inflating: Dataset/data/press_001.npy  \n",
            "  inflating: Dataset/data/press_002.npy  \n",
            "  inflating: Dataset/data/press_004.npy  \n",
            "  inflating: Dataset/data/press_005.npy  \n",
            "  inflating: Dataset/data/press_006.npy  \n",
            "  inflating: Dataset/data/press_007.npy  \n",
            "  inflating: Dataset/data/press_008.npy  \n",
            "  inflating: Dataset/data/press_010.npy  \n",
            "  inflating: Dataset/data/press_012.npy  \n",
            "  inflating: Dataset/data/press_013.npy  \n",
            "  inflating: Dataset/data/press_017.npy  \n",
            "  inflating: Dataset/data/press_018.npy  \n",
            "  inflating: Dataset/data/press_021.npy  \n",
            "  inflating: Dataset/data/press_022.npy  \n",
            "  inflating: Dataset/data/press_023.npy  \n",
            "  inflating: Dataset/data/press_025.npy  \n",
            "  inflating: Dataset/data/press_026.npy  \n",
            "  inflating: Dataset/data/press_027.npy  \n",
            "  inflating: Dataset/data/press_028.npy  \n",
            "  inflating: Dataset/data/press_029.npy  \n",
            "  inflating: Dataset/data/press_030.npy  \n",
            "  inflating: Dataset/data/press_031.npy  \n",
            "  inflating: Dataset/data/press_032.npy  \n",
            "  inflating: Dataset/data/press_034.npy  \n",
            "  inflating: Dataset/data/press_035.npy  \n",
            "  inflating: Dataset/data/press_039.npy  \n",
            "  inflating: Dataset/data/press_040.npy  \n",
            "  inflating: Dataset/data/press_043.npy  \n",
            "  inflating: Dataset/data/press_044.npy  \n",
            "  inflating: Dataset/data/press_045.npy  \n",
            "  inflating: Dataset/data/press_046.npy  \n",
            "  inflating: Dataset/data/press_047.npy  \n",
            "  inflating: Dataset/data/press_048.npy  \n",
            "  inflating: Dataset/data/press_049.npy  \n",
            "  inflating: Dataset/data/press_050.npy  \n",
            "  inflating: Dataset/data/press_051.npy  \n",
            "  inflating: Dataset/data/press_052.npy  \n",
            "  inflating: Dataset/data/press_054.npy  \n",
            "  inflating: Dataset/data/press_055.npy  \n",
            "  inflating: Dataset/data/press_056.npy  \n",
            "  inflating: Dataset/data/press_058.npy  \n",
            "  inflating: Dataset/data/press_059.npy  \n",
            "  inflating: Dataset/data/press_060.npy  \n",
            "  inflating: Dataset/data/press_061.npy  \n",
            "  inflating: Dataset/data/press_062.npy  \n",
            "  inflating: Dataset/data/press_063.npy  \n",
            "  inflating: Dataset/data/press_064.npy  \n",
            "  inflating: Dataset/data/press_065.npy  \n",
            "  inflating: Dataset/data/press_067.npy  \n",
            "  inflating: Dataset/data/press_069.npy  \n",
            "  inflating: Dataset/data/press_070.npy  \n",
            "  inflating: Dataset/data/press_071.npy  \n",
            "  inflating: Dataset/data/press_072.npy  \n",
            "  inflating: Dataset/data/press_073.npy  \n",
            "  inflating: Dataset/data/press_074.npy  \n",
            "  inflating: Dataset/data/press_075.npy  \n",
            "  inflating: Dataset/data/press_076.npy  \n",
            "  inflating: Dataset/data/press_077.npy  \n",
            "  inflating: Dataset/data/press_078.npy  \n",
            "  inflating: Dataset/data/press_079.npy  \n",
            "  inflating: Dataset/data/press_080.npy  \n",
            "  inflating: Dataset/data/press_081.npy  \n",
            "  inflating: Dataset/data/press_083.npy  \n",
            "  inflating: Dataset/data/press_084.npy  \n",
            "  inflating: Dataset/data/press_085.npy  \n",
            "  inflating: Dataset/data/press_086.npy  \n",
            "  inflating: Dataset/data/press_087.npy  \n",
            "  inflating: Dataset/data/press_088.npy  \n",
            "  inflating: Dataset/data/press_090.npy  \n",
            "  inflating: Dataset/data/press_091.npy  \n",
            "  inflating: Dataset/data/press_092.npy  \n",
            "  inflating: Dataset/data/press_094.npy  \n",
            "  inflating: Dataset/data/press_095.npy  \n",
            "  inflating: Dataset/data/press_096.npy  \n",
            "  inflating: Dataset/data/press_097.npy  \n",
            "  inflating: Dataset/data/press_100.npy  \n",
            "  inflating: Dataset/data/press_101.npy  \n",
            "  inflating: Dataset/data/press_102.npy  \n",
            "  inflating: Dataset/data/press_105.npy  \n",
            "  inflating: Dataset/data/press_106.npy  \n",
            "  inflating: Dataset/data/press_107.npy  \n",
            "  inflating: Dataset/data/press_109.npy  \n",
            "  inflating: Dataset/data/press_110.npy  \n",
            "  inflating: Dataset/data/press_111.npy  \n",
            "  inflating: Dataset/data/press_112.npy  \n",
            "  inflating: Dataset/data/press_113.npy  \n",
            "  inflating: Dataset/data/press_114.npy  \n",
            "  inflating: Dataset/data/press_115.npy  \n",
            "  inflating: Dataset/data/press_116.npy  \n",
            "  inflating: Dataset/data/press_117.npy  \n",
            "  inflating: Dataset/data/press_118.npy  \n",
            "  inflating: Dataset/data/press_119.npy  \n",
            "  inflating: Dataset/data/press_120.npy  \n",
            "  inflating: Dataset/data/press_121.npy  \n",
            "  inflating: Dataset/data/press_123.npy  \n",
            "  inflating: Dataset/data/press_124.npy  \n",
            "  inflating: Dataset/data/press_125.npy  \n",
            "  inflating: Dataset/data/press_126.npy  \n",
            "  inflating: Dataset/data/press_127.npy  \n",
            "  inflating: Dataset/data/press_128.npy  \n",
            "  inflating: Dataset/data/press_129.npy  \n",
            "  inflating: Dataset/data/press_130.npy  \n",
            "  inflating: Dataset/data/press_131.npy  \n",
            "  inflating: Dataset/data/press_133.npy  \n",
            "  inflating: Dataset/data/press_134.npy  \n",
            "  inflating: Dataset/data/press_136.npy  \n",
            "  inflating: Dataset/data/press_137.npy  \n",
            "  inflating: Dataset/data/press_138.npy  \n",
            "  inflating: Dataset/data/press_139.npy  \n",
            "  inflating: Dataset/data/press_140.npy  \n",
            "  inflating: Dataset/data/press_141.npy  \n",
            "  inflating: Dataset/data/press_142.npy  \n",
            "  inflating: Dataset/data/press_143.npy  \n",
            "  inflating: Dataset/data/press_144.npy  \n",
            "  inflating: Dataset/data/press_145.npy  \n",
            "  inflating: Dataset/data/press_146.npy  \n",
            "  inflating: Dataset/data/press_147.npy  \n",
            "  inflating: Dataset/data/press_148.npy  \n",
            "  inflating: Dataset/data/press_149.npy  \n",
            "  inflating: Dataset/data/press_150.npy  \n",
            "  inflating: Dataset/data/press_151.npy  \n",
            "  inflating: Dataset/data/press_152.npy  \n",
            "  inflating: Dataset/data/press_153.npy  \n",
            "  inflating: Dataset/data/press_155.npy  \n",
            "  inflating: Dataset/data/press_156.npy  \n",
            "  inflating: Dataset/data/press_157.npy  \n",
            "  inflating: Dataset/data/press_158.npy  \n",
            "  inflating: Dataset/data/press_159.npy  \n",
            "  inflating: Dataset/data/press_160.npy  \n",
            "  inflating: Dataset/data/press_161.npy  \n",
            "  inflating: Dataset/data/press_162.npy  \n",
            "  inflating: Dataset/data/press_163.npy  \n",
            "  inflating: Dataset/data/press_165.npy  \n",
            "  inflating: Dataset/data/press_166.npy  \n",
            "  inflating: Dataset/data/press_170.npy  \n",
            "  inflating: Dataset/data/press_172.npy  \n",
            "  inflating: Dataset/data/press_173.npy  \n",
            "  inflating: Dataset/data/press_175.npy  \n",
            "  inflating: Dataset/data/press_176.npy  \n",
            "  inflating: Dataset/data/press_177.npy  \n",
            "  inflating: Dataset/data/press_178.npy  \n",
            "  inflating: Dataset/data/press_179.npy  \n",
            "  inflating: Dataset/data/press_180.npy  \n",
            "  inflating: Dataset/data/press_181.npy  \n",
            "  inflating: Dataset/data/press_182.npy  \n",
            "  inflating: Dataset/data/press_183.npy  \n",
            "  inflating: Dataset/data/press_184.npy  \n",
            "  inflating: Dataset/data/press_186.npy  \n",
            "  inflating: Dataset/data/press_190.npy  \n",
            "  inflating: Dataset/data/press_191.npy  \n",
            "  inflating: Dataset/data/press_192.npy  \n",
            "  inflating: Dataset/data/press_193.npy  \n",
            "  inflating: Dataset/data/press_195.npy  \n",
            "  inflating: Dataset/data/press_196.npy  \n",
            "  inflating: Dataset/data/press_198.npy  \n",
            "  inflating: Dataset/data/press_199.npy  \n",
            "  inflating: Dataset/data/press_200.npy  \n",
            "  inflating: Dataset/data/press_201.npy  \n",
            "  inflating: Dataset/data/press_202.npy  \n",
            "  inflating: Dataset/data/press_203.npy  \n",
            "  inflating: Dataset/data/press_205.npy  \n",
            "  inflating: Dataset/data/press_207.npy  \n",
            "  inflating: Dataset/data/press_210.npy  \n",
            "  inflating: Dataset/data/press_211.npy  \n",
            "  inflating: Dataset/data/press_212.npy  \n",
            "  inflating: Dataset/data/press_213.npy  \n",
            "  inflating: Dataset/data/press_214.npy  \n",
            "  inflating: Dataset/data/press_215.npy  \n",
            "  inflating: Dataset/data/press_217.npy  \n",
            "  inflating: Dataset/data/press_219.npy  \n",
            "  inflating: Dataset/data/press_220.npy  \n",
            "  inflating: Dataset/data/press_221.npy  \n",
            "  inflating: Dataset/data/press_222.npy  \n",
            "  inflating: Dataset/data/press_223.npy  \n",
            "  inflating: Dataset/data/press_224.npy  \n",
            "  inflating: Dataset/data/press_225.npy  \n",
            "  inflating: Dataset/data/press_227.npy  \n",
            "  inflating: Dataset/data/press_228.npy  \n",
            "  inflating: Dataset/data/press_229.npy  \n",
            "  inflating: Dataset/data/press_230.npy  \n",
            "  inflating: Dataset/data/press_231.npy  \n",
            "  inflating: Dataset/data/press_232.npy  \n",
            "  inflating: Dataset/data/press_233.npy  \n",
            "  inflating: Dataset/data/press_234.npy  \n",
            "  inflating: Dataset/data/press_235.npy  \n",
            "  inflating: Dataset/data/press_236.npy  \n",
            "  inflating: Dataset/data/press_237.npy  \n",
            "  inflating: Dataset/data/press_241.npy  \n",
            "  inflating: Dataset/data/press_243.npy  \n",
            "  inflating: Dataset/data/press_244.npy  \n",
            "  inflating: Dataset/data/press_245.npy  \n",
            "  inflating: Dataset/data/press_246.npy  \n",
            "  inflating: Dataset/data/press_247.npy  \n",
            "  inflating: Dataset/data/press_248.npy  \n",
            "  inflating: Dataset/data/press_249.npy  \n",
            "  inflating: Dataset/data/press_251.npy  \n",
            "  inflating: Dataset/data/press_252.npy  \n",
            "  inflating: Dataset/data/press_253.npy  \n",
            "  inflating: Dataset/data/press_255.npy  \n",
            "  inflating: Dataset/data/press_257.npy  \n",
            "  inflating: Dataset/data/press_258.npy  \n",
            "  inflating: Dataset/data/press_259.npy  \n",
            "  inflating: Dataset/data/press_260.npy  \n",
            "  inflating: Dataset/data/press_261.npy  \n",
            "  inflating: Dataset/data/press_262.npy  \n",
            "  inflating: Dataset/data/press_263.npy  \n",
            "  inflating: Dataset/data/press_264.npy  \n",
            "  inflating: Dataset/data/press_266.npy  \n",
            "  inflating: Dataset/data/press_267.npy  \n",
            "  inflating: Dataset/data/press_268.npy  \n",
            "  inflating: Dataset/data/press_269.npy  \n",
            "  inflating: Dataset/data/press_271.npy  \n",
            "  inflating: Dataset/data/press_272.npy  \n",
            "  inflating: Dataset/data/press_273.npy  \n",
            "  inflating: Dataset/data/press_274.npy  \n",
            "  inflating: Dataset/data/press_275.npy  \n",
            "  inflating: Dataset/data/press_276.npy  \n",
            "  inflating: Dataset/data/press_277.npy  \n",
            "  inflating: Dataset/data/press_278.npy  \n",
            "  inflating: Dataset/data/press_279.npy  \n",
            "  inflating: Dataset/data/press_280.npy  \n",
            "  inflating: Dataset/data/press_281.npy  \n",
            "  inflating: Dataset/data/press_282.npy  \n",
            "  inflating: Dataset/data/press_283.npy  \n",
            "  inflating: Dataset/data/press_285.npy  \n",
            "  inflating: Dataset/data/press_286.npy  \n",
            "  inflating: Dataset/data/press_289.npy  \n",
            "  inflating: Dataset/data/press_290.npy  \n",
            "  inflating: Dataset/data/press_291.npy  \n",
            "  inflating: Dataset/data/press_292.npy  \n",
            "  inflating: Dataset/data/press_293.npy  \n",
            "  inflating: Dataset/data/press_294.npy  \n",
            "  inflating: Dataset/data/press_295.npy  \n",
            "  inflating: Dataset/data/press_296.npy  \n",
            "  inflating: Dataset/data/press_297.npy  \n",
            "  inflating: Dataset/data/press_298.npy  \n",
            "  inflating: Dataset/data/press_299.npy  \n",
            "  inflating: Dataset/data/press_300.npy  \n",
            "  inflating: Dataset/data/press_301.npy  \n",
            "  inflating: Dataset/data/press_302.npy  \n",
            "  inflating: Dataset/data/press_304.npy  \n",
            "  inflating: Dataset/data/press_305.npy  \n",
            "  inflating: Dataset/data/press_306.npy  \n",
            "  inflating: Dataset/data/press_308.npy  \n",
            "  inflating: Dataset/data/press_309.npy  \n",
            "  inflating: Dataset/data/press_310.npy  \n",
            "  inflating: Dataset/data/press_311.npy  \n",
            "  inflating: Dataset/data/press_312.npy  \n",
            "  inflating: Dataset/data/press_313.npy  \n",
            "  inflating: Dataset/data/press_314.npy  \n",
            "  inflating: Dataset/data/press_315.npy  \n",
            "  inflating: Dataset/data/press_319.npy  \n",
            "  inflating: Dataset/data/press_320.npy  \n",
            "  inflating: Dataset/data/press_321.npy  \n",
            "  inflating: Dataset/data/press_322.npy  \n",
            "  inflating: Dataset/data/press_323.npy  \n",
            "  inflating: Dataset/data/press_324.npy  \n",
            "  inflating: Dataset/data/press_325.npy  \n",
            "  inflating: Dataset/data/press_327.npy  \n",
            "  inflating: Dataset/data/press_328.npy  \n",
            "  inflating: Dataset/data/press_329.npy  \n",
            "  inflating: Dataset/data/press_331.npy  \n",
            "  inflating: Dataset/data/press_332.npy  \n",
            "  inflating: Dataset/data/press_333.npy  \n",
            "  inflating: Dataset/data/press_334.npy  \n",
            "  inflating: Dataset/data/press_335.npy  \n",
            "  inflating: Dataset/data/press_337.npy  \n",
            "  inflating: Dataset/data/press_338.npy  \n",
            "  inflating: Dataset/data/press_339.npy  \n",
            "  inflating: Dataset/data/press_340.npy  \n",
            "  inflating: Dataset/data/press_341.npy  \n",
            "  inflating: Dataset/data/press_344.npy  \n",
            "  inflating: Dataset/data/press_345.npy  \n",
            "  inflating: Dataset/data/press_347.npy  \n",
            "  inflating: Dataset/data/press_348.npy  \n",
            "  inflating: Dataset/data/press_349.npy  \n",
            "  inflating: Dataset/data/press_350.npy  \n",
            "  inflating: Dataset/data/press_352.npy  \n",
            "  inflating: Dataset/data/press_353.npy  \n",
            "  inflating: Dataset/data/press_354.npy  \n",
            "  inflating: Dataset/data/press_355.npy  \n",
            "  inflating: Dataset/data/press_356.npy  \n",
            "  inflating: Dataset/data/press_357.npy  \n",
            "  inflating: Dataset/data/press_358.npy  \n",
            "  inflating: Dataset/data/press_360.npy  \n",
            "  inflating: Dataset/data/press_362.npy  \n",
            "  inflating: Dataset/data/press_364.npy  \n",
            "  inflating: Dataset/data/press_365.npy  \n",
            "  inflating: Dataset/data/press_366.npy  \n",
            "  inflating: Dataset/data/press_367.npy  \n",
            "  inflating: Dataset/data/press_369.npy  \n",
            "  inflating: Dataset/data/press_371.npy  \n",
            "  inflating: Dataset/data/press_372.npy  \n",
            "  inflating: Dataset/data/press_373.npy  \n",
            "  inflating: Dataset/data/press_374.npy  \n",
            "  inflating: Dataset/data/press_375.npy  \n",
            "  inflating: Dataset/data/press_376.npy  \n",
            "  inflating: Dataset/data/press_378.npy  \n",
            "  inflating: Dataset/data/press_379.npy  \n",
            "  inflating: Dataset/data/press_380.npy  \n",
            "  inflating: Dataset/data/press_381.npy  \n",
            "  inflating: Dataset/data/press_384.npy  \n",
            "  inflating: Dataset/data/press_385.npy  \n",
            "  inflating: Dataset/data/press_389.npy  \n",
            "  inflating: Dataset/data/press_392.npy  \n",
            "  inflating: Dataset/data/press_393.npy  \n",
            "  inflating: Dataset/data/press_397.npy  \n",
            "  inflating: Dataset/data/press_398.npy  \n",
            "  inflating: Dataset/data/press_399.npy  \n",
            "  inflating: Dataset/data/press_401.npy  \n",
            "  inflating: Dataset/data/press_402.npy  \n",
            "  inflating: Dataset/data/press_403.npy  \n",
            "  inflating: Dataset/data/press_404.npy  \n",
            "  inflating: Dataset/data/press_405.npy  \n",
            "  inflating: Dataset/data/press_407.npy  \n",
            "  inflating: Dataset/data/press_408.npy  \n",
            "  inflating: Dataset/data/press_410.npy  \n",
            "  inflating: Dataset/data/press_412.npy  \n",
            "  inflating: Dataset/data/press_413.npy  \n",
            "  inflating: Dataset/data/press_414.npy  \n",
            "  inflating: Dataset/data/press_415.npy  \n",
            "  inflating: Dataset/data/press_417.npy  \n",
            "  inflating: Dataset/data/press_418.npy  \n",
            "  inflating: Dataset/data/press_419.npy  \n",
            "  inflating: Dataset/data/press_420.npy  \n",
            "  inflating: Dataset/data/press_422.npy  \n",
            "  inflating: Dataset/data/press_424.npy  \n",
            "  inflating: Dataset/data/press_425.npy  \n",
            "  inflating: Dataset/data/press_427.npy  \n",
            "  inflating: Dataset/data/press_430.npy  \n",
            "  inflating: Dataset/data/press_431.npy  \n",
            "  inflating: Dataset/data/press_433.npy  \n",
            "  inflating: Dataset/data/press_435.npy  \n",
            "  inflating: Dataset/data/press_436.npy  \n",
            "  inflating: Dataset/data/press_437.npy  \n",
            "  inflating: Dataset/data/press_439.npy  \n",
            "  inflating: Dataset/data/press_440.npy  \n",
            "  inflating: Dataset/data/press_443.npy  \n",
            "  inflating: Dataset/data/press_444.npy  \n",
            "  inflating: Dataset/data/press_446.npy  \n",
            "  inflating: Dataset/data/press_447.npy  \n",
            "  inflating: Dataset/data/press_448.npy  \n",
            "  inflating: Dataset/data/press_449.npy  \n",
            "  inflating: Dataset/data/press_450.npy  \n",
            "  inflating: Dataset/data/press_451.npy  \n",
            "  inflating: Dataset/data/press_452.npy  \n",
            "  inflating: Dataset/data/press_453.npy  \n",
            "  inflating: Dataset/data/press_454.npy  \n",
            "  inflating: Dataset/data/press_455.npy  \n",
            "  inflating: Dataset/data/press_456.npy  \n",
            "  inflating: Dataset/data/press_457.npy  \n",
            "  inflating: Dataset/data/press_459.npy  \n",
            "  inflating: Dataset/data/press_460.npy  \n",
            "  inflating: Dataset/data/press_462.npy  \n",
            "  inflating: Dataset/data/press_463.npy  \n",
            "  inflating: Dataset/data/press_464.npy  \n",
            "  inflating: Dataset/data/press_465.npy  \n",
            "  inflating: Dataset/data/press_466.npy  \n",
            "  inflating: Dataset/data/press_467.npy  \n",
            "  inflating: Dataset/data/press_468.npy  \n",
            "  inflating: Dataset/data/press_469.npy  \n",
            "  inflating: Dataset/data/press_470.npy  \n",
            "  inflating: Dataset/data/press_472.npy  \n",
            "  inflating: Dataset/data/press_473.npy  \n",
            "  inflating: Dataset/data/press_474.npy  \n",
            "  inflating: Dataset/data/press_475.npy  \n",
            "  inflating: Dataset/data/press_476.npy  \n",
            "  inflating: Dataset/data/press_478.npy  \n",
            "  inflating: Dataset/data/press_479.npy  \n",
            "  inflating: Dataset/data/press_480.npy  \n",
            "  inflating: Dataset/data/press_482.npy  \n",
            "  inflating: Dataset/data/press_483.npy  \n",
            "  inflating: Dataset/data/press_486.npy  \n",
            "  inflating: Dataset/data/press_487.npy  \n",
            "  inflating: Dataset/data/press_488.npy  \n",
            "  inflating: Dataset/data/press_490.npy  \n",
            "  inflating: Dataset/data/press_493.npy  \n",
            "  inflating: Dataset/data/press_494.npy  \n",
            "  inflating: Dataset/data/press_495.npy  \n",
            "  inflating: Dataset/data/press_496.npy  \n",
            "  inflating: Dataset/data/press_497.npy  \n",
            "  inflating: Dataset/data/press_498.npy  \n",
            "  inflating: Dataset/data/press_499.npy  \n",
            "  inflating: Dataset/data/press_501.npy  \n",
            "  inflating: Dataset/data/press_502.npy  \n",
            "  inflating: Dataset/data/press_503.npy  \n",
            "  inflating: Dataset/data/press_504.npy  \n",
            "  inflating: Dataset/data/press_505.npy  \n",
            "  inflating: Dataset/data/press_507.npy  \n",
            "  inflating: Dataset/data/press_508.npy  \n",
            "  inflating: Dataset/data/press_509.npy  \n",
            "  inflating: Dataset/data/press_511.npy  \n",
            "  inflating: Dataset/data/press_512.npy  \n",
            "  inflating: Dataset/data/press_513.npy  \n",
            "  inflating: Dataset/data/press_514.npy  \n",
            "  inflating: Dataset/data/press_515.npy  \n",
            "  inflating: Dataset/data/press_516.npy  \n",
            "  inflating: Dataset/data/press_518.npy  \n",
            "  inflating: Dataset/data/press_519.npy  \n",
            "  inflating: Dataset/data/press_521.npy  \n",
            "  inflating: Dataset/data/press_522.npy  \n",
            "  inflating: Dataset/data/press_523.npy  \n",
            "  inflating: Dataset/data/press_524.npy  \n",
            "  inflating: Dataset/data/press_525.npy  \n",
            "  inflating: Dataset/data/press_527.npy  \n",
            "  inflating: Dataset/data/press_529.npy  \n",
            "  inflating: Dataset/data/press_530.npy  \n",
            "  inflating: Dataset/data/press_532.npy  \n",
            "  inflating: Dataset/data/press_533.npy  \n",
            "  inflating: Dataset/data/press_536.npy  \n",
            "  inflating: Dataset/data/press_538.npy  \n",
            "  inflating: Dataset/data/press_539.npy  \n",
            "  inflating: Dataset/data/press_540.npy  \n",
            "  inflating: Dataset/data/press_542.npy  \n",
            "  inflating: Dataset/data/press_543.npy  \n",
            "  inflating: Dataset/data/press_545.npy  \n",
            "  inflating: Dataset/data/press_547.npy  \n",
            "  inflating: Dataset/data/press_548.npy  \n",
            "  inflating: Dataset/data/press_549.npy  \n",
            "  inflating: Dataset/data/press_550.npy  \n",
            "  inflating: Dataset/data/press_551.npy  \n",
            "  inflating: Dataset/data/press_552.npy  \n",
            "  inflating: Dataset/data/press_553.npy  \n",
            "  inflating: Dataset/data/press_554.npy  \n",
            "  inflating: Dataset/data/press_555.npy  \n",
            "  inflating: Dataset/data/press_560.npy  \n",
            "  inflating: Dataset/data/press_561.npy  \n",
            "  inflating: Dataset/data/press_562.npy  \n",
            "  inflating: Dataset/data/press_564.npy  \n",
            "  inflating: Dataset/data/press_565.npy  \n",
            "  inflating: Dataset/data/press_566.npy  \n",
            "  inflating: Dataset/data/press_567.npy  \n",
            "  inflating: Dataset/data/press_568.npy  \n",
            "  inflating: Dataset/data/press_569.npy  \n",
            "  inflating: Dataset/data/press_572.npy  \n",
            "  inflating: Dataset/data/press_573.npy  \n",
            "  inflating: Dataset/data/press_574.npy  \n",
            "  inflating: Dataset/data/press_576.npy  \n",
            "  inflating: Dataset/data/press_577.npy  \n",
            "  inflating: Dataset/data/press_579.npy  \n",
            "  inflating: Dataset/data/press_581.npy  \n",
            "  inflating: Dataset/data/press_582.npy  \n",
            "  inflating: Dataset/data/press_583.npy  \n",
            "  inflating: Dataset/data/press_584.npy  \n",
            "  inflating: Dataset/data/press_587.npy  \n",
            "  inflating: Dataset/data/press_588.npy  \n",
            "  inflating: Dataset/data/press_589.npy  \n",
            "  inflating: Dataset/data/press_591.npy  \n",
            "  inflating: Dataset/data/press_593.npy  \n",
            "  inflating: Dataset/data/press_594.npy  \n",
            "  inflating: Dataset/data/press_595.npy  \n",
            "  inflating: Dataset/data/press_596.npy  \n",
            "  inflating: Dataset/data/press_597.npy  \n",
            "  inflating: Dataset/data/press_598.npy  \n",
            "  inflating: Dataset/data/press_600.npy  \n",
            "  inflating: Dataset/data/press_602.npy  \n",
            "  inflating: Dataset/data/press_604.npy  \n",
            "  inflating: Dataset/data/press_608.npy  \n",
            "  inflating: Dataset/data/press_610.npy  \n",
            "  inflating: Dataset/data/press_611.npy  \n",
            "  inflating: Dataset/data/press_612.npy  \n",
            "  inflating: Dataset/data/press_613.npy  \n",
            "  inflating: Dataset/data/press_615.npy  \n",
            "  inflating: Dataset/data/press_616.npy  \n",
            "  inflating: Dataset/data/press_617.npy  \n",
            "  inflating: Dataset/data/press_618.npy  \n",
            "  inflating: Dataset/data/press_620.npy  \n",
            "  inflating: Dataset/data/press_621.npy  \n",
            "  inflating: Dataset/data/press_622.npy  \n",
            "  inflating: Dataset/data/press_623.npy  \n",
            "  inflating: Dataset/data/press_625.npy  \n",
            "  inflating: Dataset/data/press_626.npy  \n",
            "  inflating: Dataset/data/press_627.npy  \n",
            "  inflating: Dataset/data/press_628.npy  \n",
            "  inflating: Dataset/data/press_629.npy  \n",
            "  inflating: Dataset/data/press_630.npy  \n",
            "  inflating: Dataset/data/press_631.npy  \n",
            "  inflating: Dataset/data/press_632.npy  \n",
            "  inflating: Dataset/data/press_633.npy  \n",
            "  inflating: Dataset/data/press_634.npy  \n",
            "  inflating: Dataset/data/press_635.npy  \n",
            "  inflating: Dataset/data/press_636.npy  \n",
            "  inflating: Dataset/data/press_638.npy  \n",
            "  inflating: Dataset/data/press_639.npy  \n",
            "  inflating: Dataset/data/press_640.npy  \n",
            "  inflating: Dataset/data/press_641.npy  \n",
            "  inflating: Dataset/data/press_642.npy  \n",
            "  inflating: Dataset/data/press_643.npy  \n",
            "  inflating: Dataset/data/press_644.npy  \n",
            "  inflating: Dataset/data/press_645.npy  \n",
            "  inflating: Dataset/data/press_646.npy  \n",
            "  inflating: Dataset/data/press_647.npy  \n",
            "  inflating: Dataset/data/press_648.npy  \n",
            "  inflating: Dataset/data/press_649.npy  \n",
            "  inflating: Dataset/data/press_651.npy  \n",
            "  inflating: Dataset/data/press_652.npy  \n",
            "  inflating: Dataset/data/press_654.npy  \n",
            "  inflating: Dataset/data/press_655.npy  \n",
            "  inflating: Dataset/data/press_656.npy  \n",
            "  inflating: Dataset/data/press_657.npy  \n",
            " extracting: Dataset/data/train_pressure_min_std.txt  \n",
            "  inflating: Dataset/data/watertight_global_bounds.txt  \n",
            "  inflating: Dataset/data/watertight_meshes.txt  \n",
            "Archive:  track_A.zip\n",
            "  inflating: Dataset/track_A/mesh_658.ply  \n",
            "  inflating: Dataset/track_A/mesh_659.ply  \n",
            "  inflating: Dataset/track_A/mesh_660.ply  \n",
            "  inflating: Dataset/track_A/mesh_661.ply  \n",
            "  inflating: Dataset/track_A/mesh_662.ply  \n",
            "  inflating: Dataset/track_A/mesh_663.ply  \n",
            "  inflating: Dataset/track_A/mesh_664.ply  \n",
            "  inflating: Dataset/track_A/mesh_665.ply  \n",
            "  inflating: Dataset/track_A/mesh_666.ply  \n",
            "  inflating: Dataset/track_A/mesh_667.ply  \n",
            "  inflating: Dataset/track_A/mesh_668.ply  \n",
            "  inflating: Dataset/track_A/mesh_669.ply  \n",
            "  inflating: Dataset/track_A/mesh_670.ply  \n",
            "  inflating: Dataset/track_A/mesh_671.ply  \n",
            "  inflating: Dataset/track_A/mesh_672.ply  \n",
            "  inflating: Dataset/track_A/mesh_673.ply  \n",
            "  inflating: Dataset/track_A/mesh_674.ply  \n",
            "  inflating: Dataset/track_A/mesh_675.ply  \n",
            "  inflating: Dataset/track_A/mesh_676.ply  \n",
            "  inflating: Dataset/track_A/mesh_677.ply  \n",
            "  inflating: Dataset/track_A/mesh_678.ply  \n",
            "  inflating: Dataset/track_A/mesh_679.ply  \n",
            "  inflating: Dataset/track_A/mesh_680.ply  \n",
            "  inflating: Dataset/track_A/mesh_681.ply  \n",
            "  inflating: Dataset/track_A/mesh_682.ply  \n",
            "  inflating: Dataset/track_A/mesh_683.ply  \n",
            "  inflating: Dataset/track_A/mesh_684.ply  \n",
            "  inflating: Dataset/track_A/mesh_685.ply  \n",
            "  inflating: Dataset/track_A/mesh_686.ply  \n",
            "  inflating: Dataset/track_A/mesh_687.ply  \n",
            "  inflating: Dataset/track_A/mesh_688.ply  \n",
            "  inflating: Dataset/track_A/mesh_689.ply  \n",
            "  inflating: Dataset/track_A/mesh_690.ply  \n",
            "  inflating: Dataset/track_A/mesh_691.ply  \n",
            "  inflating: Dataset/track_A/mesh_692.ply  \n",
            "  inflating: Dataset/track_A/mesh_693.ply  \n",
            "  inflating: Dataset/track_A/mesh_694.ply  \n",
            "  inflating: Dataset/track_A/mesh_695.ply  \n",
            "  inflating: Dataset/track_A/mesh_696.ply  \n",
            "  inflating: Dataset/track_A/mesh_697.ply  \n",
            "  inflating: Dataset/track_A/mesh_698.ply  \n",
            "  inflating: Dataset/track_A/mesh_699.ply  \n",
            "  inflating: Dataset/track_A/mesh_700.ply  \n",
            "  inflating: Dataset/track_A/mesh_701.ply  \n",
            "  inflating: Dataset/track_A/mesh_702.ply  \n",
            "  inflating: Dataset/track_A/mesh_703.ply  \n",
            "  inflating: Dataset/track_A/mesh_704.ply  \n",
            "  inflating: Dataset/track_A/mesh_705.ply  \n",
            "  inflating: Dataset/track_A/mesh_706.ply  \n",
            "  inflating: Dataset/track_A/mesh_707.ply  \n",
            "  inflating: Dataset/track_A/mesh_708.ply  \n",
            "  inflating: Dataset/track_A/mesh_709.ply  \n",
            "  inflating: Dataset/track_A/mesh_710.ply  \n",
            "  inflating: Dataset/track_A/mesh_711.ply  \n",
            "  inflating: Dataset/track_A/mesh_712.ply  \n",
            "  inflating: Dataset/track_A/mesh_713.ply  \n",
            "  inflating: Dataset/track_A/mesh_714.ply  \n",
            "  inflating: Dataset/track_A/mesh_715.ply  \n",
            "  inflating: Dataset/track_A/mesh_716.ply  \n",
            "  inflating: Dataset/track_A/mesh_717.ply  \n",
            "  inflating: Dataset/track_A/mesh_718.ply  \n",
            "  inflating: Dataset/track_A/mesh_719.ply  \n",
            "  inflating: Dataset/track_A/mesh_720.ply  \n",
            "  inflating: Dataset/track_A/mesh_721.ply  \n",
            "  inflating: Dataset/track_A/mesh_722.ply  \n",
            "  inflating: Dataset/track_A/watertight_meshes.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 数据处理"
      ],
      "metadata": {
        "id": "-Vj68rbBEKES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下代码是将数据处理成官方文件夹的形式，单纯文件移动操作，不引入新的数据"
      ],
      "metadata": {
        "id": "Z9z9-NNU3F46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# 指定 train_data 目录路径\n",
        "path = 'Dataset/data'\n",
        "\n",
        "# 新建 Feature 和 Label 文件夹\n",
        "Feature_path = os.path.join(path, 'Feature')\n",
        "Label_path = os.path.join(path, 'Label')\n",
        "os.makedirs(Feature_path, exist_ok=True)\n",
        "os.makedirs(Label_path, exist_ok=True)\n",
        "\n",
        "# 遍历目录中的文件\n",
        "for filename in os.listdir(path):\n",
        "    # 跳过目录，只处理文件\n",
        "    if os.path.isfile(os.path.join(path, filename)):\n",
        "        # 获取文件名前缀\n",
        "        prefix = filename.split('_')[0]\n",
        "        # 检查文件名前缀是否是 area, info, normal 或 centroid\n",
        "        if prefix in ['mesh']:\n",
        "            # 构建源文件和目标文件路径\n",
        "            src_file = os.path.join(path, filename)\n",
        "            dst_file = os.path.join(Feature_path, filename)\n",
        "            # 移动文件\n",
        "            shutil.move(src_file, dst_file)\n",
        "            # print(f'Moved {filename} to {Feature_path}')\n",
        "        elif prefix in ['press']:\n",
        "            # 构建源文件和目标文件路径\n",
        "            src_file = os.path.join(path, filename)\n",
        "            dst_file = os.path.join(Label_path, filename)\n",
        "            # 移动文件\n",
        "            shutil.move(src_file, dst_file)\n",
        "            # print(f'Moved {filename} to {Label_path}')"
      ],
      "metadata": {
        "id": "PCb-JOgEWC-u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# 指定 track_a 目录路径\n",
        "track_a_path = 'Dataset/track_A'\n",
        "\n",
        "# 新建Inference 文件夹\n",
        "\n",
        "inference_path = os.path.join(track_a_path, 'Inference')\n",
        "\n",
        "os.makedirs(inference_path, exist_ok=True)\n",
        "\n",
        "# 遍历 track_b 目录中的文件\n",
        "for filename in os.listdir(track_a_path):\n",
        "    # 跳过目录，只处理文件\n",
        "    if os.path.isfile(os.path.join(track_a_path, filename)):\n",
        "        # 获取文件名前缀\n",
        "        prefix = filename.split('_')[0]\n",
        "        # 检查文件名前缀是否是 area, info, normal 或 centroid\n",
        "        if prefix in ['mesh']:\n",
        "            # 构建源文件和目标文件路径\n",
        "            src_file = os.path.join(track_a_path, filename)\n",
        "            dst_file = os.path.join(inference_path, filename)\n",
        "            # 移动文件\n",
        "            shutil.move(src_file, dst_file)\n",
        "            # print(f'Moved {filename} to {inference_path}')\n"
      ],
      "metadata": {
        "id": "coE0Qu-YU8pX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "VV4alT_R22se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding:utf-8 _*-\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import dgl\n",
        "from einops import repeat, rearrange\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import GELU, ReLU, Tanh, Sigmoid\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# from utils import MultipleTensors\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "ACTIVATION = {'gelu':nn.GELU(),'tanh':nn.Tanh(),'sigmoid':nn.Sigmoid(),'relu':nn.ReLU(),'leaky_relu':nn.LeakyReLU(0.1),'softplus':nn.Softplus(),'ELU':nn.ELU()}\n",
        "\n",
        "'''\n",
        "    A simple MLP class, includes at least 2 layers and n hidden layers\n",
        "'''\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, n_input, n_hidden, n_output, n_layers=1, act='gelu'):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        if act in ACTIVATION.keys():\n",
        "            self.act = ACTIVATION[act]\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        self.n_input = n_input\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = n_output\n",
        "        self.n_layers = n_layers\n",
        "        self.linear_pre = nn.Linear(n_input, n_hidden)\n",
        "        self.linear_post = nn.Linear(n_hidden, n_output)\n",
        "        self.linears = nn.ModuleList([nn.Linear(n_hidden, n_hidden) for _ in range(n_layers)])\n",
        "\n",
        "        # self.bns = nn.ModuleList([nn.BatchNorm1d(n_hidden) for _ in range(n_layers)])\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.linear_pre(x))\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.act(self.linears[i](x)) + x\n",
        "            # x = self.act(self.bns[i](self.linears[i](x))) + x\n",
        "\n",
        "        x = self.linear_post(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class MultipleTensors():\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "\n",
        "    def to(self, device):\n",
        "        self.x = [x_.to(device) for x_ in self.x]\n",
        "        return self\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.x[item]\n",
        "\n",
        "\n",
        "class MoEGPTConfig():\n",
        "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
        "\n",
        "    def __init__(self, attn_type='linear', embd_pdrop=0.0, resid_pdrop=0.0, attn_pdrop=0.0, n_embd=128, n_head=1,\n",
        "                 n_layer=3, block_size=128, n_inner=4, act='gelu', n_experts=2, space_dim=1, branch_sizes=None,\n",
        "                 n_inputs=1):\n",
        "        self.attn_type = attn_type\n",
        "        self.embd_pdrop = embd_pdrop\n",
        "        self.resid_pdrop = resid_pdrop\n",
        "        self.attn_pdrop = attn_pdrop\n",
        "        self.n_embd = n_embd  # 64\n",
        "        self.n_head = n_head\n",
        "        self.n_layer = n_layer\n",
        "        self.block_size = block_size\n",
        "        self.n_inner = n_inner * self.n_embd\n",
        "        self.act = act\n",
        "        self.n_experts = n_experts\n",
        "        self.space_dim = space_dim\n",
        "        self.branch_sizes = branch_sizes\n",
        "        self.n_inputs = n_inputs\n",
        "\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(LinearAttention, self).__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "\n",
        "        self.attn_type = 'l1'\n",
        "\n",
        "    '''\n",
        "        Linear Attention and Linear Cross Attention (if y is provided)\n",
        "    '''\n",
        "\n",
        "    def forward(self, x, y=None, layer_past=None):\n",
        "        y = x if y is None else y\n",
        "        B, T1, C = x.size()\n",
        "        _, T2, _ = y.size()\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q = self.query(x).view(B, T1, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        k = self.key(y).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        v = self.value(y).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "\n",
        "        if self.attn_type == 'l1':\n",
        "            q = q.softmax(dim=-1)\n",
        "            k = k.softmax(dim=-1)  #\n",
        "            k_cumsum = k.sum(dim=-2, keepdim=True)\n",
        "            D_inv = 1. / (q * k_cumsum).sum(dim=-1, keepdim=True)  # normalized\n",
        "        elif self.attn_type == \"galerkin\":\n",
        "            q = q.softmax(dim=-1)\n",
        "            k = k.softmax(dim=-1)  #\n",
        "            D_inv = 1. / T2  # galerkin\n",
        "        elif self.attn_type == \"l2\":  # still use l1 normalization\n",
        "            q = q / q.norm(dim=-1, keepdim=True, p=1)\n",
        "            k = k / k.norm(dim=-1, keepdim=True, p=1)\n",
        "            k_cumsum = k.sum(dim=-2, keepdim=True)\n",
        "            D_inv = 1. / (q * k_cumsum).abs().sum(dim=-1, keepdim=True)  # 这里有没有做sum都可以，按照论文公式是没有sum  # normalized\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        context = k.transpose(-2, -1) @ v\n",
        "        y = self.attn_drop((q @ context) * D_inv + q)\n",
        "\n",
        "        # output projection\n",
        "        y = rearrange(y, 'b h n d -> b n (h d)')\n",
        "        y = self.proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class LinearCrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(LinearCrossAttention, self).__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.keys = nn.ModuleList([nn.Linear(config.n_embd, config.n_embd) for _ in range(config.n_inputs)])\n",
        "        self.values = nn.ModuleList([nn.Linear(config.n_embd, config.n_embd) for _ in range(config.n_inputs)])\n",
        "        # regularization\n",
        "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_inputs = config.n_inputs\n",
        "\n",
        "        self.attn_type = 'l1'\n",
        "\n",
        "    '''\n",
        "        Linear Attention and Linear Cross Attention (if y is provided)\n",
        "    '''\n",
        "\n",
        "    def forward(self, x, y=None, layer_past=None):\n",
        "        y = x if y is None else y\n",
        "        B, T1, C = x.size()\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q = self.query(x).view(B, T1, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        q = q.softmax(dim=-1)\n",
        "        out = q\n",
        "        for i in range(self.n_inputs):\n",
        "            _, T2, _ = y[i].size()\n",
        "            k = self.keys[i](y[i]).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "            v = self.values[i](y[i]).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "            k = k.softmax(dim=-1)  #\n",
        "            k_cumsum = k.sum(dim=-2, keepdim=True)\n",
        "            D_inv = 1. / (q * k_cumsum).sum(dim=-1, keepdim=True)  # normalized\n",
        "            out = out + 1 * (q @ (k.transpose(-2, -1) @ v)) * D_inv\n",
        "\n",
        "        # output projection\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "'''\n",
        "    X: N*T*C --> N*(4*n + 3)*C\n",
        "'''\n",
        "\n",
        "\n",
        "def horizontal_fourier_embedding(X, n=3):\n",
        "    freqs = 2 ** torch.linspace(-n, n, 2 * n + 1).to(X.device)\n",
        "    freqs = freqs[None, None, None, ...]\n",
        "    X_ = X.unsqueeze(-1).repeat([1, 1, 1, 2 * n + 1])\n",
        "    X_cos = torch.cos(freqs * X_)\n",
        "    X_sin = torch.sin(freqs * X_)\n",
        "    X = torch.cat([X.unsqueeze(-1), X_cos, X_sin], dim=-1).view(X.shape[0], X.shape[1], -1)\n",
        "    return X\n",
        "\n",
        "\n",
        "'''\n",
        "    Self and Cross Attention block for CGPT, contains  a cross attention block and a self attention block\n",
        "'''\n",
        "\n",
        "\n",
        "class MIOECrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(MIOECrossAttentionBlock, self).__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2_branch = nn.ModuleList([nn.LayerNorm(config.n_embd) for _ in range(config.n_inputs)])\n",
        "        self.ln3 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln4 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln5 = nn.LayerNorm(config.n_embd)\n",
        "        if config.attn_type == 'linear':\n",
        "            print('Using Linear Attention')\n",
        "            self.selfattn = LinearAttention(config)\n",
        "            self.crossattn = LinearCrossAttention(config)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        if config.act == 'gelu':\n",
        "            self.act = GELU\n",
        "        elif config.act == \"tanh\":\n",
        "            self.act = Tanh\n",
        "        elif config.act == 'relu':\n",
        "            self.act = ReLU\n",
        "        elif config.act == 'sigmoid':\n",
        "            self.act = Sigmoid\n",
        "\n",
        "        self.resid_drop1 = nn.Dropout(config.resid_pdrop)\n",
        "        self.resid_drop2 = nn.Dropout(config.resid_pdrop)\n",
        "\n",
        "        self.n_experts = config.n_experts\n",
        "        self.n_inputs = config.n_inputs\n",
        "\n",
        "        self.moe_mlp1 = nn.ModuleList([nn.Sequential(\n",
        "            nn.Linear(config.n_embd, config.n_inner),\n",
        "            self.act(),\n",
        "            nn.Linear(config.n_inner, config.n_embd),\n",
        "        ) for _ in range(self.n_experts)])\n",
        "\n",
        "        self.moe_mlp2 = nn.ModuleList([nn.Sequential(\n",
        "            nn.Linear(config.n_embd, config.n_inner),\n",
        "            self.act(),\n",
        "            nn.Linear(config.n_inner, config.n_embd),\n",
        "        ) for _ in range(self.n_experts)])\n",
        "\n",
        "        self.gatenet = nn.Sequential(\n",
        "            nn.Linear(config.space_dim, config.n_inner),\n",
        "            self.act(),\n",
        "            nn.Linear(config.n_inner, config.n_inner),\n",
        "            self.act(),\n",
        "            nn.Linear(config.n_inner, self.n_experts)\n",
        "        )\n",
        "\n",
        "    def ln_branchs(self, y):\n",
        "        return MultipleTensors([self.ln2_branch[i](y[i]) for i in range(self.n_inputs)])\n",
        "\n",
        "    '''\n",
        "        x: [B, T1, C], y:[B, T2, C], pos:[B, T1, n]\n",
        "    '''\n",
        "\n",
        "    def forward(self, x, y, pos):\n",
        "        gate_score = F.softmax(self.gatenet(pos), dim=-1).unsqueeze(2)  # B, T1, 1, m\n",
        "        x = x + self.resid_drop1(self.crossattn(self.ln1(x), self.ln_branchs(y)))\n",
        "        x_moe1 = torch.stack([self.moe_mlp1[i](x) for i in range(self.n_experts)], dim=-1)  # B, T1, C, m\n",
        "        x_moe1 = (gate_score * x_moe1).sum(dim=-1, keepdim=False)\n",
        "        x = x + self.ln3(x_moe1)\n",
        "        x = x + self.resid_drop2(self.selfattn(self.ln4(x)))\n",
        "        x_moe2 = torch.stack([self.moe_mlp2[i](x) for i in range(self.n_experts)], dim=-1)  # B, T1, C, m\n",
        "        x_moe2 = (gate_score * x_moe2).sum(dim=-1, keepdim=False)\n",
        "        x = x + self.ln5(x_moe2)\n",
        "        return x\n",
        "\n",
        "    #### No layernorm\n",
        "    # def forward(self, x, y):\n",
        "    #     # y = self.selfattn_branch(self.ln5(y))\n",
        "    #     x = x + self.resid_drop1(self.crossattn(x, y))\n",
        "    #     x = x + self.mlp1(x)\n",
        "    #     x = x + self.resid_drop2(self.selfattn(x))\n",
        "    #     x = x + self.mlp2(x)\n",
        "    #\n",
        "    #     return x\n",
        "\n",
        "\n",
        "'''\n",
        "    Cross Attention GPT neural operator\n",
        "    Trunck Net: geom\n",
        "'''\n",
        "\n",
        "\n",
        "class GNOT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 trunk_size=2,\n",
        "                 branch_sizes=None,  # 分支输入的维度 举例[1，3，3] [sdf,wind,track ...]\n",
        "                 space_dim=2,\n",
        "                 output_size=3,\n",
        "                 n_layers=2,\n",
        "                 n_hidden=64,\n",
        "                 n_head=1,\n",
        "                 n_experts=2,\n",
        "                 n_inner=4,\n",
        "                 mlp_layers=2,\n",
        "                 attn_type='linear',\n",
        "                 act='gelu',\n",
        "                 ffn_dropout=0.0,\n",
        "                 attn_dropout=0.0,\n",
        "                 horiz_fourier_dim=0,\n",
        "                 ):\n",
        "        super(GNOT, self).__init__()\n",
        "\n",
        "        self.horiz_fourier_dim = horiz_fourier_dim\n",
        "        self.trunk_size = trunk_size * (4 * horiz_fourier_dim + 3) if horiz_fourier_dim > 0 else trunk_size\n",
        "        self.branch_sizes = [bsize * (4 * horiz_fourier_dim + 3) for bsize in\n",
        "                             branch_sizes] if horiz_fourier_dim > 0 else branch_sizes\n",
        "        self.n_inputs = len(self.branch_sizes)\n",
        "        self.output_size = output_size\n",
        "        self.space_dim = space_dim\n",
        "        self.gpt_config = MoEGPTConfig(attn_type=attn_type, embd_pdrop=ffn_dropout, resid_pdrop=ffn_dropout,\n",
        "                                       attn_pdrop=attn_dropout, n_embd=n_hidden, n_head=n_head, n_layer=n_layers,\n",
        "                                       block_size=128, act=act, n_experts=n_experts, space_dim=space_dim,\n",
        "                                       branch_sizes=branch_sizes, n_inputs=len(branch_sizes), n_inner=n_inner)\n",
        "\n",
        "        self.trunk_mlp = MLP(self.trunk_size, n_hidden, n_hidden, n_layers=mlp_layers, act=act)\n",
        "        self.branch_mlps = nn.ModuleList(\n",
        "            [MLP(bsize, n_hidden, n_hidden, n_layers=mlp_layers, act=act) for bsize in self.branch_sizes])\n",
        "\n",
        "        self.blocks = nn.Sequential(*[MIOECrossAttentionBlock(self.gpt_config) for _ in range(self.gpt_config.n_layer)])\n",
        "\n",
        "        self.out_mlp = MLP(n_hidden, n_hidden, output_size, n_layers=mlp_layers)\n",
        "\n",
        "        # self.apply(self._init_weights)\n",
        "\n",
        "        self.__name__ = 'MIOEGPT'\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.0002)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, query_points, conditions, deep=None):\n",
        "\n",
        "        pos = query_points[:, :, 0:self.space_dim]\n",
        "\n",
        "        # x = torch.cat([x, u_p.unsqueeze(1).repeat([1, x.shape[1], 1])], dim=-1)\n",
        "\n",
        "        # if self.horiz_fourier_dim > 0:\n",
        "        #     x = horizontal_fourier_embedding(x, self.horiz_fourier_dim)\n",
        "        #     z = horizontal_fourier_embedding(z, self.horiz_fourier_dim)\n",
        "\n",
        "        x = self.trunk_mlp(query_points)\n",
        "        z = MultipleTensors([self.branch_mlps[i](conditions[i]) for i in range(self.n_inputs)])\n",
        "        if deep == None:\n",
        "            for block in self.blocks:\n",
        "                x = block(x, z, pos)\n",
        "            x_out = self.out_mlp(x)\n",
        "        else:\n",
        "            for i in range(deep):\n",
        "                x = self.blocks[i](x, z, pos)\n",
        "            x_out = self.out_mlp(x)\n",
        "\n",
        "        # x_out = torch.cat([x[i, :num] for i, num in enumerate(g.batch_num_nodes())], dim=0)\n",
        "        return x_out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gS0mLrw-YUeM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding:utf-8 _*-\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import dgl\n",
        "from einops import repeat, rearrange\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import GELU, ReLU, Tanh, Sigmoid\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# from utils import MultipleTensors\n",
        "\n",
        "\n",
        "\n",
        "class MultipleTensors():\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "\n",
        "    def to(self, device):\n",
        "        self.x = [x_.to(device) for x_ in self.x]\n",
        "        return self\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.x[item]\n",
        "\n",
        "\n",
        "class MoEGPTConfig():\n",
        "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
        "\n",
        "    def __init__(self, attn_type='linear', embd_pdrop=0.0, resid_pdrop=0.0, attn_pdrop=0.0, n_embd=128, n_head=1,\n",
        "                 n_layer=3, block_size=128, n_inner=4, act='gelu', n_experts=2, space_dim=1, branch_sizes=None,\n",
        "                 n_inputs=1):\n",
        "        self.attn_type = attn_type\n",
        "        self.embd_pdrop = embd_pdrop\n",
        "        self.resid_pdrop = resid_pdrop\n",
        "        self.attn_pdrop = attn_pdrop\n",
        "        self.n_embd = n_embd  # 64\n",
        "        self.n_head = n_head\n",
        "        self.n_layer = n_layer\n",
        "        self.block_size = block_size\n",
        "        self.n_inner = n_inner * self.n_embd\n",
        "        self.act = act\n",
        "        self.n_experts = n_experts\n",
        "        self.space_dim = space_dim\n",
        "        self.branch_sizes = branch_sizes\n",
        "        self.n_inputs = n_inputs\n",
        "\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(LinearAttention, self).__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "\n",
        "        self.attn_type = 'l1'\n",
        "\n",
        "    '''\n",
        "        Linear Attention and Linear Cross Attention (if y is provided)\n",
        "    '''\n",
        "\n",
        "    def forward(self, x, y=None, layer_past=None):\n",
        "        y = x if y is None else y\n",
        "        B, T1, C = x.size()\n",
        "        _, T2, _ = y.size()\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q = self.query(x).view(B, T1, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        k = self.key(y).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        v = self.value(y).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "\n",
        "        if self.attn_type == 'l1':\n",
        "            q = q.softmax(dim=-1)\n",
        "            k = k.softmax(dim=-1)  #\n",
        "            k_cumsum = k.sum(dim=-2, keepdim=True)\n",
        "            D_inv = 1. / (q * k_cumsum).sum(dim=-1, keepdim=True)  # normalized\n",
        "        elif self.attn_type == \"galerkin\":\n",
        "            q = q.softmax(dim=-1)\n",
        "            k = k.softmax(dim=-1)  #\n",
        "            D_inv = 1. / T2  # galerkin\n",
        "        elif self.attn_type == \"l2\":  # still use l1 normalization\n",
        "            q = q / q.norm(dim=-1, keepdim=True, p=1)\n",
        "            k = k / k.norm(dim=-1, keepdim=True, p=1)\n",
        "            k_cumsum = k.sum(dim=-2, keepdim=True)\n",
        "            D_inv = 1. / (q * k_cumsum).abs().sum(dim=-1, keepdim=True)  # 这里有没有做sum都可以，按照论文公式是没有sum  # normalized\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        context = k.transpose(-2, -1) @ v\n",
        "        y = self.attn_drop((q @ context) * D_inv + q)\n",
        "\n",
        "        # output projection\n",
        "        y = rearrange(y, 'b h n d -> b n (h d)')\n",
        "        y = self.proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class LinearCrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(LinearCrossAttention, self).__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.keys = nn.ModuleList([nn.Linear(config.n_embd, config.n_embd) for _ in range(config.n_inputs)])\n",
        "        self.values = nn.ModuleList([nn.Linear(config.n_embd, config.n_embd) for _ in range(config.n_inputs)])\n",
        "        # regularization\n",
        "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_inputs = config.n_inputs\n",
        "\n",
        "        self.attn_type = 'l1'\n",
        "\n",
        "    '''\n",
        "        Linear Attention and Linear Cross Attention (if y is provided)\n",
        "    '''\n",
        "\n",
        "    def forward(self, x, y=None, layer_past=None):\n",
        "        y = x if y is None else y\n",
        "        B, T1, C = x.size()\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q = self.query(x).view(B, T1, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        q = q.softmax(dim=-1)\n",
        "        out = q\n",
        "        for i in range(self.n_inputs):\n",
        "            _, T2, _ = y[i].size()\n",
        "            k = self.keys[i](y[i]).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "            v = self.values[i](y[i]).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "            k = k.softmax(dim=-1)  #\n",
        "            k_cumsum = k.sum(dim=-2, keepdim=True)\n",
        "            D_inv = 1. / (q * k_cumsum).sum(dim=-1, keepdim=True)  # normalized\n",
        "            out = out + 1 * (q @ (k.transpose(-2, -1) @ v)) * D_inv\n",
        "\n",
        "        # output projection\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "'''\n",
        "    X: N*T*C --> N*(4*n + 3)*C\n",
        "'''\n",
        "\n",
        "\n",
        "def horizontal_fourier_embedding(X, n=3):\n",
        "    freqs = 2 ** torch.linspace(-n, n, 2 * n + 1).to(X.device)\n",
        "    freqs = freqs[None, None, None, ...]\n",
        "    X_ = X.unsqueeze(-1).repeat([1, 1, 1, 2 * n + 1])\n",
        "    X_cos = torch.cos(freqs * X_)\n",
        "    X_sin = torch.sin(freqs * X_)\n",
        "    X = torch.cat([X.unsqueeze(-1), X_cos, X_sin], dim=-1).view(X.shape[0], X.shape[1], -1)\n",
        "    return X\n",
        "\n",
        "\n",
        "'''\n",
        "    Self and Cross Attention block for CGPT, contains  a cross attention block and a self attention block\n",
        "'''\n",
        "\n",
        "\n",
        "class MIOECrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(MIOECrossAttentionBlock, self).__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2_branch = nn.ModuleList([nn.LayerNorm(config.n_embd) for _ in range(config.n_inputs)])\n",
        "        self.ln3 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln4 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln5 = nn.LayerNorm(config.n_embd)\n",
        "        if config.attn_type == 'linear':\n",
        "            print('Using Linear Attention')\n",
        "            self.selfattn = LinearAttention(config)\n",
        "            self.crossattn = LinearCrossAttention(config)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        if config.act == 'gelu':\n",
        "            self.act = GELU\n",
        "        elif config.act == \"tanh\":\n",
        "            self.act = Tanh\n",
        "        elif config.act == 'relu':\n",
        "            self.act = ReLU\n",
        "        elif config.act == 'sigmoid':\n",
        "            self.act = Sigmoid\n",
        "\n",
        "        self.resid_drop1 = nn.Dropout(config.resid_pdrop)\n",
        "        self.resid_drop2 = nn.Dropout(config.resid_pdrop)\n",
        "\n",
        "        self.n_experts = config.n_experts\n",
        "        self.n_inputs = config.n_inputs\n",
        "\n",
        "        self.moe_mlp1 = nn.ModuleList([nn.Sequential(\n",
        "            nn.Linear(config.n_embd, config.n_inner),\n",
        "            self.act(),\n",
        "            nn.Linear(config.n_inner, config.n_embd),\n",
        "        ) for _ in range(self.n_experts)])\n",
        "\n",
        "        self.moe_mlp2 = nn.ModuleList([nn.Sequential(\n",
        "            nn.Linear(config.n_embd, config.n_inner),\n",
        "            self.act(),\n",
        "            nn.Linear(config.n_inner, config.n_embd),\n",
        "        ) for _ in range(self.n_experts)])\n",
        "\n",
        "        self.gatenet = nn.Sequential(\n",
        "            nn.Linear(config.space_dim, config.n_inner),\n",
        "            self.act(),\n",
        "            nn.Linear(config.n_inner, config.n_inner),\n",
        "            self.act(),\n",
        "            nn.Linear(config.n_inner, self.n_experts)\n",
        "        )\n",
        "\n",
        "    def ln_branchs(self, y):\n",
        "        return MultipleTensors([self.ln2_branch[i](y[i]) for i in range(self.n_inputs)])\n",
        "\n",
        "    '''\n",
        "        x: [B, T1, C], y:[B, T2, C], pos:[B, T1, n]\n",
        "    '''\n",
        "\n",
        "    def forward(self, x, y, pos):\n",
        "        gate_score = F.softmax(self.gatenet(pos), dim=-1).unsqueeze(2)  # B, T1, 1, m\n",
        "        x = x + self.resid_drop1(self.crossattn(self.ln1(x), self.ln_branchs(y)))\n",
        "        x_moe1 = torch.stack([self.moe_mlp1[i](x) for i in range(self.n_experts)], dim=-1)  # B, T1, C, m\n",
        "        x_moe1 = (gate_score * x_moe1).sum(dim=-1, keepdim=False)\n",
        "        x = x + self.ln3(x_moe1)\n",
        "        x = x + self.resid_drop2(self.selfattn(self.ln4(x)))\n",
        "        x_moe2 = torch.stack([self.moe_mlp2[i](x) for i in range(self.n_experts)], dim=-1)  # B, T1, C, m\n",
        "        x_moe2 = (gate_score * x_moe2).sum(dim=-1, keepdim=False)\n",
        "        x = x + self.ln5(x_moe2)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "    Cross Attention GPT neural operator\n",
        "    Trunck Net: geom\n",
        "'''\n",
        "\n",
        "\n",
        "class GNOT2(nn.Module):\n",
        "    def __init__(self,\n",
        "                 trunk_size=2,\n",
        "                 branch_sizes=None,  # 分支输入的维度 举例[1，3，3] [sdf,wind,track ...]\n",
        "                 space_dim=3,\n",
        "                 output_size=3,\n",
        "                 n_layers=2,\n",
        "                 n_hidden=64,\n",
        "                 n_head=1,\n",
        "                 n_experts=2,\n",
        "                 n_inner=4,\n",
        "                 mlp_layers=2,\n",
        "                 attn_type='linear',\n",
        "                 act='gelu',\n",
        "                 ffn_dropout=0.0,\n",
        "                 attn_dropout=0.0,\n",
        "                 use_fourier_feat=False,\n",
        "                 ):\n",
        "        super(GNOT, self).__init__()\n",
        "\n",
        "        # self.horiz_fourier_dim = horiz_fourier_dim\n",
        "        self.trunk_size = trunk_size * 2 if use_fourier_feat  else trunk_size\n",
        "        self.branch_sizes =  branch_sizes\n",
        "        self.gauss_encoder=GaussianEncoding(10,3,3)\n",
        "        self.n_inputs = len(self.branch_sizes)\n",
        "        self.output_size = output_size\n",
        "        self.space_dim = space_dim*2 if use_fourier_feat else space_dim\n",
        "        self.gpt_config = MoEGPTConfig(attn_type=attn_type, embd_pdrop=ffn_dropout, resid_pdrop=ffn_dropout,\n",
        "                                       attn_pdrop=attn_dropout, n_embd=n_hidden, n_head=n_head, n_layer=n_layers,\n",
        "                                       block_size=128, act=act, n_experts=n_experts, space_dim=space_dim*2 if use_fourier_feat else space_dim,\n",
        "                                       branch_sizes=branch_sizes, n_inputs=len(branch_sizes), n_inner=n_inner)\n",
        "\n",
        "        self.trunk_mlp = MLP(self.trunk_size, n_hidden, n_hidden, n_layers=mlp_layers, act=act)\n",
        "        self.branch_mlps = nn.ModuleList(\n",
        "            [MLP(bsize, n_hidden, n_hidden, n_layers=mlp_layers, act=act) for bsize in self.branch_sizes])\n",
        "\n",
        "        self.blocks = nn.Sequential(*[MIOECrossAttentionBlock(self.gpt_config) for _ in range(self.gpt_config.n_layer)])\n",
        "\n",
        "        self.out_mlp = MLP(n_hidden, n_hidden, output_size, n_layers=mlp_layers)\n",
        "        self.use_fourier_feat=use_fourier_feat\n",
        "        # self.apply(self._init_weights)\n",
        "\n",
        "        self.__name__ = 'MIOEGPT'\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.0002)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, query_points, conditions, deep=None):\n",
        "        if self.use_fourier_feat:\n",
        "            pos = self.gauss_encoder(query_points[:, :, 0:self.space_dim])\n",
        "\n",
        "        # x = torch.cat([x, u_p.unsqueeze(1).repeat([1, x.shape[1], 1])], dim=-1)\n",
        "\n",
        "        # if self.horiz_fourier_dim > 0:\n",
        "        #     x = horizontal_fourier_embedding(x, self.horiz_fourier_dim)\n",
        "        #     z = horizontal_fourier_embedding(z, self.horiz_fourier_dim)\n",
        "            query_points=self.gauss_encoder(query_points)\n",
        "        else:\n",
        "            pos=query_points[:,:,0:self.space_dim]\n",
        "        x = self.trunk_mlp(query_points)\n",
        "        z = MultipleTensors([self.branch_mlps[i](conditions[i]) for i in range(self.n_inputs)])\n",
        "        if deep == None:\n",
        "            for block in self.blocks:\n",
        "                x = block(x, z, pos)\n",
        "            x_out = self.out_mlp(x)\n",
        "        else:\n",
        "            for i in range(deep):\n",
        "                x = self.blocks[i](x, z, pos)\n",
        "            x_out = self.out_mlp(x)\n",
        "\n",
        "        # x_out = torch.cat([x[i, :num] for i, num in enumerate(g.batch_num_nodes())], dim=0)\n",
        "        return x_out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BS6m20tr-9-6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Args"
      ],
      "metadata": {
        "id": "TA4DWJ3Qn60x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding:utf-8 _*-\n",
        "\n",
        "import argparse\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(description='trackB')\n",
        "    parser.add_argument('--dataset', type=str,\n",
        "                        default='ns2d',\n",
        "                        choices=['heat2d', 'ns2d', 'inductor2d', 'heatsink3d', 'ns2d_time', 'darcy2d', ])\n",
        "    parser.add_argument('--train_dir', default='./Dataset/data', type=str)\n",
        "    parser.add_argument('--test_dir', default='./Dataset/track_A', type=str)\n",
        "    parser.add_argument('--wind_direction', default='z', type=str)\n",
        "    parser.add_argument('--track', default='track_A', type=str)\n",
        "    parser.add_argument('--train_log', default='train_p', type=str)\n",
        "    parser.add_argument('--expand_data', type=int,default=0)\n",
        "    parser.add_argument('--val_ratio', default=0.05, type=float)\n",
        "    parser.add_argument('--p', default=0.5, type=float,help='p of using tracka')\n",
        "\n",
        "    parser.add_argument('--iter_per_epoch', default=150, type=int)\n",
        "    parser.add_argument('--cat_area', type=int, default=1,\n",
        "                        help='cat area or use condition')\n",
        "    parser.add_argument('--use_transform', type=int, default=0)\n",
        "    parser.add_argument('--component', type=str,\n",
        "                        default='all', )\n",
        "\n",
        "    parser.add_argument('--seed', type=int, default=2024, metavar='Seed',\n",
        "                        help='random seed (default: 1127802)')\n",
        "\n",
        "    parser.add_argument('--gpu', type=int, default=0, help='gpu id')\n",
        "    parser.add_argument('--use-tb', type=int, default=1, help='whether use tensorboard')\n",
        "    parser.add_argument('--comment', type=str, default=\"\", help=\"comment for the experiment\")\n",
        "\n",
        "    parser.add_argument('--train-num', type=str, default='all')\n",
        "    parser.add_argument('--test-num', type=str, default='all')\n",
        "\n",
        "    parser.add_argument('--sort-data', type=int, default=0)\n",
        "\n",
        "    parser.add_argument('--normalize_x', type=str, default='unit',\n",
        "                        choices=['none', 'minmax', 'unit'])\n",
        "    parser.add_argument('--use-normalizer', type=str, default='unit',\n",
        "                        choices=['none', 'minmax', 'unit', 'quantile'],\n",
        "                        help=\"whether normalize y\")\n",
        "\n",
        "    parser.add_argument('--epochs', type=int, default=500, metavar='N',\n",
        "                        help='number of epochs to train (default: 100)')\n",
        "    parser.add_argument('--val_epochs', type=int, default=10,)\n",
        "    parser.add_argument('--optimizer', type=str, default='AdamW', choices=['Adam', 'AdamW'])\n",
        "\n",
        "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
        "                        help='max learning rate (default: 0.001)')\n",
        "    parser.add_argument('--weight-decay', type=float, default=5e-5\n",
        "                        )\n",
        "    parser.add_argument('--grad-clip', type=str, default=1000.0\n",
        "                        )\n",
        "    parser.add_argument('--batch-size', type=int, default=2, metavar='bsz',\n",
        "                        help='input batch size for training (default: 8)')\n",
        "    parser.add_argument('--val-batch-size', type=int, default=8, metavar='bsz',\n",
        "                        help='input batch size for validation (default: 4)')\n",
        "    parser.add_argument('--no-cuda', default=0,type=int,\n",
        "                        help='disables CUDA training')\n",
        "\n",
        "\n",
        "    parser.add_argument('--lr-method', type=str, default='cycle',\n",
        "                        choices=['cycle', 'step', 'warmup'])\n",
        "    parser.add_argument('--lr-step-size', type=int, default=20\n",
        "                        )\n",
        "    parser.add_argument('--warmup-epochs', type=int, default=50)\n",
        "\n",
        "    parser.add_argument('--loss-name', type=str, default='rel2',\n",
        "                        choices=['rel2', 'rel1', 'l2', 'l1'])\n",
        "    #### public model architecture parameters\n",
        "\n",
        "    parser.add_argument('--model-name', type=str, default='GNOT',\n",
        "                        choices=['CGPT', 'GNOT', ])\n",
        "    parser.add_argument('--n-hidden', type=int, default=512)\n",
        "    parser.add_argument('--n-layers', type=int, default=16)\n",
        "\n",
        "    #### MLP parameters\n",
        "\n",
        "    # common\n",
        "    parser.add_argument('--act', type=str, default='gelu', choices=['gelu', 'relu', 'tanh', 'sigmoid'])\n",
        "    parser.add_argument('--loss', type=str, default='default', choices=['default','mse', 'l1', 'huber', 'smoothl1'])\n",
        "    parser.add_argument('--model', type=str, default='GNOT')\n",
        "    parser.add_argument('--n-head', type=int, default=8)\n",
        "    parser.add_argument('--slice_num', type=int, default=64)\n",
        "    parser.add_argument('--agent_num', type=int, default=32)\n",
        "    parser.add_argument('--ffn-dropout', type=float, default=0.0, metavar='ffn_dropout',\n",
        "                        help='dropout for the FFN in attention (default: 0.0)')\n",
        "    parser.add_argument('--attn-dropout', type=float, default=0.0)\n",
        "    parser.add_argument('--mlp-layers', type=int, default=3)\n",
        "\n",
        "    # Transformer\n",
        "    # parser.add_argument('--subsampled-len',type=int, default=256)\n",
        "    parser.add_argument('--attn-type', type=str, default='linear',\n",
        "                        choices=['random', 'linear', 'gated', 'hydra', 'kernel'])\n",
        "    parser.add_argument('--hfourier-dim', type=int, default=0)\n",
        "\n",
        "    # GNOT\n",
        "    parser.add_argument('--n_experts', type=int, default=2)\n",
        "    parser.add_argument('--input_dim', type=int, default=3)\n",
        "    parser.add_argument('--output_dim', type=int, default=1)\n",
        "    parser.add_argument('--space_dim', type=int, default=3)\n",
        "\n",
        "    parser.add_argument('--branch_sizes', nargs=\"*\", type=int, default=[3, 3])\n",
        "    parser.add_argument('--n-inner', type=int, default=4)\n",
        "\n",
        "    return parser.parse_known_args()[0]\n"
      ],
      "metadata": {
        "id": "KmeREhHyjUJP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding:utf-8 _*-\n",
        "\n",
        "import argparse\n",
        "\n",
        "\n",
        "def get_args_velocity():\n",
        "    parser = argparse.ArgumentParser(description='GNOT for operator learning')\n",
        "    parser.add_argument('--dataset', type=str,\n",
        "                        default='ns2d',\n",
        "                        choices=['heat2d', 'ns2d', 'inductor2d', 'heatsink3d', 'ns2d_time', 'darcy2d', ])\n",
        "    parser.add_argument('--train_dir', default='/mnt/nunu/race/IJCAI_2024/Dataset/Training_data', type=str)\n",
        "    parser.add_argument('--test_dir', default='/mnt/nunu/race/IJCAI_2024/Dataset/Testset_track_A', type=str)\n",
        "    parser.add_argument('--norm_type', default=None, type=str)\n",
        "    parser.add_argument('--wind_direction', default='z', type=str)\n",
        "    parser.add_argument('--track', default='track_A', type=str)\n",
        "    parser.add_argument('--train_log', default='train_v', type=str)\n",
        "    parser.add_argument('--expand_data', default=False, action='store_true')\n",
        "    parser.add_argument('--val_ratio', default=0.1, type=float)\n",
        "    parser.add_argument('--p', default=0.5, type=float,help='p of using tracka')\n",
        "    parser.add_argument('--iter_per_epoch', default=150, type=int)\n",
        "    parser.add_argument('--val_epoch', default=10, type=int)\n",
        "    parser.add_argument('--cat_area', action='store_true', default=False,\n",
        "                        help='cat area or use condition')\n",
        "    parser.add_argument('--use_transform', action='store_true', default=False)\n",
        "    parser.add_argument('--component', type=str,\n",
        "                        default='all', )\n",
        "\n",
        "    parser.add_argument('--seed', type=int, default=2024, metavar='Seed',\n",
        "                        help='random seed (default: 1127802)')\n",
        "\n",
        "    parser.add_argument('--gpu', type=int, default=0, help='gpu id')\n",
        "    parser.add_argument('--use-tb', type=int, default=1, help='whether use tensorboard')\n",
        "    parser.add_argument('--comment', type=str, default=\"\", help=\"comment for the experiment\")\n",
        "\n",
        "    parser.add_argument('--train-num', type=str, default='all')\n",
        "    parser.add_argument('--test-num', type=str, default='all')\n",
        "\n",
        "    parser.add_argument('--sort-data', type=int, default=0)\n",
        "\n",
        "    parser.add_argument('--normalize_x', type=str, default='none',\n",
        "                        choices=['none', 'minmax', 'unit'])\n",
        "    parser.add_argument('--use-normalizer', type=str, default='unit',\n",
        "                        choices=['none', 'minmax', 'unit', 'quantile'],\n",
        "                        help=\"whether normalize y\")\n",
        "\n",
        "    parser.add_argument('--epochs', type=int, default=300, metavar='N',\n",
        "                        help='number of epochs to train (default: 100)')\n",
        "    parser.add_argument('--optimizer', type=str, default='AdamW', choices=['Adam', 'AdamW'])\n",
        "\n",
        "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
        "                        help='max learning rate (default: 0.001)')\n",
        "    parser.add_argument('--weight-decay', type=float, default=5e-6\n",
        "                        )\n",
        "    parser.add_argument('--grad-clip', type=str, default=1000.0\n",
        "                        )\n",
        "    parser.add_argument('--batch-size', type=int, default=1, metavar='bsz',\n",
        "                        help='input batch size for training (default: 8)')\n",
        "    parser.add_argument('--val-batch-size', type=int, default=8, metavar='bsz',\n",
        "                        help='input batch size for validation (default: 4)')\n",
        "\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                        help='disables CUDA training')\n",
        "\n",
        "    parser.add_argument('--lr-method', type=str, default='cycle',\n",
        "                        choices=['cycle', 'step', 'warmup','no'])\n",
        "    parser.add_argument('--lr-step-size', type=int, default=50\n",
        "                        )\n",
        "    parser.add_argument('--warmup-epochs', type=int, default=50)\n",
        "\n",
        "    parser.add_argument('--loss-name', type=str, default='rel2',\n",
        "                        choices=['rel2', 'rel1', 'l2', 'l1'])\n",
        "    #### public model architecture parameters\n",
        "\n",
        "    parser.add_argument('--model-name', type=str, default='GNOT',\n",
        "                        choices=['CGPT', 'GNOT', ])\n",
        "    parser.add_argument('--n-hidden', type=int, default=256)\n",
        "    parser.add_argument('--n-layers', type=int, default=16)\n",
        "\n",
        "    #### MLP parameters\n",
        "\n",
        "    # common\n",
        "    parser.add_argument('--act', type=str, default='gelu', choices=['gelu', 'relu', 'tanh', 'sigmoid'])\n",
        "    parser.add_argument('--loss', type=str, default='default', choices=['default','mse', 'l1', 'huber', 'smoothl1'])\n",
        "    parser.add_argument('--model', type=str, default='GNOT')\n",
        "    parser.add_argument('--n-head', type=int, default=8)\n",
        "    parser.add_argument('--slice_num', type=int, default=64)\n",
        "    parser.add_argument('--agent_num', type=int, default=64)\n",
        "    parser.add_argument('--ffn-dropout', type=float, default=0.0, metavar='ffn_dropout',\n",
        "                        help='dropout for the FFN in attention (default: 0.0)')\n",
        "    parser.add_argument('--attn-dropout', type=float, default=0.0)\n",
        "    parser.add_argument('--mlp-layers', type=int, default=3)\n",
        "\n",
        "    # Transformer\n",
        "    # parser.add_argument('--subsampled-len',type=int, default=256)\n",
        "    parser.add_argument('--attn-type', type=str, default='linear',\n",
        "                        choices=['random', 'linear', 'gated', 'hydra', 'kernel'])\n",
        "    parser.add_argument('--use_fourier_feat', default=False,action='store_true',)\n",
        "\n",
        "    # GNOT\n",
        "    parser.add_argument('--n_experts', type=int, default=2)\n",
        "    parser.add_argument('--input_dim', type=int, default=3)\n",
        "    parser.add_argument('--output_dim', type=int, default=3)\n",
        "    parser.add_argument('--space_dim', type=int, default=3)\n",
        "\n",
        "    parser.add_argument('--branch_sizes', nargs=\"*\", type=int, default=[3])\n",
        "    parser.add_argument('--n-inner', type=int, default=2)\n",
        "\n",
        "    return parser.parse_known_args()[0]\n"
      ],
      "metadata": {
        "id": "MTw5I-wCzZvp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding:utf-8 _*-\n",
        "\n",
        "import argparse\n",
        "\n",
        "\n",
        "def get_args_cd():\n",
        "    parser = argparse.ArgumentParser(description='GNOT for operator learning')\n",
        "    parser.add_argument('--dataset', type=str,\n",
        "                        default='ns2d',\n",
        "                        choices=['heat2d', 'ns2d', 'inductor2d', 'heatsink3d', 'ns2d_time', 'darcy2d', ])\n",
        "    parser.add_argument('--train_dir', default='/mnt/nunu/race/IJCAI_2024/Dataset/Training_data', type=str)\n",
        "    parser.add_argument('--test_dir', default='/mnt/nunu/race/IJCAI_2024/Dataset/Testset_track_A', type=str)\n",
        "    parser.add_argument('--norm_type', default=None, type=str)\n",
        "    parser.add_argument('--wind_direction', default='z', type=str)\n",
        "    parser.add_argument('--track', default='track_A', type=str)\n",
        "    parser.add_argument('--train_log', default='train_cd', type=str)\n",
        "    parser.add_argument('--expand_data', default=False, action='store_true')\n",
        "    parser.add_argument('--val_ratio', default=0.1, type=float)\n",
        "    parser.add_argument('--p', default=0.5, type=float,help='p of using tracka')\n",
        "    parser.add_argument('--iter_per_epoch', default=150, type=int)\n",
        "    parser.add_argument('--val_epoch', default=10, type=int)\n",
        "    parser.add_argument('--cat_area', action='store_true', default=False,\n",
        "                        help='cat area or use condition')\n",
        "    parser.add_argument('--use_transform', action='store_true', default=False)\n",
        "    parser.add_argument('--component', type=str,\n",
        "                        default='all', )\n",
        "\n",
        "    parser.add_argument('--seed', type=int, default=2024, metavar='Seed',\n",
        "                        help='random seed (default: 1127802)')\n",
        "\n",
        "    parser.add_argument('--gpu', type=int, default=0, help='gpu id')\n",
        "    parser.add_argument('--use-tb', type=int, default=1, help='whether use tensorboard')\n",
        "    parser.add_argument('--comment', type=str, default=\"\", help=\"comment for the experiment\")\n",
        "\n",
        "    parser.add_argument('--train-num', type=str, default='all')\n",
        "    parser.add_argument('--test-num', type=str, default='all')\n",
        "\n",
        "    parser.add_argument('--sort-data', type=int, default=0)\n",
        "\n",
        "    parser.add_argument('--normalize_x', type=str, default='none',\n",
        "                        choices=['none', 'minmax', 'unit'])\n",
        "    parser.add_argument('--use-normalizer', type=str, default='unit',\n",
        "                        choices=['none', 'minmax', 'unit', 'quantile'],\n",
        "                        help=\"whether normalize y\")\n",
        "\n",
        "    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
        "                        help='number of epochs to train (default: 100)')\n",
        "    parser.add_argument('--optimizer', type=str, default='AdamW', choices=['Adam', 'AdamW'])\n",
        "\n",
        "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
        "                        help='max learning rate (default: 0.001)')\n",
        "    parser.add_argument('--weight-decay', type=float, default=5e-6\n",
        "                        )\n",
        "    parser.add_argument('--grad-clip', type=str, default=1000.0\n",
        "                        )\n",
        "    parser.add_argument('--batch-size', type=int, default=4, metavar='bsz',\n",
        "                        help='input batch size for training (default: 8)')\n",
        "    parser.add_argument('--val-batch-size', type=int, default=8, metavar='bsz',\n",
        "                        help='input batch size for validation (default: 4)')\n",
        "\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                        help='disables CUDA training')\n",
        "\n",
        "    parser.add_argument('--lr-method', type=str, default='no',\n",
        "                        choices=['cycle', 'step', 'warmup','no'])\n",
        "    parser.add_argument('--lr-step-size', type=int, default=50\n",
        "                        )\n",
        "    parser.add_argument('--warmup-epochs', type=int, default=50)\n",
        "\n",
        "    parser.add_argument('--loss-name', type=str, default='rel2',\n",
        "                        choices=['rel2', 'rel1', 'l2', 'l1'])\n",
        "    #### public model architecture parameters\n",
        "\n",
        "    parser.add_argument('--model-name', type=str, default='GNOT',\n",
        "                        choices=['CGPT', 'GNOT', ])\n",
        "    parser.add_argument('--n-hidden', type=int, default=256)\n",
        "    parser.add_argument('--n-layers', type=int, default=8)\n",
        "\n",
        "    #### MLP parameters\n",
        "\n",
        "    # common\n",
        "    parser.add_argument('--act', type=str, default='gelu', choices=['gelu', 'relu', 'tanh', 'sigmoid'])\n",
        "    parser.add_argument('--loss', type=str, default='mse', choices=['default','mse', 'l1', 'huber', 'smoothl1'])\n",
        "    parser.add_argument('--model', type=str, default='GNOT')\n",
        "    parser.add_argument('--n-head', type=int, default=8)\n",
        "    parser.add_argument('--slice_num', type=int, default=64)\n",
        "    parser.add_argument('--agent_num', type=int, default=64)\n",
        "    parser.add_argument('--ffn-dropout', type=float, default=0.0, metavar='ffn_dropout',\n",
        "                        help='dropout for the FFN in attention (default: 0.0)')\n",
        "    parser.add_argument('--attn-dropout', type=float, default=0.0)\n",
        "    parser.add_argument('--mlp-layers', type=int, default=3)\n",
        "\n",
        "    # Transformer\n",
        "    # parser.add_argument('--subsampled-len',type=int, default=256)\n",
        "    parser.add_argument('--attn-type', type=str, default='linear',\n",
        "                        choices=['random', 'linear', 'gated', 'hydra', 'kernel'])\n",
        "    parser.add_argument('--use_fourier_feat', default=False,action='store_true',)\n",
        "\n",
        "    # GNOT\n",
        "    parser.add_argument('--n_experts', type=int, default=2)\n",
        "    parser.add_argument('--input_dim', type=int, default=3)\n",
        "    parser.add_argument('--output_dim', type=int, default=1)\n",
        "    parser.add_argument('--space_dim', type=int, default=3)\n",
        "\n",
        "    parser.add_argument('--branch_sizes', nargs=\"*\", type=int, default=[3])\n",
        "    parser.add_argument('--n-inner', type=int, default=4)\n",
        "\n",
        "    return parser.parse_known_args()[0]\n"
      ],
      "metadata": {
        "id": "f-bzln2o0uHX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "UQxAwmUYnWkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding:utf-8 _*-\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.special as ts\n",
        "from torch import nn\n",
        "\n",
        "from scipy import interpolate\n",
        "from functools import reduce\n",
        "from pathlib import Path\n",
        "from torch.nn import  MSELoss,L1Loss,HuberLoss,SmoothL1Loss\n",
        "class LpLoss(nn.Module):\n",
        "    def __init__(self, d=2, p=2, size_average=True, reduction='mean'):\n",
        "        super(LpLoss, self).__init__()\n",
        "        assert d > 0 and p > 0\n",
        "        self.d = d\n",
        "        self.p = p\n",
        "        self.reduction = reduction\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        num_examples = x.size(0)\n",
        "        h = 1.0 / (x.size(1) - 1.0)\n",
        "        all_norms = (h ** (self.d / self.p)) * torch.norm(\n",
        "            x.reshape(num_examples, -1) - y.reshape(num_examples, -1), self.p, dim=1\n",
        "        )\n",
        "        if self.reduction == 'mean':\n",
        "            return all_norms.mean() if self.size_average else all_norms.sum()\n",
        "        elif self.reduction == 'sum':\n",
        "            return all_norms.sum()\n",
        "        else:\n",
        "            return all_norms\n",
        "\n",
        "class DataProcessor:\n",
        "    def load_valid_indices(self, log_file):\n",
        "        with open(log_file, \"r\") as fp:\n",
        "            mesh_ind = fp.read().split(\"\\n\")\n",
        "            mesh_ind = [int(a) for a in mesh_ind if a.strip()]\n",
        "        return mesh_ind\n",
        "\n",
        "    def split_data(self, indices, train_ratio=0.8):\n",
        "        random.shuffle(indices)\n",
        "        split_point = int(len(indices) * train_ratio)\n",
        "        train_indices = indices[:split_point]\n",
        "        val_indices = indices[split_point:]\n",
        "        return train_indices, val_indices\n",
        "\n",
        "    def save_split_indices(self, train_indices, val_indices, output_train_file,output_val_file):\n",
        "        with open(output_train_file, \"w\") as fp:\n",
        "            # fp.write(\"Train Indices:\\n\")\n",
        "            for idx in train_indices:\n",
        "                fp.write(f\"{str(idx).zfill(3)}\\n\")\n",
        "        with open(output_val_file, \"w\") as fp:\n",
        "            # fp.write(\"Val Indices:\\n\")\n",
        "            for idx in val_indices:\n",
        "                fp.write(f\"{str(idx).zfill(3)}\\n\")\n",
        "def extract_condition(data):\n",
        "\n",
        "    keys_to_keep = {'vertices', 'pressure','area'}\n",
        "\n",
        "    filtered_data = [value for key, value in data.items() if key not in keys_to_keep]\n",
        "\n",
        "    return filtered_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_model(args):\n",
        "    trunk_size,  branch_sizes, output_size = args.input_dim, args.branch_sizes, args.output_dim\n",
        "    output_size = args.output_dim\n",
        "    agent_num=args.agent_num\n",
        "    if args.model=='GNOT':\n",
        "      # print(1)\n",
        "      return GNOT(trunk_size=trunk_size, branch_sizes=branch_sizes, output_size=output_size, n_layers=args.n_layers,\n",
        "                  n_hidden=args.n_hidden, n_head=args.n_head, attn_type=args.attn_type, ffn_dropout=args.ffn_dropout,\n",
        "                  attn_dropout=args.attn_dropout, mlp_layers=args.mlp_layers, act=args.act,\n",
        "                  horiz_fourier_dim=0, space_dim=args.space_dim, n_experts=args.n_experts,\n",
        "                  n_inner=args.n_inner)\n",
        "    else:\n",
        "        # return GNOT_Agent(trunk_size=trunk_size, branch_sizes=branch_sizes, output_size=output_size, n_layers=args.n_layers,\n",
        "        #             n_hidden=args.n_hidden, n_head=args.n_head, attn_type=args.attn_type, ffn_dropout=args.ffn_dropout,\n",
        "        #             attn_dropout=args.attn_dropout, mlp_layers=args.mlp_layers, act=args.act,\n",
        "        #             horiz_fourier_dim=args.hfourier_dim, space_dim=args.space_dim, n_experts=args.n_experts,\n",
        "        #             n_inner=args.n_inner,agent_num=agent_num)\n",
        "        print(1)\n",
        "\n",
        "def get_dataset(args):\n",
        "    return CarDataset(\n",
        "        Path(args.train_dir),\n",
        "        Path(args.test_dir),\n",
        "        expand_data=args.expand_data,\n",
        "        track=args.track,\n",
        "        wind_direction=args.wind_direction,\n",
        "        val_ratio=args.val_ratio,\n",
        "        use_transform=args.use_transform\n",
        "\n",
        "    )\n",
        "def adjust_item(y,t=0.4):\n",
        "    if y < t:\n",
        "        result=t\n",
        "    else:\n",
        "        result=y\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_loss(args):\n",
        "    if args.loss=='default':\n",
        "        return LpLoss()\n",
        "    if args.loss=='mse':\n",
        "        return MSELoss()\n",
        "    if args.loss=='l1':\n",
        "        return L1Loss()\n",
        "    if args.loss=='huber':\n",
        "        return HuberLoss()\n",
        "    if args.loss=='smoothl1':\n",
        "        return SmoothL1Loss()\n",
        "\n",
        "def get_seed(s, printout=True, cudnn=True):\n",
        "    # rd.seed(s)\n",
        "    os.environ['PYTHONHASHSEED'] = str(s)\n",
        "    np.random.seed(s)\n",
        "    # pd.core.common.random_state(s)\n",
        "    # Torch\n",
        "    torch.manual_seed(s)\n",
        "    torch.cuda.manual_seed(s)\n",
        "    if cudnn:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(s)\n",
        "\n",
        "    message = f'''\n",
        "    os.environ['PYTHONHASHSEED'] = str({s})\n",
        "    numpy.random.seed({s})\n",
        "    torch.manual_seed({s})\n",
        "    torch.cuda.manual_seed({s})\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all({s})\n",
        "    '''\n",
        "    if printout:\n",
        "        print(\"\\n\")\n",
        "        print(f\"The following code snippets have been run.\")\n",
        "        print(\"=\" * 50)\n",
        "        print(message)\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "\n",
        "def get_num_params(model):\n",
        "    '''\n",
        "    a single entry in cfloat and cdouble count as two parameters\n",
        "    see https://github.com/pytorch/pytorch/issues/57518\n",
        "    '''\n",
        "    # model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    # num_params = 0\n",
        "    # for p in model_parameters:\n",
        "    #     # num_params += np.prod(p.size()+(2,) if p.is_complex() else p.size())\n",
        "    #     num_params += p.numel() * (1 + p.is_complex())\n",
        "    # return num_params\n",
        "\n",
        "    c = 0\n",
        "    for p in list(model.parameters()):\n",
        "        c += reduce(operator.mul, list(p.size() + (2,) if p.is_complex() else p.size()))  #### there is complex weight\n",
        "    return c\n",
        "\n",
        "\n",
        "### x: list of tensors\n",
        "class MultipleTensors():\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "\n",
        "    def to(self, device):\n",
        "        self.x = [x_.to(device) for x_ in self.x]\n",
        "        return self\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.x[item]\n",
        "\n",
        "\n",
        "# whether need to transpose\n",
        "def plot_heatmap(\n",
        "        x, y, z, path=None, vmin=None, vmax=None, cmap=None,\n",
        "        title=\"\", xlabel=\"x\", ylabel=\"y\", show=False\n",
        "):\n",
        "    '''\n",
        "    Plot heat map for a 3-dimension data\n",
        "    '''\n",
        "    plt.cla()\n",
        "    # plt.figure()\n",
        "    xx = np.linspace(np.min(x), np.max(x))\n",
        "    yy = np.linspace(np.min(y), np.max(y))\n",
        "    xx, yy = np.meshgrid(xx, yy)\n",
        "\n",
        "    vals = interpolate.griddata(np.array([x, y]).T, np.array(z),\n",
        "                                (xx, yy), method='cubic')\n",
        "    vals_0 = interpolate.griddata(np.array([x, y]).T, np.array(z),\n",
        "                                  (xx, yy), method='nearest')\n",
        "    vals[np.isnan(vals)] = vals_0[np.isnan(vals)]\n",
        "\n",
        "    if vmin is not None and vmax is not None:\n",
        "        fig = plt.imshow(vals,\n",
        "                         extent=[np.min(x), np.max(x), np.min(y), np.max(y)],\n",
        "                         aspect=\"equal\", interpolation=\"bicubic\", cmap=cmap,\n",
        "                         vmin=vmin, vmax=vmax, origin='lower')\n",
        "    elif vmin is not None:\n",
        "        fig = plt.imshow(vals,\n",
        "                         extent=[np.min(x), np.max(x), np.min(y), np.max(y)],\n",
        "                         aspect=\"equal\", interpolation=\"bicubic\", cmap=cmap,\n",
        "                         vmin=vmin, origin='lower')\n",
        "    else:\n",
        "        fig = plt.imshow(vals,\n",
        "                         extent=[np.min(x), np.max(x), np.min(y), np.max(y)], cmap=cmap,\n",
        "                         aspect=\"equal\", interpolation=\"bicubic\", origin='lower')\n",
        "    fig.axes.set_autoscale_on(False)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    if path:\n",
        "        plt.savefig(path)\n",
        "    if show:\n",
        "        plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "import contextlib\n",
        "\n",
        "\n",
        "class Interp1d(torch.autograd.Function):\n",
        "    def __call__(self, x, y, xnew, out=None):\n",
        "        return self.forward(x, y, xnew, out)\n",
        "\n",
        "    def forward(ctx, x, y, xnew, out=None):\n",
        "        \"\"\"\n",
        "        Linear 1D interpolation on the GPU for Pytorch.\n",
        "        This function returns interpolated values of a set of 1-D functions at\n",
        "        the desired query points `xnew`.\n",
        "        This function is working similarly to Matlab™ or scipy functions with\n",
        "        the `linear` interpolation mode on, except that it parallelises over\n",
        "        any number of desired interpolation problems.\n",
        "        The code will run on GPU if all the tensors provided are on a cuda\n",
        "        device.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : (N, ) or (D, N) Pytorch Tensor\n",
        "            A 1-D or 2-D tensor of real values.\n",
        "        y : (N,) or (D, N) Pytorch Tensor\n",
        "            A 1-D or 2-D tensor of real values. The length of `y` along its\n",
        "            last dimension must be the same as that of `x`\n",
        "        xnew : (P,) or (D, P) Pytorch Tensor\n",
        "            A 1-D or 2-D tensor of real values. `xnew` can only be 1-D if\n",
        "            _both_ `x` and `y` are 1-D. Otherwise, its length along the first\n",
        "            dimension must be the same as that of whichever `x` and `y` is 2-D.\n",
        "        out : Pytorch Tensor, same shape as `xnew`\n",
        "            Tensor for the output. If None: allocated automatically.\n",
        "        \"\"\"\n",
        "        # making the vectors at least 2D\n",
        "        is_flat = {}\n",
        "        require_grad = {}\n",
        "        v = {}\n",
        "        device = []\n",
        "        eps = torch.finfo(y.dtype).eps\n",
        "        for name, vec in {'x': x, 'y': y, 'xnew': xnew}.items():\n",
        "            assert len(vec.shape) <= 2, 'interp1d: all inputs must be ' \\\n",
        "                                        'at most 2-D.'\n",
        "            if len(vec.shape) == 1:\n",
        "                v[name] = vec[None, :]\n",
        "            else:\n",
        "                v[name] = vec\n",
        "            is_flat[name] = v[name].shape[0] == 1\n",
        "            require_grad[name] = vec.requires_grad\n",
        "            device = list(set(device + [str(vec.device)]))\n",
        "        assert len(device) == 1, 'All parameters must be on the same device.'\n",
        "        device = device[0]\n",
        "\n",
        "        # Checking for the dimensions\n",
        "        assert (v['x'].shape[1] == v['y'].shape[1]\n",
        "                and (\n",
        "                        v['x'].shape[0] == v['y'].shape[0]\n",
        "                        or v['x'].shape[0] == 1\n",
        "                        or v['y'].shape[0] == 1\n",
        "                )\n",
        "                ), (\"x and y must have the same number of columns, and either \"\n",
        "                    \"the same number of row or one of them having only one \"\n",
        "                    \"row.\")\n",
        "\n",
        "        reshaped_xnew = False\n",
        "        if ((v['x'].shape[0] == 1) and (v['y'].shape[0] == 1)\n",
        "                and (v['xnew'].shape[0] > 1)):\n",
        "            # if there is only one row for both x and y, there is no need to\n",
        "            # loop over the rows of xnew because they will all have to face the\n",
        "            # same interpolation problem. We should just stack them together to\n",
        "            # call interp1d and put them back in place afterwards.\n",
        "            original_xnew_shape = v['xnew'].shape\n",
        "            v['xnew'] = v['xnew'].contiguous().view(1, -1)\n",
        "            reshaped_xnew = True\n",
        "\n",
        "        # identify the dimensions of output and check if the one provided is ok\n",
        "        D = max(v['x'].shape[0], v['xnew'].shape[0])\n",
        "        shape_ynew = (D, v['xnew'].shape[-1])\n",
        "        if out is not None:\n",
        "            if out.numel() != shape_ynew[0] * shape_ynew[1]:\n",
        "                # The output provided is of incorrect shape.\n",
        "                # Going for a new one\n",
        "                out = None\n",
        "            else:\n",
        "                ynew = out.reshape(shape_ynew)\n",
        "        if out is None:\n",
        "            ynew = torch.zeros(*shape_ynew, device=device)\n",
        "\n",
        "        # moving everything to the desired device in case it was not there\n",
        "        # already (not handling the case things do not fit entirely, user will\n",
        "        # do it if required.)\n",
        "        for name in v:\n",
        "            v[name] = v[name].to(device)\n",
        "\n",
        "        # calling searchsorted on the x values.\n",
        "        ind = ynew.long()\n",
        "\n",
        "        # expanding xnew to match the number of rows of x in case only one xnew is\n",
        "        # provided\n",
        "        if v['xnew'].shape[0] == 1:\n",
        "            v['xnew'] = v['xnew'].expand(v['x'].shape[0], -1)\n",
        "\n",
        "        torch.searchsorted(v['x'].contiguous(),\n",
        "                           v['xnew'].contiguous(), out=ind)\n",
        "\n",
        "        # the `-1` is because searchsorted looks for the index where the values\n",
        "        # must be inserted to preserve order. And we want the index of the\n",
        "        # preceeding value.\n",
        "        ind -= 1\n",
        "        # we clamp the index, because the number of intervals is x.shape-1,\n",
        "        # and the left neighbour should hence be at most number of intervals\n",
        "        # -1, i.e. number of columns in x -2\n",
        "        ind = torch.clamp(ind, 0, v['x'].shape[1] - 1 - 1)\n",
        "\n",
        "        # helper function to select stuff according to the found indices.\n",
        "        def sel(name):\n",
        "            if is_flat[name]:\n",
        "                return v[name].contiguous().view(-1)[ind]\n",
        "            return torch.gather(v[name], 1, ind)\n",
        "\n",
        "        # activating gradient storing for everything now\n",
        "        enable_grad = False\n",
        "        saved_inputs = []\n",
        "        for name in ['x', 'y', 'xnew']:\n",
        "            if require_grad[name]:\n",
        "                enable_grad = True\n",
        "                saved_inputs += [v[name]]\n",
        "            else:\n",
        "                saved_inputs += [None, ]\n",
        "        # assuming x are sorted in the dimension 1, computing the slopes for\n",
        "        # the segments\n",
        "        is_flat['slopes'] = is_flat['x']\n",
        "        # now we have found the indices of the neighbors, we start building the\n",
        "        # output. Hence, we start also activating gradient tracking\n",
        "        with torch.enable_grad() if enable_grad else contextlib.suppress():\n",
        "            v['slopes'] = (\n",
        "                    (v['y'][:, 1:] - v['y'][:, :-1])\n",
        "                    /\n",
        "                    (eps + (v['x'][:, 1:] - v['x'][:, :-1]))\n",
        "            )\n",
        "\n",
        "            # now build the linear interpolation\n",
        "            ynew = sel('y') + sel('slopes') * (\n",
        "                    v['xnew'] - sel('x'))\n",
        "\n",
        "            if reshaped_xnew:\n",
        "                ynew = ynew.view(original_xnew_shape)\n",
        "\n",
        "        ctx.save_for_backward(ynew, *saved_inputs)\n",
        "        return ynew\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_out):\n",
        "        inputs = ctx.saved_tensors[1:]\n",
        "        gradients = torch.autograd.grad(\n",
        "            ctx.saved_tensors[0],\n",
        "            [i for i in inputs if i is not None],\n",
        "            grad_out, retain_graph=True)\n",
        "        result = [None, ] * 5\n",
        "        pos = 0\n",
        "        for index in range(len(inputs)):\n",
        "            if inputs[index] is not None:\n",
        "                result[index] = gradients[pos]\n",
        "                pos += 1\n",
        "        return (*result,)\n",
        "\n",
        "\n",
        "class TorchQuantileTransformer():\n",
        "    '''\n",
        "    QuantileTransformer implemented by PyTorch\n",
        "    '''\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            output_distribution,\n",
        "            references_,\n",
        "            quantiles_,\n",
        "            device=torch.device('cpu')\n",
        "    ) -> None:\n",
        "        self.quantiles_ = torch.Tensor(quantiles_).to(device)\n",
        "        self.output_distribution = output_distribution\n",
        "        self._norm_pdf_C = np.sqrt(2 * np.pi)\n",
        "        self.references_ = torch.Tensor(references_).to(device)\n",
        "        BOUNDS_THRESHOLD = 1e-7\n",
        "        self.clip_min = self.norm_ppf(torch.Tensor([BOUNDS_THRESHOLD - np.spacing(1)]))\n",
        "        self.clip_max = self.norm_ppf(torch.Tensor([1 - (BOUNDS_THRESHOLD - np.spacing(1))]))\n",
        "\n",
        "    def norm_pdf(self, x):\n",
        "        return torch.exp(-x ** 2 / 2.0) / self._norm_pdf_C\n",
        "\n",
        "    @staticmethod\n",
        "    def norm_cdf(x):\n",
        "        return ts.ndtr(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def norm_ppf(x):\n",
        "        return ts.ndtri(x)\n",
        "\n",
        "    def transform_col(self, X_col, quantiles, inverse):\n",
        "        BOUNDS_THRESHOLD = 1e-7\n",
        "        output_distribution = self.output_distribution\n",
        "\n",
        "        if not inverse:\n",
        "            lower_bound_x = quantiles[0]\n",
        "            upper_bound_x = quantiles[-1]\n",
        "            lower_bound_y = 0\n",
        "            upper_bound_y = 1\n",
        "        else:\n",
        "            lower_bound_x = 0\n",
        "            upper_bound_x = 1\n",
        "            lower_bound_y = quantiles[0]\n",
        "            upper_bound_y = quantiles[-1]\n",
        "            # for inverse transform, match a uniform distribution\n",
        "            with np.errstate(invalid=\"ignore\"):  # hide NaN comparison warnings\n",
        "                if output_distribution == \"normal\":\n",
        "                    X_col = self.norm_cdf(X_col)\n",
        "                # else output distribution is already a uniform distribution\n",
        "\n",
        "        # find index for lower and higher bounds\n",
        "        with np.errstate(invalid=\"ignore\"):  # hide NaN comparison warnings\n",
        "            if output_distribution == \"normal\":\n",
        "                lower_bounds_idx = X_col - BOUNDS_THRESHOLD < lower_bound_x\n",
        "                upper_bounds_idx = X_col + BOUNDS_THRESHOLD > upper_bound_x\n",
        "            if output_distribution == \"uniform\":\n",
        "                lower_bounds_idx = X_col == lower_bound_x\n",
        "                upper_bounds_idx = X_col == upper_bound_x\n",
        "\n",
        "        isfinite_mask = ~torch.isnan(X_col)\n",
        "        X_col_finite = X_col[isfinite_mask]\n",
        "        torch_interp = Interp1d()\n",
        "        X_col_out = X_col.clone()\n",
        "        if not inverse:\n",
        "            # Interpolate in one direction and in the other and take the\n",
        "            # mean. This is in case of repeated values in the features\n",
        "            # and hence repeated quantiles\n",
        "            #\n",
        "            # If we don't do this, only one extreme of the duplicated is\n",
        "            # used (the upper when we do ascending, and the\n",
        "            # lower for descending). We take the mean of these two\n",
        "            X_col_out[isfinite_mask] = 0.5 * (\n",
        "                    torch_interp(quantiles, self.references_, X_col_finite)\n",
        "                    - torch_interp(-torch.flip(quantiles, [0]), -torch.flip(self.references_, [0]), -X_col_finite)\n",
        "            )\n",
        "        else:\n",
        "            X_col_out[isfinite_mask] = torch_interp(self.references_, quantiles, X_col_finite)\n",
        "\n",
        "        X_col_out[upper_bounds_idx] = upper_bound_y\n",
        "        X_col_out[lower_bounds_idx] = lower_bound_y\n",
        "        # for forward transform, match the output distribution\n",
        "        if not inverse:\n",
        "            with np.errstate(invalid=\"ignore\"):  # hide NaN comparison warnings\n",
        "                if output_distribution == \"normal\":\n",
        "                    X_col_out = self.norm_ppf(X_col_out)\n",
        "                    # find the value to clip the data to avoid mapping to\n",
        "                    # infinity. Clip such that the inverse transform will be\n",
        "                    # consistent\n",
        "                    X_col_out = torch.clip(X_col_out, self.clip_min, self.clip_max)\n",
        "                # else output distribution is uniform and the ppf is the\n",
        "                # identity function so we let X_col unchanged\n",
        "\n",
        "        return X_col_out\n",
        "\n",
        "    def transform(self, X, inverse=True, component='all'):\n",
        "        X_out = torch.zeros_like(X, requires_grad=False)\n",
        "        for feature_idx in range(X.shape[1]):\n",
        "            X_out[:, feature_idx] = self.transform_col(\n",
        "                X[:, feature_idx], self.quantiles_[:, feature_idx], inverse\n",
        "            )\n",
        "        return X_out\n",
        "\n",
        "    def to(self, device):\n",
        "        self.quantiles_ = self.quantiles_.to(device)\n",
        "        self.references_ = self.references_.to(device)\n",
        "        return self\n",
        "\n",
        "\n",
        "'''\n",
        "    Simple normalization layer\n",
        "'''\n",
        "\n",
        "\n",
        "class UnitTransformer():\n",
        "    def __init__(self, X):\n",
        "        self.mean = X.mean(dim=0, keepdim=True)\n",
        "        self.std = X.std(dim=0, keepdim=True) + 1e-8\n",
        "\n",
        "    def to(self, device):\n",
        "        self.mean = self.mean.to(device)\n",
        "        self.std = self.std.to(device)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, inverse=True, component='all'):\n",
        "        if component == 'all' or 'all-reduce':\n",
        "            if inverse:\n",
        "                orig_shape = X.shape\n",
        "                return (X * (self.std - 1e-8) + self.mean).view(orig_shape)\n",
        "            else:\n",
        "                return (X - self.mean) / self.std\n",
        "        else:\n",
        "            if inverse:\n",
        "                orig_shape = X.shape\n",
        "                return (X * (self.std[:, component] - 1e-8) + self.mean[:, component]).view(orig_shape)\n",
        "            else:\n",
        "                return (X - self.mean[:, component]) / self.std[:, component]\n",
        "\n",
        "\n",
        "'''\n",
        "    Simple pointwise normalization layer, all data must contain the same length, used only for FNO datasets\n",
        "    X: B, N, C\n",
        "'''\n",
        "\n",
        "\n",
        "class PointWiseUnitTransformer():\n",
        "    def __init__(self, X):\n",
        "        self.mean = X.mean(dim=0, keepdim=False)\n",
        "        self.std = X.std(dim=0, keepdim=False) + 1e-8\n",
        "\n",
        "    def to(self, device):\n",
        "        self.mean = self.mean.to(device)\n",
        "        self.std = self.std.to(device)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, inverse=True, component='all'):\n",
        "        if component == 'all' or 'all-reduce':\n",
        "            if inverse:\n",
        "                orig_shape = X.shape\n",
        "                X = X.view(-1, self.mean.shape[0], self.mean.shape[1])  ### align shape for flat tensor\n",
        "                return (X * (self.std - 1e-8) + self.mean).view(orig_shape)\n",
        "            else:\n",
        "                return (X - self.mean) / self.std\n",
        "        else:\n",
        "            if inverse:\n",
        "                orig_shape = X.shape\n",
        "                X = X.view(-1, self.mean.shape[0], self.mean.shape[1])\n",
        "                return (X * (self.std[:, component] - 1e-8) + self.mean[:, component]).view(orig_shape)\n",
        "            else:\n",
        "                return (X - self.mean[:, component]) / self.std[:, component]\n",
        "\n",
        "\n",
        "'''\n",
        "    x: B, N (not necessary sorted)\n",
        "    y: B, N, C (not necessary sorted)\n",
        "    xnew: B, N (sorted)\n",
        "'''\n",
        "\n",
        "\n",
        "def binterp1d(x, y, xnew, eps=1e-9):\n",
        "    x_, x_indice = torch.sort(x, dim=-1)\n",
        "    y_ = y[torch.arange(x_.shape[0]).unsqueeze(1), x_indice]\n",
        "\n",
        "    x_, y_, xnew = x_.contiguous(), y_.contiguous(), xnew.contiguous()\n",
        "\n",
        "    ind = torch.searchsorted(x_, xnew)\n",
        "    ind -= 1\n",
        "    ind = torch.clamp(ind, 0, x_.shape[1] - 1 - 1)\n",
        "    ind = ind.unsqueeze(-1).repeat([1, 1, y_.shape[-1]])\n",
        "    x_ = x_.unsqueeze(-1).repeat([1, 1, y_.shape[-1]])\n",
        "\n",
        "    slopes = ((y_[:, 1:] - y_[:, :-1]) / (eps + (x_[:, 1:] - x_[:, :-1])))\n",
        "\n",
        "    y_sel = torch.gather(y_, 1, ind)\n",
        "    x_sel = torch.gather(x_, 1, ind)\n",
        "    slopes_sel = torch.gather(slopes, 1, ind)\n",
        "\n",
        "    ynew = y_sel + slopes_sel * (xnew.unsqueeze(-1) - x_sel)\n",
        "\n",
        "    return ynew\n"
      ],
      "metadata": {
        "id": "hFW1jQ1VnUt0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import List, Optional\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "def adam(params: List[Tensor],\n",
        "         grads: List[Tensor],\n",
        "         exp_avgs: List[Tensor],\n",
        "         exp_avg_sqs: List[Tensor],\n",
        "         max_exp_avg_sqs: List[Tensor],\n",
        "         state_steps: List[int],\n",
        "         *,\n",
        "         amsgrad: bool,\n",
        "         beta1: float,\n",
        "         beta2: float,\n",
        "         lr: float,\n",
        "         weight_decay: float,\n",
        "         eps: float):\n",
        "    r\"\"\"Functional API that performs Adam algorithm computation.\n",
        "    See :class:`~torch.optim.Adam` for details.\n",
        "    \"\"\"\n",
        "\n",
        "    for i, param in enumerate(params):\n",
        "\n",
        "        grad = grads[i]\n",
        "        exp_avg = exp_avgs[i]\n",
        "        exp_avg_sq = exp_avg_sqs[i]\n",
        "        step = state_steps[i]\n",
        "\n",
        "        bias_correction1 = 1 - beta1 ** step\n",
        "        bias_correction2 = 1 - beta2 ** step\n",
        "\n",
        "        if weight_decay != 0:\n",
        "            grad = grad.add(param, alpha=weight_decay)\n",
        "\n",
        "        # Decay the first and second moment running average coefficient\n",
        "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)\n",
        "        if amsgrad:\n",
        "            # Maintains the maximum of all 2nd moment running avg. till now\n",
        "            torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out=max_exp_avg_sqs[i])\n",
        "            # Use the max. for normalizing running avg. of gradient\n",
        "            denom = (max_exp_avg_sqs[i].sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
        "        else:\n",
        "            denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
        "\n",
        "        step_size = lr / bias_correction1\n",
        "\n",
        "        param.addcdiv_(exp_avg, denom, value=-step_size)\n",
        "\n",
        "\n",
        "class Adam(Optimizer):\n",
        "    r\"\"\"Implements Adam algorithm.\n",
        "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
        "    The implementation of the L2 penalty follows changes proposed in\n",
        "    `Decoupled Weight Decay Regularization`_.\n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
        "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
        "            (default: False)\n",
        "    .. _Adam\\: A Method for Stochastic Optimization:\n",
        "        https://arxiv.org/abs/1412.6980\n",
        "    .. _Decoupled Weight Decay Regularization:\n",
        "        https://arxiv.org/abs/1711.05101\n",
        "    .. _On the Convergence of Adam and Beyond:\n",
        "        https://openreview.net/forum?id=ryQu7f-RZ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, amsgrad=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
        "        super(Adam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Adam, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            params_with_grad = []\n",
        "            grads = []\n",
        "            exp_avgs = []\n",
        "            exp_avg_sqs = []\n",
        "            max_exp_avg_sqs = []\n",
        "            state_steps = []\n",
        "            beta1, beta2 = group['betas']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    params_with_grad.append(p)\n",
        "                    if p.grad.is_sparse:\n",
        "                        raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                    grads.append(p.grad)\n",
        "\n",
        "                    state = self.state[p]\n",
        "                    # Lazy state initialization\n",
        "                    if len(state) == 0:\n",
        "                        state['step'] = 0\n",
        "                        # Exponential moving average of gradient values\n",
        "                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                        # Exponential moving average of squared gradient values\n",
        "                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                        if group['amsgrad']:\n",
        "                            # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                            state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                    exp_avgs.append(state['exp_avg'])\n",
        "                    exp_avg_sqs.append(state['exp_avg_sq'])\n",
        "\n",
        "                    if group['amsgrad']:\n",
        "                        max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
        "\n",
        "                    # update the steps for each param group update\n",
        "                    state['step'] += 1\n",
        "                    # record the step after step update\n",
        "                    state_steps.append(state['step'])\n",
        "\n",
        "            adam(params_with_grad,\n",
        "                 grads,\n",
        "                 exp_avgs,\n",
        "                 exp_avg_sqs,\n",
        "                 max_exp_avg_sqs,\n",
        "                 state_steps,\n",
        "                 amsgrad=group['amsgrad'],\n",
        "                 beta1=beta1,\n",
        "                 beta2=beta2,\n",
        "                 lr=group['lr'],\n",
        "                 weight_decay=group['weight_decay'],\n",
        "                 eps=group['eps'])\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def adamw(params: List[Tensor],\n",
        "          grads: List[Tensor],\n",
        "          exp_avgs: List[Tensor],\n",
        "          exp_avg_sqs: List[Tensor],\n",
        "          max_exp_avg_sqs: List[Tensor],\n",
        "          state_steps: List[int],\n",
        "          *,\n",
        "          amsgrad: bool,\n",
        "          beta1: float,\n",
        "          beta2: float,\n",
        "          lr: float,\n",
        "          weight_decay: float,\n",
        "          eps: float):\n",
        "    r\"\"\"Functional API that performs AdamW algorithm computation.\n",
        "\n",
        "    See :class:`~torch.optim.AdamW` for details.\n",
        "    \"\"\"\n",
        "    for i, param in enumerate(params):\n",
        "        grad = grads[i]\n",
        "        exp_avg = exp_avgs[i]\n",
        "        exp_avg_sq = exp_avg_sqs[i]\n",
        "        step = state_steps[i]\n",
        "\n",
        "        # Perform stepweight decay\n",
        "        param.mul_(1 - lr * weight_decay)\n",
        "\n",
        "        bias_correction1 = 1 - beta1 ** step\n",
        "        bias_correction2 = 1 - beta2 ** step\n",
        "\n",
        "        # Decay the first and second moment running average coefficient\n",
        "        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "        exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)\n",
        "        if amsgrad:\n",
        "            # Maintains the maximum of all 2nd moment running avg. till now\n",
        "            torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out=max_exp_avg_sqs[i])\n",
        "            # Use the max. for normalizing running avg. of gradient\n",
        "            denom = (max_exp_avg_sqs[i].sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
        "        else:\n",
        "            denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n",
        "\n",
        "        step_size = lr / bias_correction1\n",
        "\n",
        "        param.addcdiv_(exp_avg, denom, value=-step_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Adamw(Optimizer):\n",
        "    r\"\"\"Implements AdamW algorithm.\n",
        "\n",
        "    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.\n",
        "    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.\n",
        "\n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n",
        "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
        "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
        "            (default: False)\n",
        "\n",
        "    .. _Adam\\: A Method for Stochastic Optimization:\n",
        "        https://arxiv.org/abs/1412.6980\n",
        "    .. _Decoupled Weight Decay Regularization:\n",
        "        https://arxiv.org/abs/1711.05101\n",
        "    .. _On the Convergence of Adam and Beyond:\n",
        "        https://openreview.net/forum?id=ryQu7f-RZ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=1e-2, amsgrad=False):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
        "        super(Adamw, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Adamw, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Args:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            params_with_grad = []\n",
        "            grads = []\n",
        "            exp_avgs = []\n",
        "            exp_avg_sqs = []\n",
        "            state_sums = []\n",
        "            max_exp_avg_sqs = []\n",
        "            state_steps = []\n",
        "            amsgrad = group['amsgrad']\n",
        "            beta1, beta2 = group['betas']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                params_with_grad.append(p)\n",
        "                if p.grad.is_sparse:\n",
        "                    raise RuntimeError('AdamW does not support sparse gradients')\n",
        "                grads.append(p.grad)\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                    if amsgrad:\n",
        "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                        state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                exp_avgs.append(state['exp_avg'])\n",
        "                exp_avg_sqs.append(state['exp_avg_sq'])\n",
        "\n",
        "                if amsgrad:\n",
        "                    max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n",
        "\n",
        "                # update the steps for each param group update\n",
        "                state['step'] += 1\n",
        "                # record the step after step update\n",
        "                state_steps.append(state['step'])\n",
        "\n",
        "            adamw(params_with_grad,\n",
        "                    grads,\n",
        "                    exp_avgs,\n",
        "                    exp_avg_sqs,\n",
        "                    max_exp_avg_sqs,\n",
        "                    state_steps,\n",
        "                    amsgrad=amsgrad,\n",
        "                    beta1=beta1,\n",
        "                    beta2=beta2,\n",
        "                    lr=group['lr'],\n",
        "                    weight_decay=group['weight_decay'],\n",
        "                    eps=group['eps'])\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "-nHl5bmAk8-L"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader\n"
      ],
      "metadata": {
        "id": "SjjX6nDgnKwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# Code is heavily based on paper \"Geometry-Informed Neural Operator for Large-Scale 3D PDEs\", we use paddle to reproduce the results of the paper\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class BaseDataModule:\n",
        "\n",
        "    @property\n",
        "    def train_dataset(self) -> Dataset:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def val_dataset(self) -> Dataset:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def test_dataset(self) -> Dataset:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def train_dataloader(self, **kwargs) -> DataLoader:\n",
        "        collate_fn = getattr(self, 'collate_fn', None)\n",
        "        return DataLoader(self.train_data, collate_fn=\n",
        "        collate_fn, **kwargs)\n",
        "\n",
        "    def val_dataloader(self, **kwargs) -> DataLoader:\n",
        "        collate_fn = getattr(self, 'collate_fn', None)\n",
        "        return DataLoader(self.val_data, collate_fn=\n",
        "        collate_fn, **kwargs)\n",
        "\n",
        "    def test_dataloader(self, **kwargs) -> DataLoader:\n",
        "        collate_fn = getattr(self, 'collate_fn', None)\n",
        "        return DataLoader(self.test_data, collate_fn=\n",
        "        collate_fn, **kwargs)\n",
        "\n",
        "import torch\n",
        "import open3d as o3d\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "class AddGaussianNoiseWithProbability(object):\n",
        "    def __init__(self, mean=0.0, std=1.0, probability=0.4):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.probability = probability\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        if random.random() < self.probability:\n",
        "            noise = torch.randn(tensor.size()) * self.std + self.mean\n",
        "            return tensor + noise\n",
        "        else:\n",
        "            return tensor\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1}, probability={2})'.format(self.mean, self.std,\n",
        "                                                                                       self.probability)\n",
        "\n",
        "class DictDataset(Dataset):\n",
        "    def __init__(self, data_dict: dict):\n",
        "        self.data_dict = data_dict\n",
        "        for k, v in data_dict.items():\n",
        "            assert len(v) == len(\n",
        "                data_dict[list(data_dict.keys())[0]]\n",
        "            ), \"All data must have the same length\"\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {k: v[index] for k, v in self.data_dict.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_dict[list(self.data_dict.keys())[0]])\n",
        "\n",
        "\n",
        "class DictDatasetWithConstant(DictDataset):\n",
        "    def __init__(self, data_dict: dict, constant_dict: dict,use_transform=False):\n",
        "        super().__init__(data_dict)\n",
        "        self.constant_dict = constant_dict\n",
        "        self.use_transform=use_transform\n",
        "        self.transform = AddGaussianNoiseWithProbability(0.0, 0.05)\n",
        "    def __getitem__(self, index):\n",
        "        return_dict = {k: v[index] for k, v in self.data_dict.items()}\n",
        "        return_dict.update(self.constant_dict)\n",
        "        if self.use_transform:\n",
        "            return_dict['vertices'] = self.transform(return_dict['vertices'])\n",
        "        return return_dict\n",
        "\n",
        "\n",
        "class CarDataset(BaseDataModule):\n",
        "    def __init__(\n",
        "            self,\n",
        "            train_data_dir,\n",
        "            test_data_dir,\n",
        "            n_train: int = 500,\n",
        "            n_test: int = 5,\n",
        "            val_ratio=0.1,\n",
        "            expand_data=False,\n",
        "            track='track_A',\n",
        "            wind_direction='z',use_transform=False):\n",
        "        BaseDataModule.__init__(self)\n",
        "\n",
        "        valid_train_indices = self.load_valid_mesh_indices(train_data_dir)\n",
        "        valid_test_indices = self.load_valid_mesh_indices(test_data_dir)\n",
        "        random.shuffle(valid_train_indices)\n",
        "        val_num = int(n_train * val_ratio)\n",
        "        valid_val_indices = valid_train_indices[:val_num]\n",
        "        valid_train_indices = valid_train_indices[val_num:]\n",
        "        self.all_train_mesh_path = [self.get_mesh_path_train(train_data_dir, i) for i in valid_train_indices]\n",
        "        self.all_val_mesh_path = [self.get_mesh_path_train(train_data_dir, i) for i in valid_val_indices]\n",
        "        self.all_test_mesh_path = [self.get_mesh_path_test(test_data_dir, i) for i in valid_test_indices]\n",
        "\n",
        "        all_train_vertices = torch.stack(\n",
        "            [\n",
        "                torch.tensor(self.vertices_from_mesh(i)) for i in self.all_train_mesh_path\n",
        "            ]\n",
        "        )\n",
        "        all_val_vertices = torch.stack(\n",
        "            [\n",
        "                torch.tensor(self.vertices_from_mesh(i)) for i in self.all_val_mesh_path\n",
        "            ]\n",
        "        )\n",
        "        all_test_vertices = torch.stack(\n",
        "            [\n",
        "                torch.tensor(self.vertices_from_mesh(i)) for i in self.all_test_mesh_path\n",
        "            ]\n",
        "        )\n",
        "        all_train_press = torch.stack(\n",
        "            [\n",
        "                torch.tensor(\n",
        "                    self.load_pressure(train_data_dir, meshidx), dtype=torch.float32\n",
        "                )\n",
        "                for meshidx in valid_train_indices\n",
        "            ]\n",
        "        )\n",
        "        all_val_press = torch.stack(\n",
        "            [\n",
        "                torch.tensor(\n",
        "                    self.load_pressure(train_data_dir, meshidx), dtype=torch.float32\n",
        "                )\n",
        "                for meshidx in valid_val_indices\n",
        "            ]\n",
        "        )\n",
        "        all_test_press = torch.stack(\n",
        "            [\n",
        "                torch.tensor(\n",
        "                    self.load_pressure(test_data_dir, meshidx), dtype=torch.float32\n",
        "                )\n",
        "                for meshidx in valid_test_indices\n",
        "            ]\n",
        "        )\n",
        "        if track == 'track_A':\n",
        "            track_token = torch.tensor([1, 0, 0], dtype=torch.float32).unsqueeze(0)\n",
        "        else:\n",
        "            track_token = torch.tensor([0, 1, 0], dtype=torch.float32).unsqueeze(0)\n",
        "        if wind_direction == 'x':\n",
        "            wind_token = torch.tensor([1, 0, 0], dtype=torch.float32).unsqueeze(0)\n",
        "        elif wind_direction == 'z':\n",
        "            wind_token = torch.tensor([0, 1, 0], dtype=torch.float32).unsqueeze(0)\n",
        "        if expand_data:\n",
        "            all_train_vertices = torch.cat([all_train_vertices, all_train_vertices], dim=0)\n",
        "            all_train_press = torch.cat([all_train_press, all_train_press], dim=0)\n",
        "        print(f'use {len(all_train_press)} training data!')\n",
        "        self._train_data = DictDatasetWithConstant(\n",
        "            {\"vertices\": all_train_vertices,\n",
        "             \"pressure\": all_train_press},\n",
        "            {\"track\": track_token,\n",
        "             'wind_direction': wind_token},\n",
        "            use_transform\n",
        "        )\n",
        "        self._val_data = DictDatasetWithConstant(\n",
        "            {\"vertices\": all_val_vertices,\n",
        "             \"pressure\": all_val_press},\n",
        "            {\"track\": track_token,\n",
        "             'wind_direction': wind_token},\n",
        "        )\n",
        "        self._test_data = DictDatasetWithConstant(\n",
        "            {\"vertices\": all_test_vertices,\n",
        "             \"pressure\": all_test_press},\n",
        "            {\"track\": track_token,\n",
        "             'wind_direction': wind_token},\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def train_data(self):\n",
        "        return self._train_data\n",
        "\n",
        "    @property\n",
        "    def test_data(self):\n",
        "        return self._test_data\n",
        "\n",
        "    @property\n",
        "    def val_data(self):\n",
        "        return self._val_data\n",
        "\n",
        "    def load_pressure(self, data_dir: Path, mesh_index: int) -> np.ndarray:\n",
        "        press_path = self.get_pressure_data_path(data_dir, mesh_index)\n",
        "        assert press_path.exists(), \"Pressure data does not exist\"\n",
        "        press = np.load(press_path).reshape((-1,)).astype(np.float32)\n",
        "        press = np.concatenate((press[0:16], press[112:]), axis=0)\n",
        "        return press\n",
        "\n",
        "    def get_mesh_path(self, data_dir: Path, mesh_ind: int) -> Path:\n",
        "        return data_dir / (\"mesh_\" + str(mesh_ind).zfill(3) + \".ply\")\n",
        "\n",
        "    def get_mesh_path_train(self, data_dir: Path, mesh_ind: int) -> Path:\n",
        "        return data_dir / 'Feature' / (\"mesh_\" + str(mesh_ind).zfill(3) + \".ply\")\n",
        "\n",
        "    def get_mesh_path_test(self, data_dir: Path, mesh_ind: int) -> Path:\n",
        "        return data_dir / 'Inference' / (\"mesh_\" + str(mesh_ind).zfill(3) + \".ply\")\n",
        "\n",
        "    def get_pressure_data_path(self, data_dir: Path, mesh_ind: int) -> Path:\n",
        "        return data_dir / 'Label' / (\"press_\" + str(mesh_ind).zfill(3) + \".npy\")\n",
        "\n",
        "    def load_mesh(self, mesh_path: Path) -> o3d.t.geometry.TriangleMesh:\n",
        "        assert mesh_path.exists(), \"Mesh path does not exist\"\n",
        "        mesh = o3d.io.read_triangle_mesh(str(mesh_path))\n",
        "        mesh = o3d.t.geometry.TriangleMesh.from_legacy(mesh)\n",
        "        return mesh\n",
        "\n",
        "    def vertices_from_mesh(self, mesh_path: Path):\n",
        "        mesh = self.load_mesh(mesh_path)\n",
        "        vertices = mesh.vertex.positions.numpy()\n",
        "        return vertices\n",
        "\n",
        "    def load_valid_mesh_indices(\n",
        "            self, data_dir, filename=\"watertight_meshes.txt\"\n",
        "    ):\n",
        "        with open(data_dir / filename, \"r\") as fp:\n",
        "            mesh_ind = fp.read().split(\"\\n\")\n",
        "            mesh_ind = [int(a) for a in mesh_ind if a.strip()]\n",
        "        return mesh_ind\n",
        "\n",
        "\n",
        "class CdDataset(Dataset):\n",
        "    def __init__(self, data_root, cd_file_path):\n",
        "        cd_data = np.genfromtxt(cd_file_path, delimiter=\",\", dtype=str, encoding='utf-8')\n",
        "        cd_list = cd_data[:, 2][1:501].astype(np.float32)\n",
        "        cd_name = cd_data[:, 1][1:501]\n",
        "        self.bad_file=['1328a95d69cefe32f200a72c9245aee7_aug','22d477830b1bbbded536c1ebda275556','17ac544cdfbf74b999c8924280047dd9']\n",
        "        good_indices = ~np.isin(cd_name, self.bad_file)\n",
        "        filtered_cd_data = cd_list[good_indices].astype(np.float32)\n",
        "        filtered_cd_names = cd_name[good_indices]\n",
        "        self.cd_name_dict = {n:cd for n,cd in zip(filtered_cd_names,filtered_cd_data)}\n",
        "        self.cd_list = filtered_cd_data\n",
        "        self.cd_name = filtered_cd_names\n",
        "        self.dir = Path(data_root)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        obj_name = self.cd_name[index]\n",
        "\n",
        "        cd_label=self.cd_name_dict[obj_name]\n",
        "        data_dict = read(self.dir / f\"{obj_name}.obj\")\n",
        "        wind = torch.tensor([0, 1, 0], dtype=torch.float32).unsqueeze(0)\n",
        "        data_dict['wind'] = wind\n",
        "        data_dict[\"cd\"] = cd_label\n",
        "\n",
        "        return data_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cd_name_dict)\n",
        "\n",
        "class CdTestDataset(Dataset):\n",
        "    def __init__(self, data_root):\n",
        "        self.data_list=os.listdir(data_root)\n",
        "        self.data_list.sort()\n",
        "        self.dir = Path(data_root)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        obj_name = self.data_list[index]\n",
        "        data_dict = read(self.dir / f\"{obj_name}\")\n",
        "        wind = torch.tensor([0, 1, 0], dtype=torch.float32).unsqueeze(0)\n",
        "        data_dict['wind'] = wind\n",
        "        return data_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xVtHRKMfkbhK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "velocity——Dataset\n"
      ],
      "metadata": {
        "id": "lGCb_1Lox2DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import open3d\n",
        "import os\n",
        "import vtk\n",
        "from vtk.util.numpy_support import vtk_to_numpy\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset ,DataLoader, random_split\n",
        "from time import time\n",
        "import pandas as pd\n",
        "import torch\n",
        "def read_ply(file_path):\n",
        "    reader = vtk.vtkPLYReader()\n",
        "    reader.SetFileName(file_path)\n",
        "    reader.Update()\n",
        "    polydata = reader.GetOutput()\n",
        "    return reader, polydata\n",
        "\n",
        "def read_obj(file_path):\n",
        "    reader = vtk.vtkOBJReader()\n",
        "    reader.SetFileName(file_path)\n",
        "    reader.Update()\n",
        "    polydata = reader.GetOutput()\n",
        "    return reader, polydata\n",
        "\n",
        "def read_vtk(file_path):\n",
        "    reader = vtk.vtkUnstructuredGridReader()\n",
        "    reader.SetFileName(file_path)\n",
        "    reader.Update()\n",
        "    polydata = reader.GetOutput()\n",
        "\n",
        "    # point_data_keys = [polydata.GetPointData().GetArrayName(i)for i in range(polydata.GetPointData().GetNumberOfArrays())]\n",
        "    # cell_data_keys = [polydata.GetCellData().GetArrayName(i)for i in range(polydata.GetCellData().GetNumberOfArrays())]\n",
        "    # print(\"Point Data Keys:\", point_data_keys)\n",
        "    # print(\"Cell Data Keys:\", cell_data_keys)\n",
        "    return reader, polydata\n",
        "\n",
        "def normals(polydata):\n",
        "    normals_filter = vtk.vtkPolyDataNormals()\n",
        "    normals_filter.SetInputData(polydata)\n",
        "    normals_filter.ComputeCellNormalsOn()\n",
        "    normals_filter.ConsistencyOn()\n",
        "    normals_filter.FlipNormalsOn()\n",
        "    normals_filter.AutoOrientNormalsOn()\n",
        "    normals_filter.Update()\n",
        "    numpy_cell_normals = vtk_to_numpy(normals_filter.GetOutput().GetCellData().GetNormals()).astype(np.float32)\n",
        "    return numpy_cell_normals\n",
        "\n",
        "def areas(polydata):\n",
        "    cell_size_filter = vtk.vtkCellSizeFilter()\n",
        "    cell_size_filter.SetInputData(polydata)\n",
        "    cell_size_filter.ComputeAreaOn()\n",
        "    cell_size_filter.Update()\n",
        "    numpy_cell_areas = vtk_to_numpy(cell_size_filter.GetOutput().GetCellData().GetArray(\"Area\")).astype(np.float32)\n",
        "    return numpy_cell_areas\n",
        "\n",
        "def centoirds(polydata):\n",
        "    cell_centers = vtk.vtkCellCenters()\n",
        "    cell_centers.SetInputData(polydata)\n",
        "    cell_centers.Update()\n",
        "    numpy_cell_centers = vtk_to_numpy(cell_centers.GetOutput().GetPoints().GetData()).astype(np.float32)\n",
        "    return numpy_cell_centers\n",
        "\n",
        "def nodes(polydata):\n",
        "    points = vtk_to_numpy(polydata.GetPoints().GetData()).astype(np.float32)\n",
        "    return points\n",
        "\n",
        "def velocity(polydata):\n",
        "    vel = vtk_to_numpy(polydata.GetPointData().GetArray(\"point_vectors\")).astype(np.float32)\n",
        "    return vel\n",
        "\n",
        "def load_sdf_queries():\n",
        "    tx = np.linspace(0, 1, 64)\n",
        "    ty = np.linspace(0, 1, 64)\n",
        "    tz = np.linspace(0, 1, 64)\n",
        "    sdf_q = np.stack(np.meshgrid(tx, ty, tz, indexing=\"ij\"), axis=-1).astype(np.float32)\n",
        "    sdf_q = np.transpose(sdf_q, (3,0,1,2))\n",
        "    return sdf_q\n",
        "\n",
        "def load_sdf():\n",
        "    sdf = np.ones([64,64,64]).astype(np.float32)\n",
        "    return sdf\n",
        "\n",
        "def read(file_path,train=True):\n",
        "    if file_path.suffix == \".ply\":\n",
        "        _, polydata = read_ply(file_path)\n",
        "        data_dict = {\n",
        "            \"centroids\":    centoirds(polydata),\n",
        "            \"areas\":        areas(polydata),\n",
        "            \"normal\":       normals(polydata),\n",
        "            \"sdf\":          load_sdf(),\n",
        "            \"sdf_query_points\":          load_sdf_queries(),\n",
        "        }\n",
        "    elif file_path.suffix == \".obj\":\n",
        "        _, polydata = read_obj(file_path)\n",
        "        vertices=nodes(polydata)\n",
        "        indices = np.random.permutation(vertices.shape[0])\n",
        "\n",
        "        # 抽取前8192行\n",
        "        sampled_indices = indices[:8192]\n",
        "\n",
        "        # 使用这些索引来抽取行\n",
        "        sampled_vertices = vertices[sampled_indices]\n",
        "        data_dict = {\n",
        "            # \"centroids\":    centoirds(polydata),\n",
        "            \"vertices\":     torch.tensor(sampled_vertices),\n",
        "            # \"areas\":        areas(polydata),\n",
        "            # \"normal\":       normals(polydata),\n",
        "            # \"sdf\":          load_sdf(),\n",
        "            # \"sdf_query_points\":          load_sdf_queries(),\n",
        "        }\n",
        "    elif file_path.suffix == \".vtk\":\n",
        "        if train:\n",
        "            _, polydata = read_vtk(file_path)\n",
        "\n",
        "            data_dict = {\n",
        "                \"vertices\":     torch.tensor(nodes(polydata)),\n",
        "                \"velocity\":     torch.tensor(velocity(polydata)),\n",
        "            }\n",
        "        else:\n",
        "            _, polydata = read_vtk(file_path)\n",
        "\n",
        "            data_dict = {\n",
        "                \"vertices\": torch.tensor(nodes(polydata)),\n",
        "                # \"velocity\": torch.tensor(velocity(polydata))\n",
        "            }\n",
        "    else:\n",
        "        raise NotImplemented\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "class Velocity_Dataset(Dataset):\n",
        "    def __init__(self,train_root,test_root,valid_train,valid_val,mode,norm_type=None):\n",
        "\n",
        "        all_valid_indices_train=self.load_valid_indices(valid_train)\n",
        "        all_valid_indices_test=self.load_valid_test_indices(test_root)\n",
        "        all_valid_indices_val=self.load_valid_indices(valid_val)\n",
        "        self.all_valid_indices_train=all_valid_indices_train\n",
        "        self.all_valid_indices_test=all_valid_indices_test\n",
        "        self.all_valid_indices_val=all_valid_indices_val\n",
        "        self.mode = mode\n",
        "        self.all_valid_path_train = [self.get_v_path(train_root,idx) for idx in all_valid_indices_train]\n",
        "        self.all_valid_path_val = [self.get_v_path(train_root,idx) for idx in all_valid_indices_val]\n",
        "        self.all_valid_path_test = [self.get_v_path(test_root,idx) for idx in all_valid_indices_test]\n",
        "        self.norm_type=norm_type\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.mode=='train':\n",
        "            return len(self.all_valid_path_train)\n",
        "        if self.mode=='val':\n",
        "            return len(self.all_valid_path_val)\n",
        "        if self.mode=='test':\n",
        "            return len(self.all_valid_path_test)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        wind = torch.tensor([0, 1, 0], dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            file = self.all_valid_path_train[index]\n",
        "            data_dict=read(file)\n",
        "        elif self.mode == 'val':\n",
        "            file = self.all_valid_path_val[index]\n",
        "            data_dict = read(file)\n",
        "        else:\n",
        "            file = self.all_valid_path_test[index]\n",
        "            data_dict = read(file,train=False)\n",
        "        data_dict['wind'] = wind\n",
        "        return data_dict\n",
        "\n",
        "\n",
        "    def get_v_path(self, data_dir: Path, mesh_ind: int) -> Path:\n",
        "        return data_dir / (\"vel_\" + str(mesh_ind).zfill(3) + \".vtk\")\n",
        "\n",
        "    def load_valid_indices(\n",
        "            self, log_file\n",
        "    ):\n",
        "        with open(log_file, \"r\") as fp:\n",
        "            mesh_ind = fp.read().split(\"\\n\")\n",
        "            mesh_ind = [int(a) for a in mesh_ind if a.strip()]\n",
        "        return mesh_ind\n",
        "    def load_valid_test_indices(\n",
        "            self, data_dir, filename=\"watertight_meshes.txt\"\n",
        "    ):\n",
        "        with open(data_dir / filename, \"r\") as fp:\n",
        "            mesh_ind = fp.read().split(\"\\n\")\n",
        "            mesh_ind = [int(a) for a in mesh_ind if a.strip()]\n",
        "        return mesh_ind\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MkosCQyBu7aW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "Ktwbrdh7m_gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "wget导入权重"
      ],
      "metadata": {
        "id": "9mssn11Uyhku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O ./best_val_cd.pth \"https://drive.usercontent.google.com/download?id=1rWkd-OVsA7QZeLz5HwHKB5p2UgX7ljaW&export=download&authuser=0&confirm=t&uuid=b16c8a6e-3544-42ac-98c9-15c71d131390&at=APZUnTVreAXRp899WWFU-l6q_DnL%3A1720923969619\"\n",
        "!wget -O ./best_val_velocity.pth \"https://drive.usercontent.google.com/download?id=1YLA1Zoc9WDma2rsUwGjtPN2ZtIg9Pks8&export=download&authuser=0&confirm=t&uuid=b16c8a6e-3544-42ac-98c9-15c71d131390&at=APZUnTVreAXRp899WWFU-l6q_DnL%3A1720923969619\"\n",
        "!wget -O ./best_val_press.pth \"https://drive.usercontent.google.com/download?id=131ww0So9D9dQKF0kZ2QXOjUz7BC0sPjl&export=download&authuser=0&confirm=t&uuid=b16c8a6e-3544-42ac-98c9-15c71d131390&at=APZUnTVreAXRp899WWFU-l6q_DnL%3A1720923969619\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdIpBndZrRLG",
        "outputId": "e1a1ef23-0a5c-43e5-e06c-ada0c0d8e87c",
        "collapsed": true
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-15 15:14:43--  https://drive.usercontent.google.com/download?id=1rWkd-OVsA7QZeLz5HwHKB5p2UgX7ljaW&export=download&authuser=0&confirm=t&uuid=b16c8a6e-3544-42ac-98c9-15c71d131390&at=APZUnTVreAXRp899WWFU-l6q_DnL%3A1720923969619\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.195.132, 2607:f8b0:400e:c03::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 121269984 (116M) [application/octet-stream]\n",
            "Saving to: ‘./best_val_cd.pth’\n",
            "\n",
            "./best_val_cd.pth   100%[===================>] 115.65M   210MB/s    in 0.6s    \n",
            "\n",
            "2024-09-15 15:14:45 (210 MB/s) - ‘./best_val_cd.pth’ saved [121269984/121269984]\n",
            "\n",
            "--2024-09-15 15:14:45--  https://drive.usercontent.google.com/download?id=1YLA1Zoc9WDma2rsUwGjtPN2ZtIg9Pks8&export=download&authuser=0&confirm=t&uuid=b16c8a6e-3544-42ac-98c9-15c71d131390&at=APZUnTVreAXRp899WWFU-l6q_DnL%3A1720923969619\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.195.132, 2607:f8b0:400e:c03::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 121618916 (116M) [application/octet-stream]\n",
            "Saving to: ‘./best_val_velocity.pth’\n",
            "\n",
            "./best_val_velocity 100%[===================>] 115.98M   221MB/s    in 0.5s    \n",
            "\n",
            "2024-09-15 15:14:48 (221 MB/s) - ‘./best_val_velocity.pth’ saved [121618916/121618916]\n",
            "\n",
            "--2024-09-15 15:14:48--  https://drive.usercontent.google.com/download?id=131ww0So9D9dQKF0kZ2QXOjUz7BC0sPjl&export=download&authuser=0&confirm=t&uuid=b16c8a6e-3544-42ac-98c9-15c71d131390&at=APZUnTVreAXRp899WWFU-l6q_DnL%3A1720923969619\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.195.132, 2607:f8b0:400e:c03::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 992533691 (947M) [application/octet-stream]\n",
            "Saving to: ‘./best_val_press.pth’\n",
            "\n",
            "./best_val_press.pt 100%[===================>] 946.55M   196MB/s    in 4.7s    \n",
            "\n",
            "2024-09-15 15:14:55 (201 MB/s) - ‘./best_val_press.pth’ saved [992533691/992533691]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test_press"
      ],
      "metadata": {
        "id": "pqlhYgoj2B49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding:utf-8 _*-\n",
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append('../..')\n",
        "sys.path.append('..')\n",
        "\n",
        "import re\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "'''\n",
        "    A general code framework for training neural operator on irregular domains\n",
        "'''\n",
        "\n",
        "EPOCH_SCHEDULERS = ['ReduceLROnPlateau', 'StepLR', 'MultiplicativeLR',\n",
        "                    'MultiStepLR', 'ExponentialLR', 'LambdaLR']\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "def extract_numbers_from_filename(filename):\n",
        "    # 提取文件名中的数字\n",
        "    numbers = re.findall(r'\\d+', filename)\n",
        "    return numbers\n",
        "\n",
        "def write_numbers_to_file(directory, output_file):\n",
        "    # 打开输出文件\n",
        "    with open(output_file, 'w') as file:\n",
        "        # 遍历目录中的所有文件\n",
        "        for filename in os.listdir(directory):\n",
        "            # 提取文件名中的数字\n",
        "            numbers = extract_numbers_from_filename(filename)\n",
        "            if numbers:\n",
        "                # 写入文件\n",
        "                file.write(' '.join(numbers) + '\\n')\n",
        "\n",
        "# # 指定文件夹路径和输出文件名\n",
        "# directory_path = '/mnt/nunu/race/IJCAI_2024/Dataset/Testset_track_A/Inference'  # 替换为你的文件夹路径\n",
        "# output_file_name = '/mnt/nunu/race/IJCAI_2024/Dataset/Testset_track_A/watertight_meshes.txt'\n",
        "#\n",
        "# # 调用函数\n",
        "# write_numbers_to_file(directory_path, output_file_name)\n",
        "def extract_numbers(s):\n",
        "    return [int(digit) for digit in re.findall(r'\\d+', s)]\n",
        "\n",
        "\n",
        "def write_to_vtk(p, point_data_pos=\"press on mesh points\", mesh_path=None,save_name='output_pressure'):\n",
        "    import meshio\n",
        "    # p = out_dict[\"pressure\"]\n",
        "    index = extract_numbers(mesh_path.name)[0]\n",
        "    index = str(index).zfill(3)\n",
        "\n",
        "    if point_data_pos == \"press on mesh points\":\n",
        "        mesh = meshio.read(mesh_path)\n",
        "        mesh.point_data[\"p\"] = p.cpu().numpy()\n",
        "        # if \"pred wss_x\" in out_dict:\n",
        "        #     wss_x = out_dict[\"pred wss_x\"]\n",
        "        #     mesh.point_data[\"wss_x\"] = wss_x.numpy()\n",
        "    elif point_data_pos == \"press on mesh cells\":\n",
        "        points = np.load(mesh_path.parent / f\"centroid_{index}.npy\")\n",
        "        npoint = points.shape[0]\n",
        "        mesh = meshio.Mesh(\n",
        "            points=points, cells=[(\"vertex\", np.arange(npoint).reshape(npoint, 1))]\n",
        "        )\n",
        "        mesh.point_data = {\"p\": p.numpy()}\n",
        "    pressure = p.cpu().numpy().flatten()\n",
        "    # print(f\"write : {config.run_name}/output/{mesh_path.parent.name}_{index}.vtk\")\n",
        "    # os.makedirs(f\"./result/{args.track}/output\", exist_ok=True)\n",
        "    os.makedirs(f\"./result\", exist_ok=True)\n",
        "    np.save(f\"./result/press_{index}.npy\", pressure)\n",
        "    print(f\"save to result/press_{index}.npy\")\n",
        "    # mesh.write(f\"./result/{args.track}/output/{mesh_path.parent.name}_{index}.vtk\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def validate_epoch(model, metric_func, valid_loader, device, test, best_state_dict=None,save_name='output_pressure'):\n",
        "    if test:\n",
        "        model.load_state_dict(best_state_dict)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    metric_val = []\n",
        "    for i, data in enumerate(valid_loader):\n",
        "        with torch.no_grad():\n",
        "            query_points, y = data['vertices'], data['pressure']\n",
        "            query_points, y, = query_points.to(device), y.to(device)\n",
        "            conditions = MultipleTensors(extract_condition(data))\n",
        "            conditions = conditions.to(device)\n",
        "\n",
        "            out = model(query_points, conditions)\n",
        "\n",
        "            y_pred = out.squeeze(-1)\n",
        "            # metric = metric_func(y_pred, y)\n",
        "\n",
        "            # metric_val.append(metric.cpu().numpy())\n",
        "            if test:\n",
        "                write_to_vtk(y_pred.unsqueeze(-1).squeeze(0), mesh_path=CARdataset.all_test_mesh_path[i],save_name=save_name)\n",
        "    return dict(metric=np.mean(metric_val, axis=0))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = get_args()\n",
        "\n",
        "    #生成test watertight.txt\n",
        "    if not os.path.exists(f'{args.test_dir}/watertight_meshes.txt'):\n",
        "        directory_path = f'{args.test_dir}/Inference'\n",
        "        output_file_name = f'{args.test_dir}/watertight_meshes.txt'\n",
        "        write_numbers_to_file(directory_path, output_file_name)\n",
        "\n",
        "    #生成fake pressure\n",
        "    if not os.path.exists(f'{args.test_dir}/Label'):\n",
        "        os.makedirs(f'{args.test_dir}/Label',exist_ok=True)\n",
        "        for file in os.listdir(f'{args.test_dir}/Inference'):\n",
        "            id=file.split('.')[0].split('_')[1]\n",
        "            fake_label=np.ones(3682)\n",
        "            np.save(f'{args.test_dir}/Label/press_{id}.npy',fake_label)\n",
        "\n",
        "    if not args.no_cuda and torch.cuda.is_available():\n",
        "        device = torch.device('cuda:{}'.format(str(args.gpu)))\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "    kwargs = {'pin_memory': False} if args.gpu else {}\n",
        "    get_seed(args.seed, printout=False)\n",
        "\n",
        "    CARdataset = get_dataset(args)\n",
        "    test_loader = CARdataset.test_dataloader(\n",
        "        batch_size=1, shuffle=False\n",
        "    )\n",
        "\n",
        "    #### set random seeds\n",
        "    get_seed(args.seed)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    loss_func = LpLoss()\n",
        "    metric_func = LpLoss()\n",
        "\n",
        "    model = get_model(args)\n",
        "    model = model.to(device)\n",
        "    print(f\"\\nModel: {model.__name__}\\t Number of params: {get_num_params(model)}\")\n",
        "\n",
        "    best_state_dict = torch.load('best_val_press.pth',map_location='cpu')\n",
        "    val_metric_best = validate_epoch(model, metric_func, test_loader, device, test=True, best_state_dict=best_state_dict)\n",
        "\n",
        "    print(f\"\\nBest model's validation metric in this run: {val_metric_best}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMfabw83kmqT",
        "outputId": "44bda068-d838-44f2-aed8-09062dff09bb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "use 475 training data!\n",
            "\n",
            "\n",
            "The following code snippets have been run.\n",
            "==================================================\n",
            "\n",
            "    os.environ['PYTHONHASHSEED'] = str(2024)\n",
            "    numpy.random.seed(2024)\n",
            "    torch.manual_seed(2024)\n",
            "    torch.cuda.manual_seed(2024)\n",
            "    torch.backends.cudnn.deterministic = True\n",
            "    torch.backends.cudnn.benchmark = False\n",
            "    if torch.cuda.is_available():\n",
            "        torch.cuda.manual_seed_all(2024)\n",
            "    \n",
            "==================================================\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "\n",
            "Model: MIOEGPT\t Number of params: 248052257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-fe4fe2d835c5>:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_state_dict = torch.load('best_val_press.pth',map_location='cpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save to result/press_658.npy\n",
            "save to result/press_659.npy\n",
            "save to result/press_660.npy\n",
            "save to result/press_662.npy\n",
            "save to result/press_663.npy\n",
            "save to result/press_664.npy\n",
            "save to result/press_665.npy\n",
            "save to result/press_666.npy\n",
            "save to result/press_667.npy\n",
            "save to result/press_668.npy\n",
            "save to result/press_672.npy\n",
            "save to result/press_673.npy\n",
            "save to result/press_674.npy\n",
            "save to result/press_675.npy\n",
            "save to result/press_676.npy\n",
            "save to result/press_677.npy\n",
            "save to result/press_678.npy\n",
            "save to result/press_679.npy\n",
            "save to result/press_681.npy\n",
            "save to result/press_683.npy\n",
            "save to result/press_684.npy\n",
            "save to result/press_686.npy\n",
            "save to result/press_687.npy\n",
            "save to result/press_688.npy\n",
            "save to result/press_689.npy\n",
            "save to result/press_690.npy\n",
            "save to result/press_691.npy\n",
            "save to result/press_692.npy\n",
            "save to result/press_693.npy\n",
            "save to result/press_695.npy\n",
            "save to result/press_696.npy\n",
            "save to result/press_697.npy\n",
            "save to result/press_700.npy\n",
            "save to result/press_701.npy\n",
            "save to result/press_702.npy\n",
            "save to result/press_703.npy\n",
            "save to result/press_704.npy\n",
            "save to result/press_705.npy\n",
            "save to result/press_708.npy\n",
            "save to result/press_709.npy\n",
            "save to result/press_710.npy\n",
            "save to result/press_711.npy\n",
            "save to result/press_712.npy\n",
            "save to result/press_713.npy\n",
            "save to result/press_715.npy\n",
            "save to result/press_717.npy\n",
            "save to result/press_718.npy\n",
            "save to result/press_719.npy\n",
            "save to result/press_721.npy\n",
            "save to result/press_722.npy\n",
            "\n",
            "Best model's validation metric in this run: {'metric': nan}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test_velocity"
      ],
      "metadata": {
        "id": "eV4qa7pNzTGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding:utf-8 _*-\n",
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append('../..')\n",
        "sys.path.append('..')\n",
        "\n",
        "import re\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "from torch.optim.lr_scheduler import OneCycleLR, StepLR, LambdaLR\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.nn import MSELoss ,HuberLoss,SmoothL1Loss,L1Loss\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "'''\n",
        "    A general code framework for training neural operator on irregular domains\n",
        "'''\n",
        "\n",
        "EPOCH_SCHEDULERS = ['ReduceLROnPlateau', 'StepLR', 'MultiplicativeLR',\n",
        "                    'MultiStepLR', 'ExponentialLR', 'LambdaLR']\n",
        "\n",
        "\n",
        "def extract_numbers(s):\n",
        "    return [int(digit) for digit in re.findall(r'\\d+', s)]\n",
        "\n",
        "\n",
        "def train(model, loss_func, metric_func,\n",
        "          train_loader, valid_loader,\n",
        "          optimizer, lr_scheduler,\n",
        "          epochs=10,\n",
        "          writer=None,\n",
        "          device=\"cuda\",\n",
        "          patience=10,\n",
        "          grad_clip=0.999,\n",
        "          start_epoch: int = 0,\n",
        "          model_save_path='./data/checkpoints/',\n",
        "          save_mode='state_dict',  # 'state_dict' or 'entire'\n",
        "          ):\n",
        "    loss_train = []\n",
        "    loss_val = []\n",
        "    loss_epoch = []\n",
        "    lr_history = []\n",
        "    it = 0\n",
        "\n",
        "    if patience is None or patience == 0:\n",
        "        patience = epochs\n",
        "    result = None\n",
        "    start_epoch = start_epoch\n",
        "    end_epoch = start_epoch + epochs\n",
        "    best_val_metric = np.inf\n",
        "    best_val_epoch = None\n",
        "    save_mode = 'state_dict' if save_mode is None else save_mode\n",
        "    stop_counter = 0\n",
        "    is_epoch_scheduler = any(s in str(lr_scheduler.__class__) for s in EPOCH_SCHEDULERS)\n",
        "\n",
        "    for epoch in range(start_epoch, end_epoch):\n",
        "        model.train()\n",
        "        torch.cuda.empty_cache()\n",
        "        for batch in tqdm(train_loader):\n",
        "            loss_epoch = []\n",
        "            loss = train_batch(model, loss_func, batch, optimizer, lr_scheduler, device, grad_clip=grad_clip)\n",
        "            loss = np.array(loss)\n",
        "            loss_epoch.append(loss)\n",
        "            it += 1\n",
        "            lr = optimizer.param_groups[0]['lr']\n",
        "            writer.add_scalar('LR', lr, it)\n",
        "            lr_history.append(lr)\n",
        "            # log = f\"epoch: [{epoch + 1}/{end_epoch}]\"\n",
        "            # if loss.ndim == 0:  # 1 target loss\n",
        "            #     _loss_mean = np.mean(loss_epoch)\n",
        "            #     log += \" loss: {:.6f}\".format(_loss_mean)\n",
        "            # else:\n",
        "        _loss_mean = np.mean(loss_epoch)\n",
        "        #     for j in range(len(_loss_mean)):\n",
        "        #         log += \" | loss {}: {:.6f}\".format(j, _loss_mean[j])\n",
        "        # log += \" | current lr: {:.3e}\".format(lr)\n",
        "        #\n",
        "        # if it % print_freq == 0:\n",
        "        #     print(log)\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar(\"train_loss\", _loss_mean, epoch)  #### loss 0 seems to be the sum of all loss\n",
        "\n",
        "        loss_train.append(_loss_mean)\n",
        "\n",
        "        val_result = validate_epoch(model, metric_func, valid_loader, device)\n",
        "\n",
        "        loss_val.append(val_result[\"metric\"])\n",
        "        val_metric = val_result[\"metric\"]\n",
        "        print(f'epoch:[{epoch + 1}/{end_epoch}] train_loss:{_loss_mean:.4f} val_loss:{val_metric:.4f} lr:{lr_history[-1]:.6f}')\n",
        "\n",
        "        if val_metric < best_val_metric:\n",
        "            best_val_epoch = epoch\n",
        "            best_val_metric = val_metric\n",
        "            torch.save(model.state_dict(), os.path.join(model_save_path, 'best_val.pth'))\n",
        "\n",
        "        if lr_scheduler and is_epoch_scheduler:\n",
        "            if 'ReduceLROnPlateau' in str(lr_scheduler.__class__):\n",
        "                lr_scheduler.step(val_metric)\n",
        "            else:\n",
        "                lr_scheduler.step()\n",
        "\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar('val loss', val_metric, epoch)\n",
        "\n",
        "        torch.save(model.state_dict(), os.path.join(f'runs/{args.train_log}/last.pth'))\n",
        "        result = dict(\n",
        "            # best_val_epoch=best_val_epoch,\n",
        "            # best_val_metric=best_val_metric,\n",
        "            # loss_train=np.asarray(loss_train),\n",
        "            # loss_val=np.asarray(loss_val),\n",
        "            # lr_history=np.asarray(lr_history),\n",
        "            # best_model=best_model_state_dict,\n",
        "            # optimizer_state=optimizer.state_dict()\n",
        "        )\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def train_batch(model, loss_func, data, optimizer, lr_scheduler, device, grad_clip=0.999):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    query_points, y = data['vertices'], data['velocity']\n",
        "    query_points, y, = query_points.to(device), y.to(device)\n",
        "    conditions = MultipleTensors(extract_condition(data))\n",
        "    conditions = conditions.to(device)\n",
        "\n",
        "    out = model(query_points, conditions)\n",
        "\n",
        "    # y_pred = out.squeeze(-1)\n",
        "    loss = loss_func(out, y)\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if lr_scheduler:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validate_epoch(model, metric_func, valid_loader, device):\n",
        "\n",
        "    model.eval()\n",
        "    metric_val = []\n",
        "    for i, data in enumerate(valid_loader):\n",
        "        with torch.no_grad():\n",
        "            query_points, y = data['vertices'], data['velocity']\n",
        "            query_points, y, = query_points.to(device), y.to(device)\n",
        "            conditions = MultipleTensors(extract_condition(data))\n",
        "            conditions = conditions.to(device)\n",
        "\n",
        "            out = model(query_points, conditions)\n",
        "\n",
        "            # y_pred = out.squeeze(-1)\n",
        "            metric = metric_func(out, y)\n",
        "            metric_val.append(metric.cpu().numpy())\n",
        "\n",
        "    return dict(metric=np.mean(metric_val, axis=0))\n",
        "\n",
        "def test_epoch(model, test_loader, device,model_weight):\n",
        "    model.load_state_dict(model_weight,strict=False)\n",
        "    model.eval()\n",
        "\n",
        "    if not os.path.exists(f\"./runs/{args.train_log}/output\"):\n",
        "        os.makedirs(f\"./runs/{args.train_log}/output\", exist_ok=True)\n",
        "    for i, data in enumerate(test_loader):\n",
        "        with torch.no_grad():\n",
        "            query_points = data['vertices']\n",
        "            query_points = query_points.to(device)\n",
        "            conditions = MultipleTensors(extract_condition(data))\n",
        "            conditions = conditions.to(device)\n",
        "\n",
        "            out = model(query_points, conditions)\n",
        "            test_indice = test_loader.dataset.all_valid_indices_test[i]\n",
        "            os.makedirs(f\"./result\", exist_ok=True)\n",
        "            npy_leaderboard = f\"./result/vel_{str(test_indice).zfill(3)}.npy\"\n",
        "            print(f\"saving *.npy file for [Velocity] leaderboard : \", npy_leaderboard)\n",
        "            np.save(npy_leaderboard, out.squeeze(0).cpu().numpy())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = get_args_velocity()\n",
        "    print(args)\n",
        "    if args.use_tb:\n",
        "        writer = SummaryWriter(f'runs/{args.train_log}')\n",
        "\n",
        "    else:\n",
        "        writer = None\n",
        "        log_path = None\n",
        "    if not args.no_cuda and torch.cuda.is_available():\n",
        "        device = torch.device('cuda:{}'.format(str(args.gpu)))\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    # 获取命令行参数\n",
        "    arg = sys.argv[1:]\n",
        "\n",
        "    # 将参数写入文件\n",
        "    with open(f'runs/{args.train_log}/args.txt', 'w') as f:\n",
        "        for a in arg:\n",
        "            f.write(a + '\\n')\n",
        "\n",
        "    kwargs = {'pin_memory': False} if args.gpu else {}\n",
        "    get_seed(args.seed, printout=False)\n",
        "\n",
        "    #划分训练和验证集\n",
        "    log_file = \"./Training/Dataset_1/Feature_File/watertight_meshes.txt\"  # 替换为你的日志文件路径\n",
        "    output_file_train = f\"runs/{args.train_log}/train_indices.txt\"  # 输出文件路径\n",
        "    output_file_valid = f\"runs/{args.train_log}/val_indices.txt\"  # 输出文件路径\n",
        "    processor = DataProcessor()\n",
        "    indices = processor.load_valid_indices(log_file)\n",
        "    train_indices, val_indices = processor.split_data(indices, train_ratio=(1-args.val_ratio))\n",
        "    processor.save_split_indices(train_indices, val_indices, output_file_train, output_file_valid)\n",
        "\n",
        "    train_root=Path('./Training/Dataset_1/Feature_File')\n",
        "    test_root=Path('./Test/Dataset_1/Feature_File')\n",
        "\n",
        "    V_train=Velocity_Dataset(train_root,test_root,output_file_train,output_file_valid,mode='train',norm_type=args.norm_type)\n",
        "    V_val=Velocity_Dataset(train_root,test_root,output_file_train,output_file_valid,mode='val',norm_type=args.norm_type)\n",
        "    V_test=Velocity_Dataset(train_root,test_root,output_file_train,output_file_valid,mode='test',norm_type=args.norm_type)\n",
        "\n",
        "    train_loader=DataLoader(V_train,batch_size=args.batch_size,shuffle=True)\n",
        "    val_loader=DataLoader(V_val,batch_size=1,shuffle=False)\n",
        "    test_loader=DataLoader(V_test,batch_size=1,shuffle=False)\n",
        "    # args.space_dim = int(re.search(r'\\d', args.dataset).group())\n",
        "    # args.normalizer =  train_dataset.y_normalizer.to(device) if train_dataset.y_normalizer is not None else None\n",
        "\n",
        "    #### set random seeds\n",
        "    get_seed(args.seed)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    loss_func = get_loss(args)\n",
        "    metric_func = LpLoss()\n",
        "\n",
        "    model = get_model(args)\n",
        "    model = model.to(device)\n",
        "    print(f\"\\nModel: {model.__name__}\\t Number of params: {get_num_params(model)}\")\n",
        "\n",
        "    # path_prefix = args.dataset + '_{}_'.format(args.component) + model.__name__ + args.comment + time.strftime(\n",
        "    #     '_%m%d_%H_%M_%S')\n",
        "    # model_path, result_path = path_prefix + '.pt', path_prefix + '.pkl'\n",
        "\n",
        "    # print(f\"Saving model and result in ./../models/checkpoints/{model_path}\\n\")\n",
        "\n",
        "    # print(model)\n",
        "    # print(config)\n",
        "\n",
        "    epochs = args.epochs\n",
        "    lr = args.lr\n",
        "\n",
        "    if args.optimizer == 'Adam':\n",
        "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=args.weight_decay, betas=(0.9, 0.999))\n",
        "    elif args.optimizer == \"AdamW\":\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=args.weight_decay, betas=(0.9, 0.999))\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if args.lr_method == 'cycle':\n",
        "        print('Using cycle learning rate schedule')\n",
        "        scheduler = OneCycleLR(optimizer, max_lr=lr, div_factor=1e4, pct_start=0.2, final_div_factor=1e4,\n",
        "                               steps_per_epoch=len(train_loader), epochs=epochs)\n",
        "    elif args.lr_method == 'step':\n",
        "        print('Using step learning rate schedule')\n",
        "        scheduler = StepLR(optimizer, step_size=args.lr_step_size * len(train_loader), gamma=0.7)\n",
        "    elif args.lr_method == 'warmup':\n",
        "        print('Using warmup learning rate schedule')\n",
        "        scheduler = LambdaLR(optimizer, lambda steps: min((steps + 1) / (args.warmup_epochs * len(train_loader)),\n",
        "                                                          np.power(\n",
        "                                                              args.warmup_epochs * len(train_loader) / float(steps + 1),\n",
        "                                                              0.5)))\n",
        "    else:\n",
        "        scheduler = None\n",
        "    time_start = time.time()\n",
        "\n",
        "    # result = train(model, loss_func, metric_func,\n",
        "    #                train_loader, val_loader,\n",
        "    #                optimizer, scheduler,\n",
        "    #                epochs=epochs,\n",
        "    #                grad_clip=args.grad_clip,\n",
        "    #                patience=10,\n",
        "    #                model_save_path=f'runs/{args.train_log}',\n",
        "    #                writer=writer,\n",
        "    #                device=device)\n",
        "\n",
        "    print('Training takes {} seconds.'.format(time.time() - time_start))\n",
        "\n",
        "    # result['args'], result['config'] = args, config\n",
        "    # checkpoint = {'args': args, 'model': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "    # torch.save(model.state_dict(), os.path.join(f'runs/{args.train_log}/last.pth'))\n",
        "    # model.eval()\n",
        "    best_state_dict = torch.load('./best_val_velocity.pth',map_location='cuda:0')\n",
        "    # last_state_dict = torch.load(os.path.join(f'runs/{args.train_log}', 'last.pth'))\n",
        "\n",
        "    test_epoch(model,test_loader,device,best_state_dict)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pps_GnktyxJl",
        "outputId": "2c390b28-1a0b-41c5-8dc2-c68a5912930d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='ns2d', train_dir='/mnt/nunu/race/IJCAI_2024/Dataset/Training_data', test_dir='/mnt/nunu/race/IJCAI_2024/Dataset/Testset_track_A', norm_type=None, wind_direction='z', track='track_A', train_log='train_v', expand_data=False, val_ratio=0.1, p=0.5, iter_per_epoch=150, val_epoch=10, cat_area=False, use_transform=False, component='all', seed=2024, gpu=0, use_tb=1, comment='', train_num='all', test_num='all', sort_data=0, normalize_x='none', use_normalizer='unit', epochs=300, optimizer='AdamW', lr=0.001, weight_decay=5e-06, grad_clip=1000.0, batch_size=1, val_batch_size=8, no_cuda=False, lr_method='cycle', lr_step_size=50, warmup_epochs=50, loss_name='rel2', model_name='GNOT', n_hidden=256, n_layers=16, act='gelu', loss='default', model='GNOT', n_head=8, slice_num=64, agent_num=64, ffn_dropout=0.0, attn_dropout=0.0, mlp_layers=3, attn_type='linear', use_fourier_feat=False, n_experts=2, input_dim=3, output_dim=3, space_dim=3, branch_sizes=[3], n_inner=2)\n",
            "\n",
            "\n",
            "The following code snippets have been run.\n",
            "==================================================\n",
            "\n",
            "    os.environ['PYTHONHASHSEED'] = str(2024)\n",
            "    numpy.random.seed(2024)\n",
            "    torch.manual_seed(2024)\n",
            "    torch.cuda.manual_seed(2024)\n",
            "    torch.backends.cudnn.deterministic = True\n",
            "    torch.backends.cudnn.benchmark = False\n",
            "    if torch.cuda.is_available():\n",
            "        torch.cuda.manual_seed_all(2024)\n",
            "    \n",
            "==================================================\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "\n",
            "Model: MIOEGPT\t Number of params: 30332707\n",
            "Using cycle learning rate schedule\n",
            "Training takes 1.1920928955078125e-06 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-0bec29aa173a>:298: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_state_dict = torch.load('./best_val_velocity.pth',map_location='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_658.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_659.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_660.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_662.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_663.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_664.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_665.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_666.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_667.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_668.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_672.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_673.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_674.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_675.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_676.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_677.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_678.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_679.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_681.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_683.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_684.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_686.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_687.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_688.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_689.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_690.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_691.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_692.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_693.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_695.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_696.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_697.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_700.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_701.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_702.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_703.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_704.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_705.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_708.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_709.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_710.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_711.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_712.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_713.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_715.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_717.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_718.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_719.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_721.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./result/vel_722.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test_cd"
      ],
      "metadata": {
        "id": "DJ2xzIBj2J7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding:utf-8 _*-\n",
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append('../..')\n",
        "sys.path.append('..')\n",
        "import csv\n",
        "import re\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import OneCycleLR, StepLR, LambdaLR\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "'''\n",
        "    A general code framework for training neural operator on irregular domains\n",
        "'''\n",
        "\n",
        "EPOCH_SCHEDULERS = ['ReduceLROnPlateau', 'StepLR', 'MultiplicativeLR',\n",
        "                    'MultiStepLR', 'ExponentialLR', 'LambdaLR']\n",
        "\n",
        "\n",
        "def train(model, loss_func,\n",
        "          train_loader,\n",
        "          optimizer, lr_scheduler,\n",
        "          epochs=10,\n",
        "          writer=None,\n",
        "          device=\"cuda\",\n",
        "          patience=10,\n",
        "          grad_clip=0.999,\n",
        "          start_epoch: int = 0,\n",
        "          save_mode='state_dict',  # 'state_dict' or 'entire'\n",
        "          ):\n",
        "    loss_train = []\n",
        "    loss_val = []\n",
        "    loss_epoch = []\n",
        "    lr_history = []\n",
        "    it = 0\n",
        "\n",
        "    if patience is None or patience == 0:\n",
        "        patience = epochs\n",
        "    result = None\n",
        "    start_epoch = start_epoch\n",
        "    end_epoch = start_epoch + epochs\n",
        "\n",
        "    for epoch in range(start_epoch, end_epoch):\n",
        "        model.train()\n",
        "        # torch.cuda.empty_cache()\n",
        "        for batch in tqdm(train_loader):\n",
        "            loss_epoch = []\n",
        "            loss = train_batch(model, loss_func, batch, optimizer, lr_scheduler, device, grad_clip=grad_clip)\n",
        "            loss = np.array(loss)\n",
        "            loss_epoch.append(loss)\n",
        "            it += 1\n",
        "            lr = optimizer.param_groups[0]['lr']\n",
        "            writer.add_scalar('LR', lr, it)\n",
        "            lr_history.append(lr)\n",
        "        _loss_mean = np.mean(loss_epoch)\n",
        "\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar(\"train_loss\", _loss_mean, epoch)  #### loss 0 seems to be the sum of all loss\n",
        "        loss_train.append(_loss_mean)\n",
        "        print(f'epoch:[{epoch + 1}/{end_epoch}] train_loss:{_loss_mean:.4f}')\n",
        "        torch.save(model.state_dict(), os.path.join(f'runs/{args.train_log}/last.pth'))\n",
        "        result = dict(\n",
        "        )\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def train_batch(model, loss_func, data, optimizer, lr_scheduler, device, grad_clip=0.999):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    query_points, y = data['vertices'], data['cd']\n",
        "    query_points, y, = query_points.to(device), y.to(device)\n",
        "    conditions = MultipleTensors(extract_condition(data))\n",
        "    conditions = conditions.to(device)\n",
        "\n",
        "    out = model(query_points, conditions)\n",
        "\n",
        "    y_pred = out.squeeze(-1)\n",
        "    # y_mean=y_pred.mean(-1)\n",
        "    # y_=y_mean.unsqueeze(-1)\n",
        "    # print(y.size())\n",
        "    # print(y_pred.mean(-1).size())\n",
        "    loss = loss_func(y_pred.mean(-1), y)\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if lr_scheduler:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = get_args_cd()\n",
        "    if args.use_tb:\n",
        "        writer = SummaryWriter(f'runs/{args.train_log}')\n",
        "\n",
        "    else:\n",
        "        writer = None\n",
        "        log_path = None\n",
        "    if not args.no_cuda and torch.cuda.is_available():\n",
        "        device = torch.device('cuda:{}'.format(str(args.gpu)))\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    # 获取命令行参数\n",
        "    arg = sys.argv[1:]\n",
        "\n",
        "    # 将参数写入文件\n",
        "    with open(f'runs/{args.train_log}/args.txt', 'w') as f:\n",
        "        for a in arg:\n",
        "            f.write(a + '\\n')\n",
        "\n",
        "    kwargs = {'pin_memory': False} if args.gpu else {}\n",
        "    get_seed(args.seed, printout=False)\n",
        "    data_root='./Training/Dataset_2/Feature_File'\n",
        "    data_root_test='./Test/Dataset_2/Feature_File'\n",
        "    file_path='./Training/Dataset_2/Label_File/dataset2_train_label.csv'\n",
        "    cd_dataset = CdDataset(data_root,file_path)\n",
        "    cd_dataset_test = CdTestDataset(data_root_test)\n",
        "    train_loader = DataLoader(cd_dataset,batch_size=args.batch_size,shuffle=True)\n",
        "    test_loader = DataLoader(cd_dataset_test,batch_size=1,shuffle=False)\n",
        "    # args.space_dim = int(re.search(r'\\d', args.dataset).group())\n",
        "    # args.normalizer =  train_dataset.y_normalizer.to(device) if train_dataset.y_normalizer is not None else None\n",
        "\n",
        "    #### set random seeds\n",
        "    get_seed(args.seed)\n",
        "    # torch.cuda.empty_cache()\n",
        "\n",
        "    loss_func = get_loss(args)\n",
        "    # metric_func = LpLoss()\n",
        "\n",
        "    model = get_model(args)\n",
        "    model = model.to(device)\n",
        "    print(f\"\\nModel: {model.__name__}\\t Number of params: {get_num_params(model)}\")\n",
        "\n",
        "    epochs = args.epochs\n",
        "    lr = args.lr\n",
        "\n",
        "    if args.optimizer == 'Adam':\n",
        "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=args.weight_decay, betas=(0.9, 0.999))\n",
        "    elif args.optimizer == \"AdamW\":\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=args.weight_decay, betas=(0.9, 0.999))\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if args.lr_method == 'cycle':\n",
        "        print('Using cycle learning rate schedule')\n",
        "        scheduler = OneCycleLR(optimizer, max_lr=lr, div_factor=1e4, pct_start=0.2, final_div_factor=1e4,\n",
        "                               steps_per_epoch=len(train_loader), epochs=epochs)\n",
        "    elif args.lr_method == 'step':\n",
        "        print('Using step learning rate schedule')\n",
        "        scheduler = StepLR(optimizer, step_size=args.lr_step_size * len(train_loader), gamma=0.7)\n",
        "    elif args.lr_method == 'warmup':\n",
        "        print('Using warmup learning rate schedule')\n",
        "        scheduler = LambdaLR(optimizer, lambda steps: min((steps + 1) / (args.warmup_epochs * len(train_loader)),\n",
        "                                                          np.power(\n",
        "                                                              args.warmup_epochs * len(train_loader) / float(steps + 1),\n",
        "                                                              0.5)))\n",
        "    else:\n",
        "        scheduler=None\n",
        "    time_start = time.time()\n",
        "\n",
        "    # result = train(model, loss_func,\n",
        "    #                train_loader,\n",
        "    #                optimizer,scheduler,\n",
        "    #                epochs=epochs,\n",
        "    #                grad_clip=args.grad_clip,\n",
        "    #                patience=10,\n",
        "    #                writer=writer,\n",
        "    #                device=device)\n",
        "\n",
        "\n",
        "    print('Training takes {} seconds.'.format(time.time() - time_start))\n",
        "\n",
        "    # result['args'], result['config'] = args, config\n",
        "    # checkpoint = {'args': args, 'model': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "    torch.save(model.state_dict(), os.path.join(f'runs/{args.train_log}/last.pth'))\n",
        "    # model.eval()\n",
        "    best_state_dict = torch.load('best_val_cd.pth',map_location='cpu')\n",
        "\n",
        "    #test time:\n",
        "    model.load_state_dict(best_state_dict,strict=False)\n",
        "    model=model.to(device)\n",
        "    model.eval()\n",
        "    answer=[]\n",
        "    for data in test_loader:\n",
        "        with torch.no_grad():\n",
        "            query_points = data['vertices']\n",
        "            query_points = query_points.to(device)\n",
        "            conditions = MultipleTensors(extract_condition(data))\n",
        "            conditions = conditions.to(device)\n",
        "\n",
        "            out = model(query_points, conditions)\n",
        "            y_pred = out.squeeze(-1)\n",
        "            y_ = y_pred.mean(-1)[0].item()\n",
        "            y_=adjust_item(y_)\n",
        "            answer.append(y_)\n",
        "    os.makedirs(f\"./result\", exist_ok=True)\n",
        "    csv_file_name = f'./result/Answer.csv'\n",
        "\n",
        "    # 使用with语句打开文件，确保文件正确关闭\n",
        "    with open(csv_file_name, mode='w', newline='') as file:\n",
        "        # 创建一个csv.writer对象\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # 写入标题行（可选，但如果你想要的话）\n",
        "        writer.writerow(['', 'Cd'])\n",
        "\n",
        "        # 遍历数据和索引，并将它们写入CSV文件\n",
        "        for index, value in enumerate(answer):\n",
        "            # enumerate会为我们提供索引（从0开始）和值\n",
        "            writer.writerow([index, value])\n",
        "    print('over write to answer!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7Zi0l3S2IKE",
        "outputId": "7058a263-343a-42ac-8c15-aa8dac3b129c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The following code snippets have been run.\n",
            "==================================================\n",
            "\n",
            "    os.environ['PYTHONHASHSEED'] = str(2024)\n",
            "    numpy.random.seed(2024)\n",
            "    torch.manual_seed(2024)\n",
            "    torch.cuda.manual_seed(2024)\n",
            "    torch.backends.cudnn.deterministic = True\n",
            "    torch.backends.cudnn.benchmark = False\n",
            "    if torch.cuda.is_available():\n",
            "        torch.cuda.manual_seed_all(2024)\n",
            "    \n",
            "==================================================\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "\n",
            "Model: MIOEGPT\t Number of params: 30287121\n",
            "Training takes 1.1920928955078125e-06 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-7d94441460db>:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_state_dict = torch.load('best_val_cd.pth',map_location='cpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "over write to answer!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf result/\n",
        "!zip -r ./submission.zip ./result/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5hq2LJUH69Ra",
        "outputId": "b0588ba5-1183-473f-e3e5-cf840219921d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: result/Answer.csv (deflated 72%)\n",
            "  adding: result/press_658.npy (deflated 13%)\n",
            "  adding: result/press_659.npy (deflated 13%)\n",
            "  adding: result/press_660.npy (deflated 13%)\n",
            "  adding: result/press_662.npy (deflated 13%)\n",
            "  adding: result/press_663.npy (deflated 12%)\n",
            "  adding: result/press_664.npy (deflated 13%)\n",
            "  adding: result/press_665.npy (deflated 11%)\n",
            "  adding: result/press_666.npy (deflated 13%)\n",
            "  adding: result/press_667.npy (deflated 12%)\n",
            "  adding: result/press_668.npy (deflated 13%)\n",
            "  adding: result/press_672.npy (deflated 13%)\n",
            "  adding: result/press_673.npy (deflated 13%)\n",
            "  adding: result/press_674.npy (deflated 13%)\n",
            "  adding: result/press_675.npy (deflated 12%)\n",
            "  adding: result/press_676.npy (deflated 12%)\n",
            "  adding: result/press_677.npy (deflated 13%)\n",
            "  adding: result/press_678.npy (deflated 12%)\n",
            "  adding: result/press_679.npy (deflated 13%)\n",
            "  adding: result/press_681.npy (deflated 13%)\n",
            "  adding: result/press_683.npy (deflated 13%)\n",
            "  adding: result/press_684.npy (deflated 13%)\n",
            "  adding: result/press_686.npy (deflated 13%)\n",
            "  adding: result/press_687.npy (deflated 13%)\n",
            "  adding: result/press_688.npy (deflated 13%)\n",
            "  adding: result/press_689.npy (deflated 13%)\n",
            "  adding: result/press_690.npy (deflated 10%)\n",
            "  adding: result/press_691.npy (deflated 13%)\n",
            "  adding: result/press_692.npy (deflated 13%)\n",
            "  adding: result/press_693.npy (deflated 11%)\n",
            "  adding: result/press_695.npy (deflated 12%)\n",
            "  adding: result/press_696.npy (deflated 13%)\n",
            "  adding: result/press_697.npy (deflated 13%)\n",
            "  adding: result/press_700.npy (deflated 13%)\n",
            "  adding: result/press_701.npy (deflated 13%)\n",
            "  adding: result/press_702.npy (deflated 12%)\n",
            "  adding: result/press_703.npy (deflated 11%)\n",
            "  adding: result/press_704.npy (deflated 13%)\n",
            "  adding: result/press_705.npy (deflated 13%)\n",
            "  adding: result/press_708.npy (deflated 13%)\n",
            "  adding: result/press_709.npy (deflated 12%)\n",
            "  adding: result/press_710.npy (deflated 11%)\n",
            "  adding: result/press_711.npy (deflated 12%)\n",
            "  adding: result/press_712.npy (deflated 13%)\n",
            "  adding: result/press_713.npy (deflated 13%)\n",
            "  adding: result/press_715.npy (deflated 13%)\n",
            "  adding: result/press_717.npy (deflated 12%)\n",
            "  adding: result/press_718.npy (deflated 13%)\n",
            "  adding: result/press_719.npy (deflated 13%)\n",
            "  adding: result/press_721.npy (deflated 13%)\n",
            "  adding: result/press_722.npy (deflated 13%)\n",
            "  adding: result/vel_658.npy (deflated 7%)\n",
            "  adding: result/vel_659.npy (deflated 7%)\n",
            "  adding: result/vel_660.npy (deflated 7%)\n",
            "  adding: result/vel_662.npy (deflated 7%)\n",
            "  adding: result/vel_663.npy (deflated 7%)\n",
            "  adding: result/vel_664.npy (deflated 7%)\n",
            "  adding: result/vel_665.npy (deflated 7%)\n",
            "  adding: result/vel_666.npy (deflated 7%)\n",
            "  adding: result/vel_667.npy (deflated 7%)\n",
            "  adding: result/vel_668.npy (deflated 7%)\n",
            "  adding: result/vel_672.npy (deflated 7%)\n",
            "  adding: result/vel_673.npy (deflated 7%)\n",
            "  adding: result/vel_674.npy (deflated 7%)\n",
            "  adding: result/vel_675.npy (deflated 7%)\n",
            "  adding: result/vel_676.npy (deflated 7%)\n",
            "  adding: result/vel_677.npy (deflated 7%)\n",
            "  adding: result/vel_678.npy (deflated 7%)\n",
            "  adding: result/vel_679.npy (deflated 7%)\n",
            "  adding: result/vel_681.npy (deflated 7%)\n",
            "  adding: result/vel_683.npy (deflated 7%)\n",
            "  adding: result/vel_684.npy (deflated 7%)\n",
            "  adding: result/vel_686.npy (deflated 7%)\n",
            "  adding: result/vel_687.npy (deflated 7%)\n",
            "  adding: result/vel_688.npy (deflated 7%)\n",
            "  adding: result/vel_689.npy (deflated 7%)\n",
            "  adding: result/vel_690.npy (deflated 7%)\n",
            "  adding: result/vel_691.npy (deflated 7%)\n",
            "  adding: result/vel_692.npy (deflated 7%)\n",
            "  adding: result/vel_693.npy (deflated 7%)\n",
            "  adding: result/vel_695.npy (deflated 7%)\n",
            "  adding: result/vel_696.npy (deflated 7%)\n",
            "  adding: result/vel_697.npy (deflated 7%)\n",
            "  adding: result/vel_700.npy (deflated 7%)\n",
            "  adding: result/vel_701.npy (deflated 7%)\n",
            "  adding: result/vel_702.npy (deflated 7%)\n",
            "  adding: result/vel_703.npy (deflated 7%)\n",
            "  adding: result/vel_704.npy (deflated 7%)\n",
            "  adding: result/vel_705.npy (deflated 7%)\n",
            "  adding: result/vel_708.npy (deflated 7%)\n",
            "  adding: result/vel_709.npy (deflated 7%)\n",
            "  adding: result/vel_710.npy (deflated 7%)\n",
            "  adding: result/vel_711.npy (deflated 7%)\n",
            "  adding: result/vel_712.npy (deflated 7%)\n",
            "  adding: result/vel_713.npy (deflated 7%)\n",
            "  adding: result/vel_715.npy (deflated 7%)\n",
            "  adding: result/vel_717.npy (deflated 7%)\n",
            "  adding: result/vel_718.npy (deflated 7%)\n",
            "  adding: result/vel_719.npy (deflated 7%)\n",
            "  adding: result/vel_721.npy (deflated 7%)\n",
            "  adding: result/vel_722.npy (deflated 7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train!\n"
      ],
      "metadata": {
        "id": "Ce0Ef-gOobJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "训练结束后可以在 runs/train/output_pressure下得到输出结果"
      ],
      "metadata": {
        "id": "P51Ja8cw0pgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding:utf-8 _*-\n",
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append('../..')\n",
        "sys.path.append('..')\n",
        "\n",
        "import re\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.optim.lr_scheduler import OneCycleLR, StepLR, LambdaLR\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.nn import MSELoss ,HuberLoss,SmoothL1Loss,L1Loss\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "'''\n",
        "    A general code framework for training neural operator on irregular domains\n",
        "'''\n",
        "\n",
        "EPOCH_SCHEDULERS = ['ReduceLROnPlateau', 'StepLR', 'MultiplicativeLR',\n",
        "                    'MultiStepLR', 'ExponentialLR', 'LambdaLR']\n",
        "\n",
        "\n",
        "def extract_numbers(s):\n",
        "    return [int(digit) for digit in re.findall(r'\\d+', s)]\n",
        "\n",
        "\n",
        "def write_to_vtk(p, point_data_pos=\"press on mesh points\", mesh_path=None,save_name='output_pressure'):\n",
        "    import meshio\n",
        "    # p = out_dict[\"pressure\"]\n",
        "    index = extract_numbers(mesh_path.name)[0]\n",
        "    index = str(index).zfill(3)\n",
        "\n",
        "    if point_data_pos == \"press on mesh points\":\n",
        "        mesh = meshio.read(mesh_path)\n",
        "        mesh.point_data[\"p\"] = p.cpu().numpy()\n",
        "        # if \"pred wss_x\" in out_dict:\n",
        "        #     wss_x = out_dict[\"pred wss_x\"]\n",
        "        #     mesh.point_data[\"wss_x\"] = wss_x.numpy()\n",
        "    elif point_data_pos == \"press on mesh cells\":\n",
        "        points = np.load(mesh_path.parent / f\"centroid_{index}.npy\")\n",
        "        npoint = points.shape[0]\n",
        "        mesh = meshio.Mesh(\n",
        "            points=points, cells=[(\"vertex\", np.arange(npoint).reshape(npoint, 1))]\n",
        "        )\n",
        "        mesh.point_data = {\"p\": p.numpy()}\n",
        "    pressure = p.cpu().numpy().flatten()\n",
        "    # print(f\"write : {config.run_name}/output/{mesh_path.parent.name}_{index}.vtk\")\n",
        "    os.makedirs(f\"./runs/{args.train_log}/output\", exist_ok=True)\n",
        "    os.makedirs(f\"./runs/{args.train_log}/{save_name}\", exist_ok=True)\n",
        "    np.save(f\"./runs/{args.train_log}/{save_name}/press_{index}.npy\", pressure)\n",
        "    mesh.write(f\"./runs/{args.train_log}/output/{mesh_path.parent.name}_{index}.vtk\")\n",
        "\n",
        "\n",
        "def train(model, loss_func, metric_func,\n",
        "          train_loader, valid_loader,\n",
        "          optimizer, lr_scheduler,\n",
        "          epochs=10,\n",
        "          writer=None,\n",
        "          device=\"cuda\",\n",
        "          patience=10,\n",
        "          grad_clip=0.999,\n",
        "          start_epoch: int = 0,\n",
        "          model_save_path='./data/checkpoints/',\n",
        "          save_mode='state_dict',  # 'state_dict' or 'entire'\n",
        "          ):\n",
        "    loss_train = []\n",
        "    loss_val = []\n",
        "    loss_epoch = []\n",
        "    lr_history = []\n",
        "    it = 0\n",
        "\n",
        "    if patience is None or patience == 0:\n",
        "        patience = epochs\n",
        "    result = None\n",
        "    start_epoch = start_epoch\n",
        "    end_epoch = start_epoch + epochs\n",
        "    best_val_metric = np.inf\n",
        "    best_val_epoch = None\n",
        "    save_mode = 'state_dict' if save_mode is None else save_mode\n",
        "    stop_counter = 0\n",
        "    is_epoch_scheduler = any(s in str(lr_scheduler.__class__) for s in EPOCH_SCHEDULERS)\n",
        "\n",
        "    for epoch in range(start_epoch, end_epoch):\n",
        "        model.train()\n",
        "        torch.cuda.empty_cache()\n",
        "        for batch in tqdm(train_loader):\n",
        "            loss_epoch = []\n",
        "            loss = train_batch(model, loss_func, batch, optimizer, lr_scheduler, device, grad_clip=grad_clip)\n",
        "            loss = np.array(loss)\n",
        "            loss_epoch.append(loss)\n",
        "            it += 1\n",
        "            lr = optimizer.param_groups[0]['lr']\n",
        "            writer.add_scalar('LR', lr, it)\n",
        "            lr_history.append(lr)\n",
        "            # log = f\"epoch: [{epoch + 1}/{end_epoch}]\"\n",
        "            # if loss.ndim == 0:  # 1 target loss\n",
        "            #     _loss_mean = np.mean(loss_epoch)\n",
        "            #     log += \" loss: {:.6f}\".format(_loss_mean)\n",
        "            # else:\n",
        "        _loss_mean = np.mean(loss_epoch)\n",
        "        #     for j in range(len(_loss_mean)):\n",
        "        #         log += \" | loss {}: {:.6f}\".format(j, _loss_mean[j])\n",
        "        # log += \" | current lr: {:.3e}\".format(lr)\n",
        "        #\n",
        "        # if it % print_freq == 0:\n",
        "        #     print(log)\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar(\"train_loss\", _loss_mean, epoch)  #### loss 0 seems to be the sum of all loss\n",
        "\n",
        "        loss_train.append(_loss_mean)\n",
        "\n",
        "        if epoch % args.val_epochs==0:\n",
        "            val_result = validate_epoch(model, metric_func, valid_loader, device,False)\n",
        "\n",
        "            loss_val.append(val_result[\"metric\"])\n",
        "            val_metric = val_result[\"metric\"]\n",
        "            print(\n",
        "                f'epoch:[{epoch + 1}/{end_epoch}] train_loss:{_loss_mean:.4f} val_loss:{val_metric:.4f} lr:{lr_history[-1]:.6f}')\n",
        "        else:\n",
        "            print(\n",
        "                f'epoch:[{epoch + 1}/{end_epoch}] train_loss:{_loss_mean:.4f} lr:{lr_history[-1]:.6f}')\n",
        "\n",
        "        if val_metric < best_val_metric:\n",
        "            best_val_epoch = epoch\n",
        "            best_val_metric = val_metric\n",
        "            torch.save(model.state_dict(), os.path.join(model_save_path, 'best_val.pth'))\n",
        "\n",
        "        if lr_scheduler and is_epoch_scheduler:\n",
        "            if 'ReduceLROnPlateau' in str(lr_scheduler.__class__):\n",
        "                lr_scheduler.step(val_metric)\n",
        "            else:\n",
        "                lr_scheduler.step()\n",
        "\n",
        "        # if val_result[\"metric\"].size == 1:\n",
        "        #     log = \"| val metric 0: {:.6f} \".format(val_metric)\n",
        "        #\n",
        "        # else:\n",
        "        #     log = ''\n",
        "        #     for i, metric_i in enumerate(val_result['metric']):\n",
        "        #         log += '| val metric {} : {:.6f} '.format(i, metric_i)\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar('val loss', val_metric, epoch)\n",
        "\n",
        "        # log += \"| best val: {:.6f} at epoch {} | current lr: {:.3e}\".format(best_val_metric, best_val_epoch + 1, lr)\n",
        "\n",
        "        # desc_ep = \"\"\n",
        "        # if _loss_mean.ndim == 0:  # 1 target loss\n",
        "        #     desc_ep += \"| loss: {:.6f}\".format(_loss_mean)\n",
        "        # else:\n",
        "        #     for j in range(len(_loss_mean)):\n",
        "        #         if _loss_mean[j] > 0:\n",
        "        #             desc_ep += \"| loss {}: {:.3e}\".format(j, _loss_mean[j])\n",
        "        #\n",
        "        # desc_ep += log\n",
        "        # print(desc_ep)\n",
        "        torch.save(model.state_dict(), os.path.join(f'runs/{args.train_log}/last.pth'))\n",
        "        result = dict(\n",
        "            # best_val_epoch=best_val_epoch,\n",
        "            # best_val_metric=best_val_metric,\n",
        "            # loss_train=np.asarray(loss_train),\n",
        "            # loss_val=np.asarray(loss_val),\n",
        "            # lr_history=np.asarray(lr_history),\n",
        "            # best_model=best_model_state_dict,\n",
        "            # optimizer_state=optimizer.state_dict()\n",
        "        )\n",
        "        # pickle.dump(result, open(os.path.join(model_save_path, result_name), 'wb'))\n",
        "    return result\n",
        "\n",
        "\n",
        "def train_batch(model, loss_func, data, optimizer, lr_scheduler, device, grad_clip=0.999):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    query_points, y = data['vertices'], data['pressure']\n",
        "    query_points, y, = query_points.to(device), y.to(device)\n",
        "    conditions = MultipleTensors(extract_condition(data))\n",
        "    conditions = conditions.to(device)\n",
        "\n",
        "    out = model(query_points, conditions)\n",
        "\n",
        "    y_pred = out.squeeze(-1)\n",
        "    loss = loss_func(y_pred, y)\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if lr_scheduler:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validate_epoch(model, metric_func, valid_loader, device, test, best_state_dict=None,save_name='output_pressure'):\n",
        "    if test:\n",
        "        model.load_state_dict(best_state_dict)\n",
        "    model.eval()\n",
        "    metric_val = []\n",
        "    for i, data in enumerate(valid_loader):\n",
        "        with torch.no_grad():\n",
        "            query_points, y = data['vertices'], data['pressure']\n",
        "            query_points, y, = query_points.to(device), y.to(device)\n",
        "            conditions = MultipleTensors(extract_condition(data))\n",
        "            conditions = conditions.to(device)\n",
        "\n",
        "            out = model(query_points, conditions)\n",
        "\n",
        "            y_pred = out.squeeze(-1)\n",
        "            metric = metric_func(y_pred, y)\n",
        "\n",
        "            metric_val.append(metric.cpu().numpy())\n",
        "            if test:\n",
        "                write_to_vtk(y_pred.unsqueeze(-1).squeeze(0), mesh_path=CARdataset.all_test_mesh_path[i],save_name=save_name)\n",
        "    return dict(metric=np.mean(metric_val, axis=0))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = get_args()\n",
        "    if args.use_tb:\n",
        "        writer = SummaryWriter(f'runs/{args.train_log}')\n",
        "\n",
        "    else:\n",
        "        writer = None\n",
        "        log_path = None\n",
        "        # 生成test watertight.txt\n",
        "    if not os.path.exists(f'{args.test_dir}/watertight_meshes.txt'):\n",
        "        directory_path = f'{args.test_dir}/Inference'\n",
        "        output_file_name = f'{args.test_dir}/watertight_meshes.txt'\n",
        "        write_numbers_to_file(directory_path, output_file_name)\n",
        "\n",
        "    # 生成fake pressure\n",
        "    if not os.path.exists(f'{args.test_dir}/Label'):\n",
        "        os.makedirs(f'{args.test_dir}/Label', exist_ok=True)\n",
        "        for file in os.listdir(f'{args.test_dir}/Inference'):\n",
        "            id = file.split('.')[0].split('_')[1]\n",
        "            fake_label = np.ones(3682)\n",
        "            np.save(f'{args.test_dir}/Label/press_{id}.npy', fake_label)\n",
        "\n",
        "    if not args.no_cuda and torch.cuda.is_available():\n",
        "        device = torch.device('cuda:{}'.format(str(args.gpu)))\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    # 获取命令行参数\n",
        "    arg = sys.argv[1:]\n",
        "\n",
        "    # 将参数写入文件\n",
        "    with open(f'runs/{args.train_log}/args.txt', 'w') as f:\n",
        "        for a in arg:\n",
        "            f.write(a + '\\n')\n",
        "\n",
        "    kwargs = {'pin_memory': False} if args.gpu else {}\n",
        "    get_seed(args.seed, printout=False)\n",
        "\n",
        "    CARdataset = get_dataset(args)\n",
        "    # test_dataset = get_dataset(args)\n",
        "\n",
        "    train_loader = CARdataset.train_dataloader(\n",
        "        batch_size=args.batch_size, shuffle=True\n",
        "    )\n",
        "    val_loader = CARdataset.val_dataloader(\n",
        "        batch_size=1, shuffle=False\n",
        "    )\n",
        "    test_loader = CARdataset.test_dataloader(\n",
        "        batch_size=1, shuffle=False\n",
        "    )\n",
        "\n",
        "    # args.space_dim = int(re.search(r'\\d', args.dataset).group())\n",
        "    # args.normalizer =  train_dataset.y_normalizer.to(device) if train_dataset.y_normalizer is not None else None\n",
        "\n",
        "    #### set random seeds\n",
        "    get_seed(args.seed)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    loss_func = get_loss(args)\n",
        "    metric_func = LpLoss()\n",
        "\n",
        "    model = get_model(args)\n",
        "    model = model.to(device)\n",
        "    print(f\"\\nModel: {model.__name__}\\t Number of params: {get_num_params(model)}\")\n",
        "\n",
        "    # path_prefix = args.dataset + '_{}_'.format(args.component) + model.__name__ + args.comment + time.strftime(\n",
        "    #     '_%m%d_%H_%M_%S')\n",
        "    # model_path, result_path = path_prefix + '.pt', path_prefix + '.pkl'\n",
        "\n",
        "    # print(f\"Saving model and result in ./../models/checkpoints/{model_path}\\n\")\n",
        "\n",
        "    # print(model)\n",
        "    # print(config)\n",
        "\n",
        "    epochs = args.epochs\n",
        "    lr = args.lr\n",
        "\n",
        "    if args.optimizer == 'Adam':\n",
        "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=args.weight_decay, betas=(0.9, 0.999))\n",
        "    elif args.optimizer == \"AdamW\":\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=args.weight_decay, betas=(0.9, 0.999))\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if args.lr_method == 'cycle':\n",
        "        print('Using cycle learning rate schedule')\n",
        "        scheduler = OneCycleLR(optimizer, max_lr=lr, div_factor=1e4, pct_start=0.2, final_div_factor=1e4,\n",
        "                               steps_per_epoch=len(train_loader), epochs=epochs)\n",
        "    elif args.lr_method == 'step':\n",
        "        print('Using step learning rate schedule')\n",
        "        scheduler = StepLR(optimizer, step_size=args.lr_step_size * len(train_loader), gamma=0.7)\n",
        "    elif args.lr_method == 'warmup':\n",
        "        print('Using warmup learning rate schedule')\n",
        "        scheduler = LambdaLR(optimizer, lambda steps: min((steps + 1) / (args.warmup_epochs * len(train_loader)),\n",
        "                                                          np.power(\n",
        "                                                              args.warmup_epochs * len(train_loader) / float(steps + 1),\n",
        "                                                              0.5)))\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    result = train(model, loss_func, metric_func,\n",
        "                   train_loader, val_loader,\n",
        "                   optimizer, scheduler,\n",
        "                   epochs=epochs,\n",
        "                   grad_clip=args.grad_clip,\n",
        "                   patience=10,\n",
        "                   model_save_path=f'runs/{args.train_log}',\n",
        "                   writer=writer,\n",
        "                   device=device)\n",
        "\n",
        "    print('Training takes {} seconds.'.format(time.time() - time_start))\n",
        "\n",
        "    # result['args'], result['config'] = args, config\n",
        "    # checkpoint = {'args': args, 'model': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "    torch.save(model.state_dict(), os.path.join(f'runs/{args.train_log}/last.pth'))\n",
        "    # model.eval()\n",
        "    best_state_dict = torch.load(os.path.join(f'runs/{args.train_log}', 'best_val.pth'))\n",
        "    last_state_dict = torch.load(os.path.join(f'runs/{args.train_log}', 'last.pth'))\n",
        "\n",
        "    val_metric_best = validate_epoch(model, metric_func, test_loader, device, test=True, best_state_dict=best_state_dict)\n",
        "    val_metric_last = validate_epoch(model, metric_func, test_loader, device, test=True, best_state_dict=last_state_dict,save_name='output_pressure_last')\n",
        "    # with open(f'runs/{args.train_log}/result.txt', 'w') as f:\n",
        "    #     f.write(f\"Best model's validation metric in this run: {val_metric_best['metric']} \\n\")\n",
        "    #     f.write(f\"Last model's validation metric in this run: {val_metric_last['metric']} \\n\")\n",
        "    #\n",
        "    # print(f\"\\nBest model's validation metric in this run: {val_metric_best}\")\n",
        "    # print(f\"\\nLast model's validation metric in this run: {val_metric_last}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "85tvzBxnocZH",
        "outputId": "741477d8-a7db-4a3b-aa50-e06431541241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "use 475 training data!\n",
            "\n",
            "\n",
            "The following code snippets have been run.\n",
            "==================================================\n",
            "\n",
            "    os.environ['PYTHONHASHSEED'] = str(2024)\n",
            "    numpy.random.seed(2024)\n",
            "    torch.manual_seed(2024)\n",
            "    torch.cuda.manual_seed(2024)\n",
            "    torch.backends.cudnn.deterministic = True\n",
            "    torch.backends.cudnn.benchmark = False\n",
            "    if torch.cuda.is_available():\n",
            "        torch.cuda.manual_seed_all(2024)\n",
            "    \n",
            "==================================================\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "\n",
            "Model: MIOEGPT\t Number of params: 248052257\n",
            "Using cycle learning rate schedule\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 238/238 [02:57<00:00,  1.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:[1/600] train_loss:0.7613 val_loss:0.7890 lr:0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 29/238 [00:22<02:40,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-36db56224c4e>\u001b[0m in \u001b[0;36m<cell line: 224>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0mtime_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     result = train(model, loss_func, metric_func,\n\u001b[0m\u001b[1;32m    324\u001b[0m                    \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                    \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-36db56224c4e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_func, metric_func, train_loader, valid_loader, optimizer, lr_scheduler, epochs, writer, device, patience, grad_clip, start_epoch, model_save_path, save_mode)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mloss_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mloss_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-36db56224c4e>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(model, loss_func, data, optimizer, lr_scheduler, device, grad_clip)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_velocity"
      ],
      "metadata": {
        "id": "dI9SKNIxPwMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding:utf-8 _*-\n",
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append('../..')\n",
        "sys.path.append('..')\n",
        "\n",
        "import re\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "from torch.optim.lr_scheduler import OneCycleLR, StepLR, LambdaLR\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.nn import MSELoss ,HuberLoss,SmoothL1Loss,L1Loss\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "'''\n",
        "    A general code framework for training neural operator on irregular domains\n",
        "'''\n",
        "\n",
        "EPOCH_SCHEDULERS = ['ReduceLROnPlateau', 'StepLR', 'MultiplicativeLR',\n",
        "                    'MultiStepLR', 'ExponentialLR', 'LambdaLR']\n",
        "\n",
        "\n",
        "def extract_numbers(s):\n",
        "    return [int(digit) for digit in re.findall(r'\\d+', s)]\n",
        "\n",
        "\n",
        "def train(model, loss_func, metric_func,\n",
        "          train_loader, valid_loader,\n",
        "          optimizer, lr_scheduler,\n",
        "          epochs=10,\n",
        "          writer=None,\n",
        "          device=\"cuda\",\n",
        "          patience=10,\n",
        "          grad_clip=0.999,\n",
        "          start_epoch: int = 0,\n",
        "          model_save_path='./data/checkpoints/',\n",
        "          save_mode='state_dict',  # 'state_dict' or 'entire'\n",
        "          ):\n",
        "    loss_train = []\n",
        "    loss_val = []\n",
        "    loss_epoch = []\n",
        "    lr_history = []\n",
        "    it = 0\n",
        "\n",
        "    if patience is None or patience == 0:\n",
        "        patience = epochs\n",
        "    result = None\n",
        "    start_epoch = start_epoch\n",
        "    end_epoch = start_epoch + epochs\n",
        "    best_val_metric = np.inf\n",
        "    best_val_epoch = None\n",
        "    save_mode = 'state_dict' if save_mode is None else save_mode\n",
        "    stop_counter = 0\n",
        "    is_epoch_scheduler = any(s in str(lr_scheduler.__class__) for s in EPOCH_SCHEDULERS)\n",
        "\n",
        "    for epoch in range(start_epoch, end_epoch):\n",
        "        model.train()\n",
        "        torch.cuda.empty_cache()\n",
        "        for batch in tqdm(train_loader):\n",
        "            loss_epoch = []\n",
        "            loss = train_batch(model, loss_func, batch, optimizer, lr_scheduler, device, grad_clip=grad_clip)\n",
        "            loss = np.array(loss)\n",
        "            loss_epoch.append(loss)\n",
        "            it += 1\n",
        "            lr = optimizer.param_groups[0]['lr']\n",
        "            writer.add_scalar('LR', lr, it)\n",
        "            lr_history.append(lr)\n",
        "            # log = f\"epoch: [{epoch + 1}/{end_epoch}]\"\n",
        "            # if loss.ndim == 0:  # 1 target loss\n",
        "            #     _loss_mean = np.mean(loss_epoch)\n",
        "            #     log += \" loss: {:.6f}\".format(_loss_mean)\n",
        "            # else:\n",
        "        _loss_mean = np.mean(loss_epoch)\n",
        "        #     for j in range(len(_loss_mean)):\n",
        "        #         log += \" | loss {}: {:.6f}\".format(j, _loss_mean[j])\n",
        "        # log += \" | current lr: {:.3e}\".format(lr)\n",
        "        #\n",
        "        # if it % print_freq == 0:\n",
        "        #     print(log)\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar(\"train_loss\", _loss_mean, epoch)  #### loss 0 seems to be the sum of all loss\n",
        "\n",
        "        loss_train.append(_loss_mean)\n",
        "\n",
        "        val_result = validate_epoch(model, metric_func, valid_loader, device)\n",
        "\n",
        "        loss_val.append(val_result[\"metric\"])\n",
        "        val_metric = val_result[\"metric\"]\n",
        "        print(f'epoch:[{epoch + 1}/{end_epoch}] train_loss:{_loss_mean:.4f} val_loss:{val_metric:.4f} lr:{lr_history[-1]:.6f}')\n",
        "\n",
        "        if val_metric < best_val_metric:\n",
        "            best_val_epoch = epoch\n",
        "            best_val_metric = val_metric\n",
        "            torch.save(model.state_dict(), os.path.join(model_save_path, 'best_val.pth'))\n",
        "\n",
        "        if lr_scheduler and is_epoch_scheduler:\n",
        "            if 'ReduceLROnPlateau' in str(lr_scheduler.__class__):\n",
        "                lr_scheduler.step(val_metric)\n",
        "            else:\n",
        "                lr_scheduler.step()\n",
        "\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar('val loss', val_metric, epoch)\n",
        "\n",
        "        torch.save(model.state_dict(), os.path.join(f'runs/{args.train_log}/last.pth'))\n",
        "        result = dict(\n",
        "            # best_val_epoch=best_val_epoch,\n",
        "            # best_val_metric=best_val_metric,\n",
        "            # loss_train=np.asarray(loss_train),\n",
        "            # loss_val=np.asarray(loss_val),\n",
        "            # lr_history=np.asarray(lr_history),\n",
        "            # best_model=best_model_state_dict,\n",
        "            # optimizer_state=optimizer.state_dict()\n",
        "        )\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def train_batch(model, loss_func, data, optimizer, lr_scheduler, device, grad_clip=0.999):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    query_points, y = data['vertices'], data['velocity']\n",
        "    query_points, y, = query_points.to(device), y.to(device)\n",
        "    conditions = MultipleTensors(extract_condition(data))\n",
        "    conditions = conditions.to(device)\n",
        "\n",
        "    out = model(query_points, conditions)\n",
        "\n",
        "    # y_pred = out.squeeze(-1)\n",
        "    loss = loss_func(out, y)\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if lr_scheduler:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validate_epoch(model, metric_func, valid_loader, device):\n",
        "\n",
        "    model.eval()\n",
        "    metric_val = []\n",
        "    for i, data in enumerate(valid_loader):\n",
        "        with torch.no_grad():\n",
        "            query_points, y = data['vertices'], data['velocity']\n",
        "            query_points, y, = query_points.to(device), y.to(device)\n",
        "            conditions = MultipleTensors(extract_condition(data))\n",
        "            conditions = conditions.to(device)\n",
        "\n",
        "            out = model(query_points, conditions)\n",
        "\n",
        "            # y_pred = out.squeeze(-1)\n",
        "            metric = metric_func(out, y)\n",
        "            metric_val.append(metric.cpu().numpy())\n",
        "\n",
        "    return dict(metric=np.mean(metric_val, axis=0))\n",
        "\n",
        "def test_epoch(model, test_loader, device,model_weight):\n",
        "    model.load_state_dict(model_weight,strict=False)\n",
        "    model.eval()\n",
        "\n",
        "    if not os.path.exists(f\"./runs/{args.train_log}/output\"):\n",
        "        os.makedirs(f\"./runs/{args.train_log}/output\", exist_ok=True)\n",
        "    for i, data in enumerate(test_loader):\n",
        "        with torch.no_grad():\n",
        "            query_points = data['vertices']\n",
        "            query_points = query_points.to(device)\n",
        "            conditions = MultipleTensors(extract_condition(data))\n",
        "            conditions = conditions.to(device)\n",
        "\n",
        "            out = model(query_points, conditions)\n",
        "            test_indice = test_loader.dataset.all_valid_indices_test[i]\n",
        "\n",
        "            npy_leaderboard = f\"./runs/{args.train_log}/output/vel_{str(test_indice).zfill(3)}.npy\"\n",
        "            print(f\"saving *.npy file for [Velocity] leaderboard : \", npy_leaderboard)\n",
        "            np.save(npy_leaderboard, out.squeeze(0).cpu().numpy())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = get_args_velocity()\n",
        "    print(args)\n",
        "    if args.use_tb:\n",
        "        writer = SummaryWriter(f'runs/{args.train_log}')\n",
        "\n",
        "    else:\n",
        "        writer = None\n",
        "        log_path = None\n",
        "    if not args.no_cuda and torch.cuda.is_available():\n",
        "        device = torch.device('cuda:{}'.format(str(args.gpu)))\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    # 获取命令行参数\n",
        "    arg = sys.argv[1:]\n",
        "\n",
        "    # 将参数写入文件\n",
        "    with open(f'runs/{args.train_log}/args.txt', 'w') as f:\n",
        "        for a in arg:\n",
        "            f.write(a + '\\n')\n",
        "\n",
        "    kwargs = {'pin_memory': False} if args.gpu else {}\n",
        "    get_seed(args.seed, printout=False)\n",
        "\n",
        "    #划分训练和验证集\n",
        "    log_file = \"./Training/Dataset_1/Feature_File/watertight_meshes.txt\"  # 替换为你的日志文件路径\n",
        "    output_file_train = f\"runs/{args.train_log}/train_indices.txt\"  # 输出文件路径\n",
        "    output_file_valid = f\"runs/{args.train_log}/val_indices.txt\"  # 输出文件路径\n",
        "    processor = DataProcessor()\n",
        "    indices = processor.load_valid_indices(log_file)\n",
        "    train_indices, val_indices = processor.split_data(indices, train_ratio=(1-args.val_ratio))\n",
        "    processor.save_split_indices(train_indices, val_indices, output_file_train, output_file_valid)\n",
        "\n",
        "    train_root=Path('./Training/Dataset_1/Feature_File')\n",
        "    test_root=Path('./Test/Dataset_1/Feature_File')\n",
        "\n",
        "    V_train=Velocity_Dataset(train_root,test_root,output_file_train,output_file_valid,mode='train',norm_type=args.norm_type)\n",
        "    V_val=Velocity_Dataset(train_root,test_root,output_file_train,output_file_valid,mode='val',norm_type=args.norm_type)\n",
        "    V_test=Velocity_Dataset(train_root,test_root,output_file_train,output_file_valid,mode='test',norm_type=args.norm_type)\n",
        "\n",
        "    train_loader=DataLoader(V_train,batch_size=args.batch_size,shuffle=True)\n",
        "    val_loader=DataLoader(V_val,batch_size=1,shuffle=False)\n",
        "    test_loader=DataLoader(V_test,batch_size=1,shuffle=False)\n",
        "    # args.space_dim = int(re.search(r'\\d', args.dataset).group())\n",
        "    # args.normalizer =  train_dataset.y_normalizer.to(device) if train_dataset.y_normalizer is not None else None\n",
        "\n",
        "    #### set random seeds\n",
        "    get_seed(args.seed)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    loss_func = get_loss(args)\n",
        "    metric_func = LpLoss()\n",
        "\n",
        "    model = get_model(args)\n",
        "    model = model.to(device)\n",
        "    print(f\"\\nModel: {model.__name__}\\t Number of params: {get_num_params(model)}\")\n",
        "\n",
        "    # path_prefix = args.dataset + '_{}_'.format(args.component) + model.__name__ + args.comment + time.strftime(\n",
        "    #     '_%m%d_%H_%M_%S')\n",
        "    # model_path, result_path = path_prefix + '.pt', path_prefix + '.pkl'\n",
        "\n",
        "    # print(f\"Saving model and result in ./../models/checkpoints/{model_path}\\n\")\n",
        "\n",
        "    # print(model)\n",
        "    # print(config)\n",
        "\n",
        "    epochs = args.epochs\n",
        "    lr = args.lr\n",
        "\n",
        "    if args.optimizer == 'Adam':\n",
        "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=args.weight_decay, betas=(0.9, 0.999))\n",
        "    elif args.optimizer == \"AdamW\":\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=args.weight_decay, betas=(0.9, 0.999))\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if args.lr_method == 'cycle':\n",
        "        print('Using cycle learning rate schedule')\n",
        "        scheduler = OneCycleLR(optimizer, max_lr=lr, div_factor=1e4, pct_start=0.2, final_div_factor=1e4,\n",
        "                               steps_per_epoch=len(train_loader), epochs=epochs)\n",
        "    elif args.lr_method == 'step':\n",
        "        print('Using step learning rate schedule')\n",
        "        scheduler = StepLR(optimizer, step_size=args.lr_step_size * len(train_loader), gamma=0.7)\n",
        "    elif args.lr_method == 'warmup':\n",
        "        print('Using warmup learning rate schedule')\n",
        "        scheduler = LambdaLR(optimizer, lambda steps: min((steps + 1) / (args.warmup_epochs * len(train_loader)),\n",
        "                                                          np.power(\n",
        "                                                              args.warmup_epochs * len(train_loader) / float(steps + 1),\n",
        "                                                              0.5)))\n",
        "    else:\n",
        "        scheduler = None\n",
        "    time_start = time.time()\n",
        "\n",
        "    result = train(model, loss_func, metric_func,\n",
        "                   train_loader, val_loader,\n",
        "                   optimizer, scheduler,\n",
        "                   epochs=epochs,\n",
        "                   grad_clip=args.grad_clip,\n",
        "                   patience=10,\n",
        "                   model_save_path=f'runs/{args.train_log}',\n",
        "                   writer=writer,\n",
        "                   device=device)\n",
        "\n",
        "    print('Training takes {} seconds.'.format(time.time() - time_start))\n",
        "\n",
        "    # result['args'], result['config'] = args, config\n",
        "    # checkpoint = {'args': args, 'model': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "    torch.save(model.state_dict(), os.path.join(f'runs/{args.train_log}/last.pth'))\n",
        "    # model.eval()\n",
        "    best_state_dict = torch.load(os.path.join(f'runs/{args.train_log}', 'best_val.pth'))\n",
        "    # last_state_dict = torch.load(os.path.join(f'runs/{args.train_log}', 'last.pth'))\n",
        "\n",
        "    test_epoch(model,test_loader,device,best_state_dict)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0AeXnSXHSfq",
        "outputId": "bc73be80-a43f-4bfd-866f-20f28bea9d53"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='ns2d', train_dir='/mnt/nunu/race/IJCAI_2024/Dataset/Training_data', test_dir='/mnt/nunu/race/IJCAI_2024/Dataset/Testset_track_A', norm_type=None, wind_direction='z', track='track_A', train_log='train_v', expand_data=False, val_ratio=0.1, p=0.5, iter_per_epoch=150, val_epoch=10, cat_area=False, use_transform=False, component='all', seed=2024, gpu=0, use_tb=1, comment='', train_num='all', test_num='all', sort_data=0, normalize_x='none', use_normalizer='unit', epochs=300, optimizer='AdamW', lr=0.001, weight_decay=5e-06, grad_clip=1000.0, batch_size=1, val_batch_size=8, no_cuda=False, lr_method='cycle', lr_step_size=50, warmup_epochs=50, loss_name='rel2', model_name='GNOT', n_hidden=256, n_layers=16, act='gelu', loss='default', model='GNOT', n_head=8, slice_num=64, agent_num=64, ffn_dropout=0.0, attn_dropout=0.0, mlp_layers=3, attn_type='linear', use_fourier_feat=False, n_experts=2, input_dim=3, output_dim=3, space_dim=3, branch_sizes=[3], n_inner=2)\n",
            "\n",
            "\n",
            "The following code snippets have been run.\n",
            "==================================================\n",
            "\n",
            "    os.environ['PYTHONHASHSEED'] = str(2024)\n",
            "    numpy.random.seed(2024)\n",
            "    torch.manual_seed(2024)\n",
            "    torch.cuda.manual_seed(2024)\n",
            "    torch.backends.cudnn.deterministic = True\n",
            "    torch.backends.cudnn.benchmark = False\n",
            "    if torch.cuda.is_available():\n",
            "        torch.cuda.manual_seed_all(2024)\n",
            "    \n",
            "==================================================\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "Using Linear Attention\n",
            "\n",
            "Model: MIOEGPT\t Number of params: 30332707\n",
            "Using cycle learning rate schedule\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 450/450 [07:34<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:[1/1] train_loss:0.0131 val_loss:0.0120 lr:0.000000\n",
            "Training takes 476.02619218826294 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-97b2ee5d7a6a>:298: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_state_dict = torch.load(os.path.join(f'runs/{args.train_log}', 'best_val.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_658.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_659.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_660.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_662.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_663.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_664.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_665.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_666.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_667.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_668.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_672.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_673.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_674.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_675.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_676.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_677.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_678.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_679.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_681.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_683.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_684.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_686.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_687.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_688.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_689.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_690.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_691.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_692.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_693.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_695.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_696.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_697.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_700.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_701.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_702.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_703.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_704.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_705.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_708.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_709.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_710.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_711.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_712.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_713.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_715.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_717.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_718.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_719.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_721.npy\n",
            "saving *.npy file for [Velocity] leaderboard :  ./runs/train_v/output/vel_722.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_cd"
      ],
      "metadata": {
        "id": "IdzaZOAjPzok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding:utf-8 _*-\n",
        "import sys\n",
        "import os\n",
        "\n",
        "sys.path.append('../..')\n",
        "sys.path.append('..')\n",
        "import csv\n",
        "import re\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import OneCycleLR, StepLR, LambdaLR\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "'''\n",
        "    A general code framework for training neural operator on irregular domains\n",
        "'''\n",
        "\n",
        "EPOCH_SCHEDULERS = ['ReduceLROnPlateau', 'StepLR', 'MultiplicativeLR',\n",
        "                    'MultiStepLR', 'ExponentialLR', 'LambdaLR']\n",
        "\n",
        "\n",
        "def train(model, loss_func,\n",
        "          train_loader,\n",
        "          optimizer, lr_scheduler,\n",
        "          epochs=10,\n",
        "          writer=None,\n",
        "          device=\"cuda\",\n",
        "          patience=10,\n",
        "          grad_clip=0.999,\n",
        "          start_epoch: int = 0,\n",
        "          save_mode='state_dict',  # 'state_dict' or 'entire'\n",
        "          ):\n",
        "    loss_train = []\n",
        "    loss_val = []\n",
        "    loss_epoch = []\n",
        "    lr_history = []\n",
        "    it = 0\n",
        "\n",
        "    if patience is None or patience == 0:\n",
        "        patience = epochs\n",
        "    result = None\n",
        "    start_epoch = start_epoch\n",
        "    end_epoch = start_epoch + epochs\n",
        "\n",
        "    for epoch in range(start_epoch, end_epoch):\n",
        "        model.train()\n",
        "        # torch.cuda.empty_cache()\n",
        "        for batch in tqdm(train_loader):\n",
        "            loss_epoch = []\n",
        "            loss = train_batch(model, loss_func, batch, optimizer, lr_scheduler, device, grad_clip=grad_clip)\n",
        "            loss = np.array(loss)\n",
        "            loss_epoch.append(loss)\n",
        "            it += 1\n",
        "            lr = optimizer.param_groups[0]['lr']\n",
        "            writer.add_scalar('LR', lr, it)\n",
        "            lr_history.append(lr)\n",
        "        _loss_mean = np.mean(loss_epoch)\n",
        "\n",
        "\n",
        "        if writer is not None:\n",
        "            writer.add_scalar(\"train_loss\", _loss_mean, epoch)  #### loss 0 seems to be the sum of all loss\n",
        "        loss_train.append(_loss_mean)\n",
        "        print(f'epoch:[{epoch + 1}/{end_epoch}] train_loss:{_loss_mean:.4f}')\n",
        "        torch.save(model.state_dict(), os.path.join(f'runs/{args.train_log}/last.pth'))\n",
        "        result = dict(\n",
        "        )\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def train_batch(model, loss_func, data, optimizer, lr_scheduler, device, grad_clip=0.999):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    query_points, y = data['vertices'], data['cd']\n",
        "    query_points, y, = query_points.to(device), y.to(device)\n",
        "    conditions = MultipleTensors(extract_condition(data))\n",
        "    conditions = conditions.to(device)\n",
        "\n",
        "    out = model(query_points, conditions)\n",
        "\n",
        "    y_pred = out.squeeze(-1)\n",
        "    # y_mean=y_pred.mean(-1)\n",
        "    # y_=y_mean.unsqueeze(-1)\n",
        "    # print(y.size())\n",
        "    # print(y_pred.mean(-1).size())\n",
        "    loss = loss_func(y_pred.mean(-1), y)\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if lr_scheduler:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = get_args_cd()\n",
        "    if args.use_tb:\n",
        "        writer = SummaryWriter(f'runs/{args.train_log}')\n",
        "\n",
        "    else:\n",
        "        writer = None\n",
        "        log_path = None\n",
        "    if not args.no_cuda and torch.cuda.is_available():\n",
        "        device = torch.device('cuda:{}'.format(str(args.gpu)))\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    # 获取命令行参数\n",
        "    arg = sys.argv[1:]\n",
        "\n",
        "    # 将参数写入文件\n",
        "    with open(f'runs/{args.train_log}/args.txt', 'w') as f:\n",
        "        for a in arg:\n",
        "            f.write(a + '\\n')\n",
        "\n",
        "    kwargs = {'pin_memory': False} if args.gpu else {}\n",
        "    get_seed(args.seed, printout=False)\n",
        "    data_root='./Training/Dataset_2/Feature_File'\n",
        "    data_root_test='./Test/Dataset_2/Feature_File'\n",
        "    file_path='./Training/Dataset_2/Label_File/dataset2_train_label.csv'\n",
        "    cd_dataset = CdDataset(data_root,file_path)\n",
        "    cd_dataset_test = CdTestDataset(data_root_test)\n",
        "    train_loader = DataLoader(cd_dataset,batch_size=args.batch_size,shuffle=True)\n",
        "    test_loader = DataLoader(cd_dataset_test,batch_size=1,shuffle=False)\n",
        "    # args.space_dim = int(re.search(r'\\d', args.dataset).group())\n",
        "    # args.normalizer =  train_dataset.y_normalizer.to(device) if train_dataset.y_normalizer is not None else None\n",
        "\n",
        "    #### set random seeds\n",
        "    get_seed(args.seed)\n",
        "    # torch.cuda.empty_cache()\n",
        "\n",
        "    loss_func = get_loss(args)\n",
        "    # metric_func = LpLoss()\n",
        "\n",
        "    model = get_model(args)\n",
        "    model = model.to(device)\n",
        "    print(f\"\\nModel: {model.__name__}\\t Number of params: {get_num_params(model)}\")\n",
        "\n",
        "    epochs = args.epochs\n",
        "    lr = args.lr\n",
        "\n",
        "    if args.optimizer == 'Adam':\n",
        "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=args.weight_decay, betas=(0.9, 0.999))\n",
        "    elif args.optimizer == \"AdamW\":\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=args.weight_decay, betas=(0.9, 0.999))\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if args.lr_method == 'cycle':\n",
        "        print('Using cycle learning rate schedule')\n",
        "        scheduler = OneCycleLR(optimizer, max_lr=lr, div_factor=1e4, pct_start=0.2, final_div_factor=1e4,\n",
        "                               steps_per_epoch=len(train_loader), epochs=epochs)\n",
        "    elif args.lr_method == 'step':\n",
        "        print('Using step learning rate schedule')\n",
        "        scheduler = StepLR(optimizer, step_size=args.lr_step_size * len(train_loader), gamma=0.7)\n",
        "    elif args.lr_method == 'warmup':\n",
        "        print('Using warmup learning rate schedule')\n",
        "        scheduler = LambdaLR(optimizer, lambda steps: min((steps + 1) / (args.warmup_epochs * len(train_loader)),\n",
        "                                                          np.power(\n",
        "                                                              args.warmup_epochs * len(train_loader) / float(steps + 1),\n",
        "                                                              0.5)))\n",
        "    else:\n",
        "        scheduler=None\n",
        "    time_start = time.time()\n",
        "\n",
        "    result = train(model, loss_func,\n",
        "                   train_loader,\n",
        "                   optimizer,scheduler,\n",
        "                   epochs=epochs,\n",
        "                   grad_clip=args.grad_clip,\n",
        "                   patience=10,\n",
        "                   writer=writer,\n",
        "                   device=device)\n",
        "\n",
        "\n",
        "    print('Training takes {} seconds.'.format(time.time() - time_start))\n",
        "\n",
        "    # result['args'], result['config'] = args, config\n",
        "    # checkpoint = {'args': args, 'model': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "    torch.save(model.state_dict(), os.path.join(f'runs/{args.train_log}/last.pth'))\n",
        "    # model.eval()\n",
        "    best_state_dict = torch.load('best_val_cd.pth',map_location='cpu')\n",
        "\n",
        "    #test time:\n",
        "    model.load_state_dict(best_state_dict,strict=False)\n",
        "    model=model.to(device)\n",
        "    model.eval()\n",
        "    answer=[]\n",
        "    for data in test_loader:\n",
        "        with torch.no_grad():\n",
        "            query_points = data['vertices']\n",
        "            query_points = query_points.to(device)\n",
        "            conditions = MultipleTensors(extract_condition(data))\n",
        "            conditions = conditions.to(device)\n",
        "\n",
        "            out = model(query_points, conditions)\n",
        "            y_pred = out.squeeze(-1)\n",
        "            y_ = y_pred.mean(-1)[0].item()\n",
        "            y_=adjust_item(y_)\n",
        "            answer.append(y_)\n",
        "\n",
        "    csv_file_name = f'runs/{args.train_log}/Answer.csv'\n",
        "\n",
        "    # 使用with语句打开文件，确保文件正确关闭\n",
        "    with open(csv_file_name, mode='w', newline='') as file:\n",
        "        # 创建一个csv.writer对象\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # 写入标题行（可选，但如果你想要的话）\n",
        "        writer.writerow(['', 'Cd'])\n",
        "\n",
        "        # 遍历数据和索引，并将它们写入CSV文件\n",
        "        for index, value in enumerate(answer):\n",
        "            # enumerate会为我们提供索引（从0开始）和值\n",
        "            writer.writerow([index, value])\n",
        "    print('over write to answer!')"
      ],
      "metadata": {
        "id": "F4lQil98P0wi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}